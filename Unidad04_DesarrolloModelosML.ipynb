{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ad9bb0b-0d93-4b47-ba79-3343d3b967f9",
   "metadata": {},
   "source": [
    "![alt text](img/MIoT_ML.png \"MIoT_ML\")\n",
    "# Unidad 04  Entrenamiento de Modelos de Aprendizaje Automático\n",
    "\n",
    "El objetivo principal de esta práctica es el conocer los pasos básicos para el desarrollo, optimización y evaluación de los modelos de Aprendizaje Automático. \n",
    "El Notebook contiene varios ejercicios sencillos. Debéis desarrollarlos durante la clase y subirlos al aula virtual.\n",
    "\n",
    "## Referencias útiles para la práctica\n",
    "1. API Pandas: [https://pandas.pydata.org/docs/reference/index.html](https://pandas.pydata.org/docs/reference/index.html)\n",
    "2. API Scikit-learn: [https://scikit-learn.org/stable/api/index.html](https://scikit-learn.org/stable/api/index.html)\n",
    "3. Dataset para el ejercicio: [https://www.kaggle.com/datasets/camnugent/california-housing-prices](https://www.kaggle.com/datasets/camnugent/california-housing-prices)\n",
    "4. Géron, Aurélien. Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow. \" O'Reilly Media, Inc.\", 2022.\n",
    "5. Bergstra, J., & Bengio, Y. (2012). Random search for hyperparameter optimization. Journal of machine learning research, 13 (2). *Para profundizar en la optimización de hiperparámetros*\n",
    "6. Girish Chandrashekar and Ferat Sahin. A survey on feature selection methods. Computers & Electrical Engineering, 40(1):16–28, 2014\n",
    "   \n",
    "## 1. Flujo de trabajo básico en problemas de Aprendizaje Automático (*ML workflow*)\n",
    "A la hora de enfrentarnos a un nuevo problema de Aprendizaje Automático (ML), existen una serie de pasos típicos y comunes que debemos afrontar:\n",
    "1. Entender el problema y su contexto.\n",
    "2. Obtener los datos (histórico).\n",
    "3. Explorar, analizar y entender los datos.\n",
    "4. Preparar los datos para los modelos.\n",
    "5. Seleccionar, optimizar y entrenar los modelos ML.\n",
    "6. Evaluar y presentar el modelo seleccionado.\n",
    "7. Desplegar, monitorizar y mantener la solución.\n",
    "\n",
    "En esta unidad nos centraremos en los pasos 5 y 6 del flujo de trabajo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1570b6b9-50ae-4f9e-a721-1f2c1124db86",
   "metadata": {},
   "source": [
    "## 2. Carga y partición de datos\n",
    "### 2.1. Carga de los datos\n",
    "Cargaremos los datos generados durante la Unidad02, almacenados al finalizar la misma. Suponemos que se han mantenido los nombres generados durante dicha unidad, y que los datos están almacenados en el directorio raíz.\n",
    "\n",
    "- Características de entrada preprocesadas (*Inputs*) de las observaciones del conjunto de entrenamiento (*preprocessing_trainset_inputs.csv*).\n",
    "- Etiquetas de salida (*Outputs*) de las observaciones del conjunto de entrenamiento (*trainset_ouputs.csv*).\n",
    "- Características de entrada preprocesadas (*Inputs*) de las observaciones del conjunto de Test (*preprocessing_testset_inputs.csv*).\n",
    "- Etiquetas de salida (*Outputs*) de las observaciones del conjunto de Test (*testset_outputs.csv*).\n",
    "- Columna de los índices \"*idx*\" (tal como se guardó en la Unidad 02)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98859249-f101-4d2e-bc90-7b4309a4363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas library\n",
    "import pandas as pd\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ImportError as err:\n",
    "    !pip install pandas\n",
    "    import pandas as pd\n",
    "\n",
    "# leemos los datos de training, indicando que la columna que tiene los índices originales es \"idx\", que se mantendrá en los nuevos Dataframes\n",
    "training_inputs = pd.read_csv(\"./preprocessing_trainset_inputs.csv\", index_col=\"idx\")\n",
    "training_outputs = pd.read_csv(\"./trainset_ouputs.csv\", index_col=\"idx\")\n",
    "print(\"Longitud del conjunto de entrenamiento:\", len(training_inputs))\n",
    "\n",
    "# leemos los datos de testing\n",
    "testing_inputs = pd.read_csv(\"./preprocessing_testset_inputs.csv\", index_col=\"idx\")\n",
    "testing_outputs = pd.read_csv(\"./testset_outputs.csv\", index_col=\"idx\")\n",
    "print(\"Longitud del conjunto de test:\", len(testing_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18407a45-c5fa-4770-ac57-66053185940e",
   "metadata": {},
   "source": [
    "### 2.2. Creación del conjunto de validación\n",
    "\n",
    "Vamos a dividir el conjunto de test original en dos conjuntos de igual tamaño: el conjunto de validación, que emplearemos para seleccionar los hiperparámetros que den mejores resultados; y el conjunto de test propiamente dicho, que emplearemos para medir la calidad del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fbb504-b200-4845-89f3-dc48a3b8278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "except ImportError as err:\n",
    "    !pip install sklearn\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED=1234\n",
    "\n",
    "# dividimos el conjunto de test original: testing_inputs\n",
    "test_inputs, val_inputs = train_test_split(testing_inputs, test_size=0.5, train_size=0.5, random_state=SEED, shuffle=True)\n",
    "\n",
    "# generamos los outputs correspondientes a los dos conjuntos creados\n",
    "# val_inputs.index es la lista de índices seleccionados para el conjunto de inputs de validación\n",
    "# deben ser los mismos índices para el conjunto de outputs de validación\n",
    "val_outputs = testing_outputs.loc[val_inputs.index]\n",
    "print(\"Longitud del conjunto de validación:\", len(val_outputs))\n",
    "\n",
    "# igualmente para el conjunto de test\n",
    "test_outputs = testing_outputs.loc[test_inputs.index]\n",
    "print(\"Longitud del conjunto de test:\", len(test_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a547424-5e3e-491a-ac68-960abe0df9ce",
   "metadata": {},
   "source": [
    "## 3. Creación y evaluación de un modelo básico\n",
    "\n",
    "### 3.1. Selección de una métrica de evaluación\n",
    "\n",
    "Para medir la calidad del modelo que vamos a desarrollar, necesitamos seleccionar una métrica que nos dé un valor que cuantifique la diferencia entre los datos conocidos del conjunto de test y los datos predichos por el modelo. \n",
    "Una medida de rendimiento típica para los problemas de regresión es el error cuadrático medio (RMSE) que nos da una idea de cuánto error suele cometer el sistema en sus predicciones en las mismas unidades que la variable dependiente, lo que puede facilitar la interpretación de los resultados. \n",
    "\n",
    "$$RMSE=\\large\\sqrt{\\frac{1}{n} \\sum_{i=1}^N{ (\\hat{y}_i-y_i)^2}}$$\n",
    "\n",
    "\n",
    "\n",
    "El RMSE tiene una serie de características que debemos tener en cuenta a la hora de seleccionarlo como, por ejemplo: \n",
    "\n",
    "* Promedia los errores individuales sin tener en cuenta el signo.\n",
    "    * No informa de los sesgos sistemáticos (ej. el modelo tiende a sobrestimar o subestimar).\n",
    "* Penaliza los errores grandes.\n",
    "* Muy sensible a los atípicos (*outliers*).\n",
    "    * Pueden disparar la métricas y dar una percepción inexacta.\n",
    " \n",
    "Existen otras métricas interesantes para regresión que debemos valorar dependiendo de nuestro objetivo y nuestro problema. Por ejemplo, si el número de *outliers* en nuestras manzanas fuese muy alto, quizás nos conveniese emplear una métrica robusta contra los atípicos como el error absoluto medio (MAE):\n",
    "\n",
    "$$\\large MAE=\\frac{1}{N}\\sum_{i=1}^N|y_i-\\hat{y}_i|$$\n",
    "\n",
    "\n",
    "\n",
    "En nuestro caso, vamos a emplear la métrica RMSE (en Scikit-learn está disponible mediante la función \"[*root_mean_squared_error*](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.root_mean_squared_error.html)\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a3f1a8-207d-4a49-ac4b-1a253834ba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3700c60f-3079-4860-a6f7-6ee2a788a3c0",
   "metadata": {},
   "source": [
    "### 3.2. Creación y evaluación de un modelo básico\n",
    "#### 3.2.1. Creación de un modelo de regresión lineal\n",
    "\n",
    "Como dijimos en la unidad anterior, en la vida real normalmente no programaremos un modelo, sino que eligiremos uno de los muchos que hay en las librerías conocidas, como Scikit-learn. En este caso, vamos a elegir un modelo básico de regresión lineal multivariable (varios atributos de entrada), que en Scitkit-learn es implementado por la clase **LinearRegression**. Y llamaremos a su función *fit* pasándole las entradas y los valores reales (del conjunto de entrenamiento) para que entrene el modelo con esos datos y ajuste los parámetros del modelo.\n",
    "\n",
    "**Nota**: se puede añadir al propio pipeline de preprocesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3ede68-081e-4a2c-a8b0-b4073f60501d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model_lin_reg = LinearRegression()\n",
    "model_lin_reg.fit(training_inputs, training_outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3c0f1c-47e7-464b-8d03-0c6e16ac0e90",
   "metadata": {},
   "source": [
    "#### 3.2.2. Evaluación con el dataset de entrenamiento\n",
    "\n",
    "Una vez entrenado el modelo con el conjunto de entrenamiento, podemos ver qué tal es su desempeño sobre el conjunto de entrenamiento (midiéndolo con la métrica RMSE). El grado de acierto sobre las primeras muestras:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d3cd1c-b3f2-49f2-aa01-1d1c5b890490",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_predictions = model_lin_reg.predict(training_inputs)\n",
    "\n",
    "print(\"Primeras 5 observaciones del entrenamiento\")\n",
    "for real, pred in zip(training_predictions[:5].round(-2) ,training_outputs.iloc[:5].values):\n",
    "    print(f\"Valor real: {real}, valor predicho: {pred}. Diferencia en absoluto{abs(real-pred)}. Porcentaje desvío: {(abs(real-pred)*100/real).round(2)}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d397f8d7-da9d-4076-b843-a86832d601da",
   "metadata": {},
   "source": [
    "Y el grado de acierto sobre todo el dataset de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7467dd-0e9d-47ba-809f-4cef8ff4533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rmse_train = root_mean_squared_error(training_outputs, training_predictions)\n",
    "print(f\"RMSE del conjunto de entrenamiento con modelo de regresión lineal: {rmse_train.round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83b03ba-19bb-4045-b1b5-55b4fd415688",
   "metadata": {},
   "source": [
    "#### 3.2.3. Evaluación con un conjunto de datos no visto previamente\n",
    "\n",
    "Veamos ahora cómo se comporta el modelo con un conjunto de datos que no ha visto durante el entrenamiento, por ejemplo, el conjunto de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d49d84-5dbd-49c6-af0d-d5181b90e700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación con el dataset de validación para el modelo de regresión lineal\n",
    "val_predictions = model_lin_reg.predict(val_inputs)\n",
    "rmse_val_lr = root_mean_squared_error(val_outputs, val_predictions)\n",
    "print(f\"RMSE del conjunto de validación con modelo de regresión lineal: {rmse_val_lr.round(2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a1198c-ee5c-4c6c-99e1-7b4b21d0fe95",
   "metadata": {},
   "source": [
    "### 3.3. Creación y evaluación de otro modelo (DecisionTree)\n",
    "\n",
    "Veamos ahora otro modelo de predicción, un árbol de decisión (*decision tree*).\n",
    "\n",
    "#### 3.3.1. Creación del modelo de árbol de decisión\n",
    "\n",
    "Para esta tarea, tenemos la clase **DecisionTreeRegressor** del Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c67395-6142-40f6-9f5f-bc3c3a816ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "model_tree = DecisionTreeRegressor(random_state=SEED)\n",
    "model_tree.fit(training_inputs, training_outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3ffc6b-777e-44ed-96a7-cb0c9942d542",
   "metadata": {},
   "source": [
    "#### 3.3.2. Evaluación con el dataset de entrenamiento\n",
    "\n",
    "Una vez entrenado el modelo con el conjunto de entrenamiento, podemos ver qué tal es su desempeño sobre el conjunto de entrenamiento (midiéndolo con la métrica RMSE). El grado de acierto sobre las primeras muestras y sobre todo el conjunto de entrenamiento es:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe963bb-0a8a-4865-a778-153b2bc90110",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_predictions = model_tree.predict(training_inputs)\n",
    "\n",
    "print(\"Primeras 5 observaciones del entrenamiento\")\n",
    "for real, pred in zip(training_predictions[:5].round(-2),training_outputs.iloc[:5].values):\n",
    "    print(f\"Valor real: {real}, valor predicho: {pred}. Diferencia en absoluto{abs(real-pred)}. Porcentaje desvío: {(abs(real-pred)*100/real).round(2)}%\")\n",
    "\n",
    "rmse_train = root_mean_squared_error(training_outputs, training_predictions)\n",
    "print(f\"RMSE del conjunto de entrenamiento con modelo de árbol de decisión: {rmse_train.round(2)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b5ebd9-cdaf-4c80-a800-2d0581311d7c",
   "metadata": {},
   "source": [
    "**¿Qué ha pasado? ¿Cómo puede ser que no tenga ningún error?**\n",
    "\n",
    "**EJERCICIO**\n",
    "Explica en una celda de texto qué puede estar pasando para que no exista error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67121e97-976d-42e9-adbc-00694f8dc548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJERCICIO\n",
    "# Explica en esta celda de texto que puede estar pasando para que no exista error\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3991fc6e-cb3c-4a75-96e1-3f7ae73ad381",
   "metadata": {},
   "source": [
    "#### 3.3.3. Evaluación con un conjunto de datos no visto previamente\n",
    "\n",
    "Veamos ahora cómo se comporta el modelo con un conjunto de datos que no ha visto durante el entrenamiento, por ejemplo, el conjunto de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d797236-23f9-4b06-8ed8-9e107a2a89e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_predictions = model_tree.predict(val_inputs)\n",
    "rmse_val_dt = root_mean_squared_error(val_outputs, val_predictions)\n",
    "print(f\"RMSE del conjunto de validación con el modelo árbol de decisión: {rmse_val_dt.round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf909fc-9e42-45b0-9757-8a1dd2d46bfd",
   "metadata": {},
   "source": [
    "### 3.4. Creación y evaluación de otro modelo (*Random Forest*)\n",
    "\n",
    "Veamos ahora otro modelo de predicción, uno llamado *Random Forest*, que es un *ensemble* (conjunto) de *decision trees*. Cada uno se entrena con un conjunto aleatorio de los datos y de los atributos. Al final, se promedia el resultado entre todos los árboles.\n",
    "\n",
    "Para esta tarea, tenemos la clase **RandomForestRegressor** del Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e4fce1-13f8-4f02-b87f-e43c31d274c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "    \n",
    "model_forest = RandomForestRegressor(n_estimators=80, random_state=SEED)\n",
    "# ravel() convierte el segundo parámetro en un array 1D, lo que necesita su fit, DecisionTreeRegressor acepta ambos (1D o 2D)\n",
    "model_forest.fit(training_inputs, np.array(training_outputs).ravel()) \n",
    "training_predictions = model_forest.predict(training_inputs)\n",
    "\n",
    "print(\"Primeras 5 observaciones del entrenamiento\")\n",
    "for real, pred in zip(training_predictions[:5].round(-2),training_outputs.iloc[:5].values):\n",
    "    print(f\"Valor real: {real}, valor predicho: {pred}. Diferencia en absoluto{abs(real-pred)}. Porcentaje desvío: {(abs(real-pred)*100/real).round(2)}%\")\n",
    "\n",
    "rmse_train = root_mean_squared_error(training_outputs, training_predictions)\n",
    "print(f\"RMSE del conjunto de entrenamiento con modelo de random forest: {rmse_train.round(2)}\")\n",
    "\n",
    "val_predictions = model_forest.predict(val_inputs)\n",
    "rmse_val_rf = root_mean_squared_error(val_outputs, val_predictions)\n",
    "print(f\"RMSE del conjunto de validación con el modelo random forest: {rmse_val_rf.round(2)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9c2f08-60c3-4091-9711-9193c46aa399",
   "metadata": {},
   "source": [
    "**Conclusión:** Como vemos, el modelo *Random Forest* da mejores resultados sobre el conjunto de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bacc25-0196-4230-a689-98f9be8332da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RSME LR =\", rmse_val_lr, \"    RSME DT =\", rmse_val_dt, \"    RSME RF =\", rmse_val_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9be94e-1cc6-4138-a4d3-94b229f9ea30",
   "metadata": {},
   "source": [
    "## 4. Selección de características\n",
    "\n",
    "Trabajar con muchos atributos (dimensiones) puede parecer lo adecuado cuando tenemos que generar modelos y buscamos un alto rendimiento, pero la realidad es que, cuando aumenta la dimensionalidad, el volumen del espacio de características aumenta exponencialmente, haciendo que los datos disponibles estén muy dispersos, lo que dificultará el trabajo de los modelos y que estos converjan y, como resultado, su rendimiento no será bueno. Es la llamada maldición de la dimensionalidad. \n",
    "\n",
    "El volumen y variabilidad de datos que necesitamos para entrenar en entornos de alta dimensionalidad es muy alto. Habitualmente, los modelos se ven beneficiados si reducimos la dimensionalidad. Una posibilidad para reducirla es aplicar técnicas de selección de las características (Feature Selection, FS), es decir, eliminar las características que proporcionen poca o ninguna información al modelo. La aplicación de estas técnicas tiene como consecuencia directa reducir las dimensiones, e, indirectamente, nos proporciona información sobre el problema con el que estamos trabajando (nos muestra las variables más relevantes). Las técnicas de selección de características se engloban en 3 grandes grupos: Filtros, Embebidos y Wrappers.\n",
    "\n",
    "**Nota**: *encontraréis una explicación más detallada de este apartado en la teoría de la materia*.\n",
    "\n",
    "### 4.1. Filtros\n",
    "Técnicas de preprocesado que crean listas ordenadas de las características en base a algún tipo de métrica. El objetivo es eliminar las características con peores posiciones en el ranking (es necesario establecer un umbral).\n",
    "* Son métodos simples y rápidos.\n",
    "* Son robustos al sobreentrenamiento.\n",
    "* Son fácilmente interpretables.\n",
    "* Asignar el umbral no es sencillo.\n",
    "* No valoran la combinación de variables (puede que dos variables independientes no aporten información al modelo, pero combinadas sí).\n",
    "* Variables altamente correladas y redundantes no se eliminan (si dos variables tienen una muy alta correlación entre ellas, es decir, representan lo mismo, es probable que no tenga sentido mantener las dos).\n",
    "\n",
    "\n",
    "Scikit-learn proporcina varios métodos de filtrado para poder aplicar sobre nuestros datos. Dos posibles estrategias una vez aplicado el método es:\n",
    "1. Seleccionar las 'k' características mejor posicionadas en el ranking.\n",
    "2. Seleccionar un porcetaje de las mejores características según el orden del ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0b7ab5-18a0-4b81-aa8c-8751bc143707",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ejemplos con filtros empleando Scikit-learn\n",
    "from sklearn.feature_selection import SelectKBest, r_regression # r_regression es una función que mide la correlación Pearson\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except ImportError as err:\n",
    "    !pip install matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_selected_features(features, scores, title=None, err=None):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel(\"Características\")\n",
    "    plt.ylabel(\"Importancia\")\n",
    "    plt.title(title)\n",
    "    plt.bar(features,scores,orientation='vertical', yerr=err )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Seleccionamos las 'K' mejores características\n",
    "K=10\n",
    "\n",
    "# creamos un K-selector que usa la correlación Pearson para ordenar las características\n",
    "f_selector = SelectKBest(r_regression, k=K)\n",
    "\n",
    "# calculamos la correlación entre las variables de entrada y la salida, y seleccionamos las de mayor correlación Pearson\n",
    "# Necesitamos convertir el dataframe \"training_outputs\" en un array de 1D. training_outputs.values.ravel()\n",
    "# training_outputs.values devuelve los valores en forma de array multidimensional (2D)\n",
    "# training_outputs.values.ravel() \"aplana\" el array (lo convierte a 1D)\n",
    "f_selector.fit(training_inputs, training_outputs.values.ravel())\n",
    "\n",
    "print(\"Todas las características:\\n\", training_inputs.columns)\n",
    "\n",
    "indices_fs = f_selector.get_support(indices=True)\n",
    "print(\"Índices de las características seleccionadas:\", indices_fs, \"\\n\")\n",
    "\n",
    "selected_fs = f_selector.get_feature_names_out(training_inputs.columns)\n",
    "print(\"Nombres de las características seleccionadas:\\n\", selected_fs, \"\\n\")\n",
    "\n",
    "selected_scores = f_selector.scores_\n",
    "print(\"Todos los scores:\\n\", selected_scores, \"\\n\")\n",
    "\n",
    "selected_fs_tuplas = [(training_inputs.columns[i], selected_scores[i]) for i in indices_fs]\n",
    "\n",
    "print(f\"Las {K} características con mayor correlación son:\")\n",
    "for t in sorted(selected_fs_tuplas, key = lambda t: t[1], reverse=True):  # ordenamos por score\n",
    "    print(t[0], \"=\", t[1])\n",
    "\n",
    "# dubujamos los scores obtenidos para cada característica de entrada en su correlación con la salida\n",
    "plot_selected_features(training_inputs.columns.values, f_selector.scores_, title=\"FS. Filtro correlación Pearson\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2240ed7e-8dff-47fa-9902-5616ef8359cb",
   "metadata": {},
   "source": [
    "**Fijaos** que en el ejemplo anterior quizás no estemos seleccionando las que nos interesan porque solo estamos **seleccionando las de mayor correlación positiva**. La correlación Pearson también nos da información interesante con la correlación negativa. Podemos modificar el código anterior para que tenga en cuenta el valor absoluto. Comprobaréis en el resultado que nos selecciona otras distintas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ca3dc1-1846-41c0-b87e-6294b061541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_r_regression(inputs, outputs):\n",
    "    return np.abs(r_regression(inputs, outputs))  \n",
    "\n",
    "# Seleccionamos las 'K' mejores características\n",
    "K=10\n",
    "\n",
    "# creamos un K-selector que usa el valor absoluto de la correlación Pearson \n",
    "f_selector = SelectKBest(abs_r_regression, k=K)\n",
    "\n",
    "# calculamos la correlación entre las variables de entrada y la salida, y seleccionamos las de mayor correlación Pearson\n",
    "f_selector.fit(training_inputs, training_outputs.values.ravel())\n",
    "\n",
    "indices_fs = f_selector.get_support(indices=True)\n",
    "selected_fs = f_selector.get_feature_names_out(training_inputs.columns)\n",
    "selected_scores = f_selector.scores_\n",
    "\n",
    "selected_fs_tuplas = [(training_inputs.columns[i], selected_scores[i]) for i in indices_fs]\n",
    "\n",
    "print(f\"Las {K} características con mayor correlación absoluta son:\")\n",
    "for t in sorted(selected_fs_tuplas, key = lambda t: t[1], reverse=True):\n",
    "    print(t[0], \"=\", t[1])\n",
    "\n",
    "# dubujamos los scores obtenidos por cada característica de entrada en su correlación con la salida\n",
    "plot_selected_features(training_inputs.columns.values, f_selector.scores_, title=\"FS. Filtro correlación Pearson\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465ca2fa-d89a-40ac-9e2d-012d247aaf8a",
   "metadata": {},
   "source": [
    "### 4.2. Embebidos\n",
    "Métodos que incorporan la selección de características en el proceso de entrenamiento de un modelo de ML. Dos tipos principales:\n",
    "1. Aproximaciones basadas en regularización (ej. Lasso-L1). Son algoritmos que tratan de minimizar una función de coste en la que participan términos de regularización. En el proceso de construcción del modelo se eliminan las características cuyo peso sea 0 ($\\beta_j = 0$).\n",
    "\n",
    "$$\\large F_{coste}=\\sum_{i=1}^N(y_i-\\sum(x_{ij}\\beta_j))^2 + \\lambda\\sum_{j=1}^P|\\beta_j|$$\n",
    "$$\\large L_1=\\lambda\\sum_{j=1}^P|\\beta_j|$$ \n",
    "       \n",
    "2. Aproximaciones basadas en la información extraída con la construcción del modelo. El ejemplo más claro son los modelos\n",
    "basados en árboles en los que la propia construcción del modelo produce la selección"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d2527c-9848-4986-bdd6-de945c1631ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Ejemplos embebidos\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn import linear_model\n",
    "\n",
    "#Modelo de regularización\n",
    "model = linear_model.Lasso(alpha=0.1, max_iter=1000)\n",
    "model.fit(training_inputs,training_outputs)\n",
    "#selector = SelectFromModel(estimator=model)\n",
    "#selector.fit(training_inputs,training_outputs)\n",
    "\n",
    "características_sin_relevancia=[ var for var, coef in zip(training_inputs.columns.values, model.coef_) if coef==0]\n",
    "\n",
    "print(\"Características con peso=0: \\n\"+\"\\n \".join(características_sin_relevancia))    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plot_selected_features(training_inputs.columns.values, model.coef_, title=\"FS. Embebido modelo lineal con regularización\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fc63c1-b4db-4626-a7c8-f8391fe7ec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=50)\n",
    "model.fit(training_inputs,training_outputs.values.ravel())\n",
    "\n",
    "importances = model.feature_importances_ #importancia general de los atributos en el RandomForest\n",
    "std = np.std([importances for tree in model.estimators_],axis=0)#Podemos calcular la importancia para cada árbol y con ello la std\n",
    "plot_selected_features(training_inputs.columns.values, importances, title=\"FS. Embebido RandomForest\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f136932-5682-4110-b335-a53c9add27db",
   "metadata": {},
   "source": [
    "### 4.3. Wrappers\n",
    "Algoritmos que integran un predictor durante el proceso de selección. Se prueban diferentes combinaciones de atributos para entrenar un modelo y el subconjunto con mejores resultados es el seleccionado. Es inviable computacionalmente probar todas las posibles combinaciones, por lo que es necesario establecer un método de búsqueda en el espacio de características que genere los subconjuntos.\n",
    "* Son simples, universales, tienen buen rendimiento (el modelo resultante) PERO:\n",
    "    * Son computacionalmente muy costosos.\n",
    "    * Dependen en gran medida del modelo seleccionado.\n",
    "    * Pueden caer en el sobreentrenamiento.\n",
    "    * Es difícil de configurar los modelos de forma justa (agregar el proceso de optimización de hiperparámetros produciría una sobrecarga muy importante).\n",
    "    * Cada método para seleccionar subconjuntos tiene problemáticas asociadas que es necesario valorar.\n",
    "\n",
    "Dos de los métodos más sencillos para establecer los conjuntos son:\n",
    "1. **Sequential forward selection algorithm**:\n",
    "    1. Empezamos con un conjunto vacío FS_DATASET.\n",
    "    2. Añadimos a FS_DATASET la característica X que maximiza el rendimiento del modelo.\n",
    "    3. Repetimos el paso 'B' hasta que se cumple la condición de salida. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d097bbf-f856-494e-b8e4-0dacac381cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ejemplos wrappers\n",
    "\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn import tree\n",
    "\n",
    "#Features to select\n",
    "K=10\n",
    "model = tree.DecisionTreeRegressor()\n",
    "\n",
    "sfs = SequentialFeatureSelector(model, n_features_to_select=K,direction='forward')\n",
    "\n",
    "sfs.fit(training_inputs, training_outputs)\n",
    "selected_features=sfs.get_feature_names_out(training_inputs.columns)\n",
    "print(\"Características seleccionadas empleando SFS:\\n\"+\"\\n\".join(selected_features))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bedf461-c97d-45a9-b1fa-69e823d9ba71",
   "metadata": {},
   "source": [
    "2. **Sequential backward selection algorithm**:\n",
    "    1. Empezamos con un conjunto completo FS_DATASET (con todas las posibles características).\n",
    "    2. Eliminamos de FS_DATASET la característica X que menos reduce el rendimiento del predictor (la más irrelevante).\n",
    "    3. Repetimos el paso 'B' hasta que se cumple la condición de salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55618645-dcf9-4cee-81ce-2ab93edbb495",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ejemplos wrappers\n",
    "\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn import tree\n",
    "\n",
    "#Features to select\n",
    "K=10\n",
    "model = tree.DecisionTreeRegressor()\n",
    "\n",
    "sfs = SequentialFeatureSelector(model, n_features_to_select=K,direction='backward')\n",
    "\n",
    "sfs.fit(training_inputs, training_outputs)\n",
    "selected_features=sfs.get_feature_names_out(training_inputs.columns)\n",
    "print(\"Características seleccionadas empleando SFS:\\n\"+\"\\n\".join(selected_features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dedec8-2dee-4eff-ba9d-f497ef9384b8",
   "metadata": {},
   "source": [
    "## 5. Validación cruzada (Cross Validation)\n",
    "\n",
    "Para estar seguros de que la selección de un conjunto de prueba determinado no influye en la medición de la calidad del modelo, se suele recurrir a un procedimiento de validación cruzada. En ese mecanismo se realizan distintas particiones del dataset, por ejemplo N, y se llevan a cabo N entrenamientos con su correspondiente evaluación, en cada una de ellas seleccionando una partición distinta como conjunto de test (y el resto como conjunto de entrenamiento). Luego se estudia la lista de resultados (evaluaciones), y se decide un resultado del modelo de alguna forma (por ejemplo, haciendo la media).\n",
    "\n",
    "Por supuesto, es posible hacerlo a mano, pero, afortunadamente, Scikit-learn nos ofrece herramientas para hacerlo de forma automática (la función \"cross_val_score\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0bf9e1-5b1d-4d58-b3fc-cd4b23965ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "inputs = pd.concat([training_inputs, testing_inputs])  # partimos del dataset completo, sobre el que se harán las particiones\n",
    "outputs = pd.concat([training_outputs,testing_outputs])\n",
    "\n",
    "scores = cross_val_score(model_lin_reg, inputs, outputs, cv=5, scoring='neg_mean_squared_error')\n",
    "root_scores = np.sqrt(-scores)\n",
    "print(root_scores, \"   Media=\", root_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb37184-7e3f-47b7-9e3d-23105c931645",
   "metadata": {},
   "source": [
    "En el ejemplo anterior elegimos realizar una validación cruzada con 5 particiones, pero, por defecto, las particiones se realizan siguiendo el orden secuencial de las observaciones, sin aleatoriedad ni estratificación. Si queremos hacerlo de forma aleatoria en el atributo de salidas, podemos emplear la clase **KFold** para crear un objeto y dárselo al parámetro *cv*. Y si queremos hacerlo de forma estratificada sobre la variable a predecir (sólo aplicable en caso de ser una variable categórica), emplearemos la clase **StratifiedKFold**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b06927-a8d4-41c3-9501-7ffc619a11d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "skf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "scores = cross_val_score(model_lin_reg, inputs, outputs, cv=skf, scoring='neg_mean_squared_error')\n",
    "root_scores = np.sqrt(-scores)\n",
    "print(root_scores, \"   Media=\", root_scores.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d0e9f4-c478-433f-9f5c-2b57342e8f5c",
   "metadata": {},
   "source": [
    "## 6. Optimización de hiperparámetros\n",
    "Los algoritmos de ML tienen una serie de parámetros de configuración que afectan al entrenamiento del modelo y posteriormente al desempeño de éste (ej. *learning rate* en regresión lineal, número de estimadores en *Random Forest*, etc.). Los parámetros por defecto raras veces son los óptimos, y es necesario optimizarlos para cada problema concreto. Este es un proceso muy costoso computacionalmente, y que requiere muchas veces de recursos dedicados. Existen diferentes técnicas para realizar la optimización (ej. GRID, aleatoria, evolutiva, etc.). La más básica es la optimización basada en una búsqueda GRID.\n",
    "\n",
    "### 6.1. Grid Search\n",
    "\n",
    "La optimización GRID es la prueba exhaustiva de todas las combinaciones posibles de varios parámetros (Scikit-learn nos proporciona la clase **GridSearchCV** para esta tarea). Para aplicarla, lo primero que necesitamos es establecer los rangos de valores que queremos probar. La prueba exhaustiva es el equivalente a probar todas las combinaciones empleando bucles “for” anidados. Las evaluaciones de hiperparámetros se hacen y se comparan a través de un conjunto de validación (o empleando validación cruzada), pero nunca contra el conjunto de test).\n",
    "\n",
    "El siguiente es un ejemplo de GridSearchCV sobre el modelo de árbol de decisión, donde exploramos:\n",
    "- dos valores para el hiperparámetro *criterion*, empleado para decidir la forma de creación de nodos al constuir el árbol\n",
    "- 4 para *max_features*, donde se indican los porcentajes de atributos que se incluirán en la creación del árbol (los atributos se eligen al azar).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e35d29-a0c1-4981-8520-4dfcf8814379",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# estos son los parámetros a analizar y los valores de cada uno a explorar\n",
    "param_grid = {\n",
    "    'criterion': [\"squared_error\", \"absolute_error\"], # para decidir la creación de nodos al constuir el árbol\n",
    "    'max_features': [0.25, 0.5, 0.75, 1.0]  # porcentajes de las features incluidas\n",
    "}\n",
    "\n",
    "\n",
    "# buscaremos los mejores parámetros para el modelo de árbol de decisión (model_tree)\n",
    "# cv puede ser un número fijo, un KFold, o una lista de tuplas (train,test) de índices POSICIONALES a probar (este caso)\n",
    "# en este caso, cv será una lista de un único elemento, una tupla que recoge todos los índices POSICIONALES de entrenamiento y validación\n",
    "grid_search = GridSearchCV(estimator=model_tree, param_grid=param_grid, \n",
    "                           scoring=\"neg_mean_squared_error\", # criterio para seleccionar la mejor combinación de parámetros\n",
    "                           return_train_score=True,\n",
    "                           cv=[(np.arange(len(training_inputs)),  # índices para entrenamiento\n",
    "                                np.arange(len(training_inputs),len(training_inputs) + len(val_inputs)) # índices para validación\n",
    "                               )])\n",
    "\n",
    "# x e y tienen los índices originales del dataframe en su columna idx, pero esto no se usará por el GridSearchCV.\n",
    "# si el primer índice del cv-train es '0', GridSearchCV cogerá el valor de la primera posición de x, no el índice idx=0.\n",
    "x = pd.concat([training_inputs, val_inputs])\n",
    "y = pd.concat([training_outputs, val_outputs])\n",
    "\n",
    "results = grid_search.fit(x,y)\n",
    "\n",
    "#for f,v in zip(x.columns, model_tree.feature_importances_):\n",
    "#    print(f,\"=\",v)\n",
    "\n",
    "print(f\"Mejor score en validación: {np.sqrt(-grid_search.best_score_)}\")\n",
    "print(f\"Params del mejor score en validación: {grid_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3a36a7-70fe-4658-823e-8e7fef57b246",
   "metadata": {},
   "source": [
    "Y, a continuación, otro ejemplo de GridSearchCV con el modelo *Random Forest*, donde exploramos:\n",
    "- 4 valores para el hiperparámetro *max_features*, el número de atributos usados al construir cada árbol del modelo\n",
    "- 5 para *n_estimators*, el número de árboles usados en el modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0d70a9-5557-4fc7-a06a-6285c4cc31aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ahora con Random Forest\n",
    "# estos son los parámetros a analizar y los valores de cada uno a explorar\n",
    "param_grid = {\n",
    "    'max_features': [2,4,6,8],\n",
    "    'n_estimators': [70,80,90,100,110]  \n",
    "}\n",
    "\n",
    "# buscaremos los mejores parámetros para el modelo de árbol de decisión (model_tree)\n",
    "# cv puede ser un número fijo, un KFold, o una lista de tuplas (train,test) de índices POSICIONALES a probar (este caso)\n",
    "# en este caso, cv será una lista de un único elemento, una tupla que recoge todos los índices POSICIONALES de entrenamiento y validación\n",
    "grid_search = GridSearchCV(estimator=model_forest, param_grid=param_grid, \n",
    "                           scoring=\"neg_mean_squared_error\", # criterio para seleccionar la mejor combinación de parámetros\n",
    "                           return_train_score=True,\n",
    "                           cv=[(np.arange(len(training_inputs)),  # índices para entrenamiento\n",
    "                                np.arange(len(training_inputs),len(training_inputs) + len(val_inputs)) # índices para validación\n",
    "                               )])\n",
    "\n",
    "# x e y tienen los índices originales del dataframe en su columna idx, pero esto no se usará por el GridSearchCV.\n",
    "# si el primer índice del cv-train es '0', GridSearchCV cogerá el valor de la primera posición de x, no el índice idx=0.\n",
    "x = pd.concat([training_inputs, val_inputs])\n",
    "y = pd.concat([training_outputs, val_outputs])\n",
    "\n",
    "results = grid_search.fit(x, np.array(y).ravel())\n",
    "\n",
    "print(f\"Mejor score en validación: {np.sqrt(-grid_search.best_score_)}\")\n",
    "print(f\"Params del mejor score en validación: {grid_search.best_params_}\\n\")\n",
    "\n",
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "\n",
    "sorted_importances = sorted(zip(x.columns, feature_importances), key=lambda x: x[1], reverse=True)\n",
    "for f,v in sorted_importances:\n",
    "    print(f,\"=\",v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76036149-7a80-4fe7-a6fc-c4c1a937c627",
   "metadata": {},
   "source": [
    "Vemos que el mejor resultado se obtiene para 100 árboles y 6 atributos, y que el atributo más importante ha sido *median_income*.\n",
    "\n",
    "Si dibujásemos los resultados (información contenida en *results* en el anterior código), veríamos algo como la siguiente figura, que muestra que la evaluación se realiza para todas las combinaciones, y las zonas donde se producen mejores y peores modelos (la figura no tiene relación con los datos, es sólo un ejemplo).\n",
    "\n",
    "\n",
    "| ![alt text](img/gridSearch.png \"Grid Search\") | \n",
    "|:--:| \n",
    "| **Grid Search**: Búsqueda a través de diferentes valores de dos hiperparámetros. Para cada hiperparámetro se consideran 10 valores diferentes (100 combinaciones distintas). Los contornos azules indican las regiones con mejores resultados, mientras que los rojos son regiones con peores resultados. Fuente de la imagen [wikipedia](https://es.m.wikipedia.org/wiki/Archivo:Hyperparameter_Optimization_using_Grid_Search.svg)|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eefe89-95bd-413b-b639-ca1fcad2e38b",
   "metadata": {},
   "source": [
    "### 6.2. Random Search\n",
    "\n",
    "En este caso, realizamos una búsqueda aleatoria (no completa) en el espacio de combinaciones de los diferentes valores de los hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f93689b-6367-4a8c-9fcd-663d78ad56d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemlo de Random Search con Random Forest\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# estos son los parámetros a analizar y los valores de cada uno a explorar\n",
    "param_distributions = {\n",
    "    'max_features': [2,4,6,8],\n",
    "    'n_estimators': [70,80,90,100,110]  \n",
    "}\n",
    "\n",
    "# buscaremos los mejores parámetros para el modelo de árbol de decisión (model_tree)\n",
    "# cv puede ser un número fijo, un KFold, o una lista de tuplas (train,test) de índices POSICIONALES a probar (este caso)\n",
    "# en este caso, cv será una lista de un único elemento, una tupla que recoge todos los índices POSICIONALES de entrenamiento y validación\n",
    "random_search = RandomizedSearchCV(estimator=model_forest, param_distributions=param_distributions,\n",
    "                           scoring=\"neg_mean_squared_error\", # criterio para seleccionar la mejor combinación de parámetros\n",
    "                           return_train_score=True, n_iter=10, # límite de combinaciones a probar (10 es el valor por defecto) \n",
    "                           cv=[(np.arange(len(training_inputs)),  # índices para entrenamiento\n",
    "                                np.arange(len(training_inputs),len(training_inputs) + len(val_inputs)) # índices para validación\n",
    "                               )])\n",
    "\n",
    "# x e y tienen los índices originales del dataframe en su columna idx, pero esto no se usará por el GridSearchCV.\n",
    "# si el primer índice del cv-train es '0', GridSearchCV cogerá el valor de la primera posición de x, no el índice idx=0.\n",
    "x = pd.concat([training_inputs, val_inputs])\n",
    "y = pd.concat([training_outputs, val_outputs])\n",
    "\n",
    "results = random_search.fit(x, np.array(y).ravel())\n",
    "\n",
    "print(f\"Mejor score en validación: {np.sqrt(-random_search.best_score_)}\")\n",
    "print(f\"Params del mejor score en validación: {random_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3958e36-7e24-4df6-92e4-4b7908c7b926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0f5ca92-6555-493e-9930-27f19f4744b2",
   "metadata": {},
   "source": [
    "Vemos que, si sólo probamos con 10 combinaciones, es posible que el mejor resultado no sea el de 100 árboles y 6 atributos.\n",
    "\n",
    "Si repitiésemos la figuar anterior para este caso, veríamos que las combinaciones probadas muestran una forma aleatoria, no la rejilla anterior.\n",
    "\n",
    "| ![alt text](img/randomSearch.png \"Random Search\")| \n",
    "|:--:| \n",
    "| **Random Search**: Búsqueda aleatoria entre diferentes combinaciones de valores para dos hiperparámetros. En este ejemplo se evalúan 100 opciones aleatorias diferentes.Los contornos azules indican las regiones con mejores resultados, mientras que los rojos son regiones con peores resultados. Fuente de la imagen [wikipedia](https://commons.wikimedia.org/wiki/File:Hyperparameter_Optimization_using_Random_Search.svg)|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f17f96-1af0-480d-88b2-77d4af703fcb",
   "metadata": {},
   "source": [
    "#### TODO: \n",
    "1. Entrenar y optimizar varios modelos\n",
    "2. Compararlos con un conjunto de validación.\n",
    "3. Seleccionar 1 y testearlo\n",
    "4. Mostrar las métricas finales\n",
    "5. Reordenar los apartados de forma lógica y completar las definiciones y textos\n",
    "6. Añadir ejercicios"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
