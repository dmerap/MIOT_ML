{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ad9bb0b-0d93-4b47-ba79-3343d3b967f9",
   "metadata": {},
   "source": [
    "![alt text](img/MIoT_ML.png \"MIoT_ML\")\n",
    "# Unidad 04  Entrenamiento de Modelos de Aprendizaje Automático\n",
    "\n",
    "El objetivo principal de esta práctica es el conocer los pasos básicos para el desarrollo, optimización y evaluación de los modelos de Aprendizaje Automático. \n",
    "El Notebook contiene varios ejercicios sencillos. Debéis desarrollarlos durante la clase y subirlos al aula virtual.\n",
    "\n",
    "## Referencias útiles para la práctica\n",
    "1. API Pandas: [https://pandas.pydata.org/docs/reference/index.html](https://pandas.pydata.org/docs/reference/index.html)\n",
    "2. API Scikit-learn: [https://scikit-learn.org/stable/api/index.html](https://scikit-learn.org/stable/api/index.html)\n",
    "3. Dataset para el ejercicio: [https://www.kaggle.com/datasets/camnugent/california-housing-prices](https://www.kaggle.com/datasets/camnugent/california-housing-prices)\n",
    "4. Géron, Aurélien. Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow. \" O'Reilly Media, Inc.\", 2022.\n",
    "5. Bergstra, J., & Bengio, Y. (2012). Random search for hyperparameter optimization. Journal of machine learning research, 13 (2). *Para profundizar en la optimización de hiperparámetros*\n",
    "6. Girish Chandrashekar and Ferat Sahin. A survey on feature selection methods. Computers & Electrical Engineering, 40(1):16–28, 2014\n",
    "   \n",
    "## 1. Flujo de trabajo básico en problemas de Aprendizaje Automático (*ML workflow*)\n",
    "A la hora de enfrentarnos a un nuevo problema de Aprendizaje Automático (ML), existen una serie de pasos típicos y comunes que debemos afrontar:\n",
    "1. Entender el problema y su contexto.\n",
    "2. Obtener los datos (histórico).\n",
    "3. Explorar, analizar y entender los datos.\n",
    "4. Preparar los datos para los modelos.\n",
    "5. Seleccionar, optimizar y entrenar los modelos ML.\n",
    "6. Evaluar y presentar el modelo seleccionado.\n",
    "7. Desplegar, monitorizar y mantener la solución.\n",
    "\n",
    "En esta unidad nos centraremos en los pasos 5 y 6 del flujo de trabajo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1570b6b9-50ae-4f9e-a721-1f2c1124db86",
   "metadata": {},
   "source": [
    "## 2. Carga y partición de datos\n",
    "### 2.1. Carga de los datos\n",
    "Cargaremos los datos generados durante la Unidad02, almacenados al finalizar la misma. Suponemos que se han mantenido los nombres generados durante dicha unidad, y que los datos están almacenados en el directorio raíz.\n",
    "\n",
    "- Características de entrada preprocesadas (*Inputs*) de las observaciones del conjunto de entrenamiento (*preprocessing_trainset_inputs.csv*).\n",
    "- Etiquetas de salida (*Outputs*) de las observaciones del conjunto de entrenamiento (*trainset_ouputs.csv*).\n",
    "- Características de entrada preprocesadas (*Inputs*) de las observaciones del conjunto de Test (*preprocessing_testset_inputs.csv*).\n",
    "- Etiquetas de salida (*Outputs*) de las observaciones del conjunto de Test (*testset_outputs.csv*).\n",
    "- Columna de los índices \"*idx*\" (tal como se guardó en la Unidad 02)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98859249-f101-4d2e-bc90-7b4309a4363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas library\n",
    "import pandas as pd\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ImportError as err:\n",
    "    !pip install pandas\n",
    "    import pandas as pd\n",
    "\n",
    "# leemos los datos de training, indicando que la columna que tiene los índices originales es \"idx\", que se mantendrá en los nuevos Dataframes\n",
    "training_inputs = pd.read_csv(\"./preprocessing_trainset_inputs.csv\", index_col=\"idx\")\n",
    "training_outputs = pd.read_csv(\"./trainset_ouputs.csv\", index_col=\"idx\")\n",
    "print(\"Longitud del conjunto de entrenamiento:\", len(training_inputs))\n",
    "\n",
    "# leemos los datos de testing\n",
    "testing_inputs = pd.read_csv(\"./preprocessing_testset_inputs.csv\", index_col=\"idx\")\n",
    "testing_outputs = pd.read_csv(\"./testset_outputs.csv\", index_col=\"idx\")\n",
    "print(\"Longitud del conjunto de test:\", len(testing_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4b145b-40e7-47e6-9bca-9a629e4896a2",
   "metadata": {},
   "source": [
    "### 2.2. Creación del conjunto de validación\n",
    "\n",
    "Si tenemos solo 2 conjuntos para entrenar y para testear, no podemos comparar modelos ni configuraciones de hiperparámetros. **Un error común** es probar o comparar los resultados conseguidos por un modelo o configuración contra el conjunto de Test. **Esto es un error grave**. \n",
    "\n",
    "El conjunto de Test no puede emplearse hasta que todas las decisiones estén ya tomadas y el modelo desarrollado. El conjunto de Test es el que usaremos para obtener las métricas generales, y que consideramos representativas del comportamiento del modelo en el futuro con datos nunca vistos. Si se toma cualquier decisión basada en los resultados de Test, estamos *contaminando* el conjunto y las métricas finales serán excesivamente optimistas, ya que están *mejoradas* al tomar una decisión que las hizo aumentar (no podéis esperar que, al desplegar el modelo, los datos nunca vistos tengan las mismas métricas.\n",
    "\n",
    "\n",
    "Para poder comparar modelos y/o configuraciones, tenemos 2 aproximaciones:\n",
    "1. Generamos un conjunto de validación y lo empleamos para las comparaciones.\n",
    "2. Empleamos la validación cruzada con el conjunto de entrenamiento.\n",
    "\n",
    "\n",
    "Con una de las 2 aproximaciones ya sería suficiente pero, en esta unidad, por motivos docentes, veremos las 2 alternativas en diferentes aspectos.\n",
    "\n",
    "\n",
    "A continuación vamos a **crear un conjunto de validación**. En las unidades anteriores reservamos un 30% de los datos para evaluar y validar el modelo. La distribución típica de los datos al trabajar con 3 conjuntos (train, val y test) es 70%, 15% y 15% repectivamente, por lo que usaremos el 30% reservado previamente para generar 2 subconjuntos que representen el 15% cada uno del total. El 15% del conjunto de Test lo reservaremos hasta el final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fbb504-b200-4845-89f3-dc48a3b8278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "except ImportError as err:\n",
    "    !pip install sklearn\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED=1234\n",
    "\n",
    "# dividimos el conjunto de test original: testing_inputs\n",
    "test_inputs, val_inputs = train_test_split(testing_inputs, test_size=0.5, train_size=0.5, random_state=SEED, shuffle=True)\n",
    "\n",
    "# generamos los outputs correspondientes a los dos conjuntos creados\n",
    "# val_inputs.index es la lista de índices seleccionados para el conjunto de inputs de validación\n",
    "# deben ser los mismos índices para el conjunto de outputs de validación\n",
    "val_outputs = testing_outputs.loc[val_inputs.index]\n",
    "print(\"Longitud del conjunto de validación:\", len(val_outputs))\n",
    "\n",
    "# igualmente para el conjunto de test\n",
    "test_outputs = testing_outputs.loc[test_inputs.index]\n",
    "print(\"Longitud del conjunto de test:\", len(test_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a547424-5e3e-491a-ac68-960abe0df9ce",
   "metadata": {},
   "source": [
    "## 3. Creación y evaluación de un modelo básico\n",
    "\n",
    "### 3.1. Selección de una métrica de evaluación\n",
    "\n",
    "Para medir la calidad del modelo que vamos a desarrollar, necesitamos seleccionar una métrica que nos dé un valor que cuantifique la diferencia entre los datos conocidos del conjunto de test y los datos predichos por el modelo. \n",
    "Una medida de rendimiento típica para los problemas de regresión es el error cuadrático medio (RMSE) que nos da una idea de cuánto error suele cometer el sistema en sus predicciones en las mismas unidades que la variable dependiente, lo que puede facilitar la interpretación de los resultados. \n",
    "\n",
    "$$RMSE=\\large\\sqrt{\\frac{1}{n} \\sum_{i=1}^N{ (\\hat{y}_i-y_i)^2}}$$\n",
    "\n",
    "\n",
    "\n",
    "El RMSE tiene una serie de características que debemos tener en cuenta a la hora de seleccionarlo como, por ejemplo: \n",
    "\n",
    "* Promedia los errores individuales sin tener en cuenta el signo.\n",
    "    * No informa de los sesgos sistemáticos (ej. el modelo tiende a sobrestimar o subestimar).\n",
    "* Penaliza los errores grandes.\n",
    "* Muy sensible a los atípicos (*outliers*).\n",
    "    * Pueden disparar la métricas y dar una percepción inexacta.\n",
    " \n",
    "Existen otras métricas interesantes para regresión que debemos valorar dependiendo de nuestro objetivo y nuestro problema. Por ejemplo, si el número de *outliers* en nuestras manzanas fuese muy alto, quizás nos conveniese emplear una métrica robusta contra los atípicos como el error absoluto medio (MAE):\n",
    "\n",
    "$$\\large MAE=\\frac{1}{N}\\sum_{i=1}^N|y_i-\\hat{y}_i|$$\n",
    "\n",
    "\n",
    "\n",
    "En nuestro caso, vamos a emplear la métrica RMSE (en Scikit-learn está disponible mediante la función \"[*root_mean_squared_error*](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.root_mean_squared_error.html)\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a3f1a8-207d-4a49-ac4b-1a253834ba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3700c60f-3079-4860-a6f7-6ee2a788a3c0",
   "metadata": {},
   "source": [
    "### 3.2. Creación y evaluación de un modelo básico\n",
    "#### 3.2.1. Creación de un modelo de regresión lineal\n",
    "\n",
    "Como dijimos en la unidad anterior, en la vida real normalmente no programaremos un modelo, sino que eligiremos uno de los muchos que hay en las librerías conocidas, como Scikit-learn. En este caso, vamos a elegir un modelo básico de regresión lineal multivariable (varios atributos de entrada), que en Scitkit-learn es implementado por la clase **LinearRegression**,  y llamaremos a su función *fit* pasándole las entradas y los valores reales (del conjunto de entrenamiento) para que entrene el modelo con esos datos y ajuste sus parámetros. El objetivo final es obtener un modelo que sea capaz de predecir sobre datos nunca vistos.\n",
    "\n",
    "**Nota**: el entrenamiento de los modelos se puede añadir pipeline que generamos con las operaciones de preprocesado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3ede68-081e-4a2c-a8b0-b4073f60501d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Creamos un objeto de la clase LinearRegression\n",
    "model_lin_reg = LinearRegression()\n",
    "# Ajustamos (entrenamos) el modelo en base a los datos de entrenamniento (inputs y outputs)\n",
    "model_lin_reg.fit(training_inputs, training_outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3c0f1c-47e7-464b-8d03-0c6e16ac0e90",
   "metadata": {},
   "source": [
    "#### 3.2.2. Evaluación con el dataset de entrenamiento\n",
    "\n",
    "Una vez entrenado el modelo con el conjunto de entrenamiento, podemos ver qué tal es su desempeño sobre el conjunto de entrenamiento (midiéndolo con la métrica RMSE). El grado de acierto sobre las primeras muestras:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d3cd1c-b3f2-49f2-aa01-1d1c5b890490",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# El método predict permite generar predicciones sobre los Inputs pasados como argumento\n",
    "lr_training_predictions = model_lin_reg.predict(training_inputs)\n",
    "\n",
    "print(\"Primeras 5 observaciones del entrenamiento\")\n",
    "for pred,real in zip(lr_training_predictions[:5].round(-2) ,training_outputs.iloc[:5].values):\n",
    "    print(f\"Valor real: {real}, valor predicho: {pred}. Diferencia en absoluto{abs(real-pred)}. Porcentaje desvío: {(abs(real-pred)*100/real).round(2)}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d397f8d7-da9d-4076-b843-a86832d601da",
   "metadata": {},
   "source": [
    "Y el grado de acierto sobre todo el dataset de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7467dd-0e9d-47ba-809f-4cef8ff4533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except ImportError as err:\n",
    "    !pip install matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "# La propia clase nos da un score para el modelo, pasándole las entradas y salidas\n",
    "print(\"Training score:\", model_lin_reg.score(training_inputs, training_outputs))\n",
    "\n",
    "# Calculamos el RMSE sobre todo el conjunto de datos de entrenamiento comparando la realidad con la predicción\n",
    "lr_rmse_train = root_mean_squared_error(training_outputs, lr_training_predictions)\n",
    "print(f\"RMSE del conjunto de entrenamiento con modelo de regresión lineal: {lr_rmse_train:.2f}\")\n",
    "\n",
    "plt.scatter(training_outputs, lr_training_predictions)\n",
    "plt.xlabel(\"Realidad\")\n",
    "plt.ylabel(\"Predicción\")\n",
    "plt.title(\"Regresión lineal - Conjunto de entrenamiento\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d734d6e2-51f2-43cd-b0db-edeb69e8c232",
   "metadata": {},
   "source": [
    "En este caso, los resultados no son muy buenos, y todo indica que este problema no puede modelarse con un modelo tan simple y que requiere de alternativas más complejas.  De cualquier forma, es importante recalcar que las métricas sobre el conjunto de entrenamiento son poco relevantes, ya que son los datos empleados para generar el propio modelo y, típicamente, suelen ser métricas muy optimistas y no pueden considerarse métricas generales. Se considera que el modelo está sobreentrenado con esos datos. Esto lo veréis más claro con el ejemplo siguiente (árbol de decisión binaria). Para obtener métricas de comportamiento general, es necesario evaluar el modelo contra datos nunca vistos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a73c81-5215-4315-8d35-2797cbcd751f",
   "metadata": {},
   "source": [
    "#### 3.2.3. Evaluación con un conjunto de datos no visto previamente\n",
    "\n",
    "Veamos ahora cómo se comporta el modelo con un conjunto de datos que no ha visto durante el entrenamiento. Usaremos para esto el conjunto de validación. Recordad que el conjunto de Test está reservado y no lo emplearemos hasta el final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d49d84-5dbd-49c6-af0d-d5181b90e700",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluación con el dataset de validación para el modelo de regresión lineal\n",
    "\n",
    "# La propia clase nos da un score para el modelo, pasándole las entradas y salidas\n",
    "print(\"Val score:\", model_lin_reg.score(val_inputs, val_outputs))\n",
    "\n",
    "lr_val_predictions = model_lin_reg.predict(val_inputs)\n",
    "lr_rmse_val = root_mean_squared_error(val_outputs, lr_val_predictions)\n",
    "print(f\"RMSE del conjunto de validación con modelo de regresión lineal: {lr_rmse_val:.2f}\")\n",
    "\n",
    "plt.scatter(val_outputs, lr_val_predictions)\n",
    "plt.xlabel(\"Realidad\")\n",
    "plt.ylabel(\"Predicción\")\n",
    "plt.title(\"Regresión lineal - Conjunto de validación\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a1198c-ee5c-4c6c-99e1-7b4b21d0fe95",
   "metadata": {},
   "source": [
    "### 3.3. Creación y evaluación de otro modelo (DecisionTree)\n",
    "\n",
    "Veamos ahora otro modelo de predicción, un árbol de decisión (*decision tree*).\n",
    "\n",
    "#### 3.3.1. Creación del modelo de árbol de decisión\n",
    "\n",
    "Para esta tarea, tenemos la clase **DecisionTreeRegressor** del Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c67395-6142-40f6-9f5f-bc3c3a816ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import tree\n",
    "model_tree = tree.DecisionTreeRegressor(random_state=SEED)\n",
    "model_tree.fit(training_inputs, training_outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9519c9a-f051-4ccd-870e-d8f254d95958",
   "metadata": {},
   "source": [
    "#### 3.3.2. Evaluación con el dataset de entrenamiento\n",
    "\n",
    "Una vez entrenado el modelo con el conjunto de entrenamiento, podemos ver qué tal es su desempeño sobre el conjunto de entrenamiento (midiéndolo con la métrica RMSE). El grado de acierto sobre las primeras muestras, y sobre todo el conjunto de entrenamiento, es:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe963bb-0a8a-4865-a778-153b2bc90110",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# La propia clase nos da un score para el modelo, pasándole las entradas y salidas\n",
    "print(\"Training score:\", model_tree.score(training_inputs, training_outputs))\n",
    "\n",
    "dt_training_predictions = model_tree.predict(training_inputs)\n",
    "\n",
    "print(\"Primeras 5 observaciones del entrenamiento\")\n",
    "for pred,real in zip(dt_training_predictions[:5].round(-2),training_outputs.iloc[:5].values):\n",
    "    print(f\"Valor real: {real}, valor predicho: {pred}. Diferencia en absoluto{abs(real-pred)}. Porcentaje desvío: {(abs(real-pred)*100/real).round(2)}%\")\n",
    "\n",
    "dt_rmse_train = root_mean_squared_error(training_outputs, dt_training_predictions)\n",
    "print(f\"RMSE del conjunto de entrenamiento con modelo de árbol de decisión: {dt_rmse_train:.2f}\")\n",
    "\n",
    "plt.scatter(training_outputs, dt_training_predictions)\n",
    "plt.xlabel(\"Realidad\")\n",
    "plt.ylabel(\"Predicción\")\n",
    "plt.title(\"Árbol de decisión binario - Conjunto de entrenamiento\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b5ebd9-cdaf-4c80-a800-2d0581311d7c",
   "metadata": {},
   "source": [
    "**¿Qué ha pasado? ¿Cómo puede ser que no tenga ningún error?**\n",
    "\n",
    "**EJERCICIO 1 PARA ENTREGAR EN EL AULA VIRTUAL**\n",
    "\n",
    "Explica en una celda de texto qué puede estar pasando para que no exista error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67121e97-976d-42e9-adbc-00694f8dc548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJERCICIO 1\n",
    "# Explica en esta celda de texto que puede estar pasando para que no exista error\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cc2cb2-cf7d-4dd9-874a-850d4906306d",
   "metadata": {},
   "source": [
    "#### 3.3.3. Evaluación con un conjunto de datos no visto previamente\n",
    "\n",
    "Veamos ahora cómo se comporta el modelo con un conjunto de datos que no ha visto durante el entrenamiento, por ejemplo, el conjunto de validación. La gráfica os mostrará la gran diferencia entre probar el modelo con datos usados en el entrenamiento, y con datos nunca vistos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d797236-23f9-4b06-8ed8-9e107a2a89e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# La propia clase nos da un score para el modelo, pasándole las entradas y salidas\n",
    "print(\"Val score:\", model_tree.score(val_inputs, val_outputs))\n",
    "\n",
    "dt_val_predictions = model_tree.predict(val_inputs)\n",
    "\n",
    "dt_rmse_val = root_mean_squared_error(val_outputs, dt_val_predictions)\n",
    "print(f\"RMSE del conjunto de validación con el modelo árbol de decisión: {dt_rmse_val:.2f}\")\n",
    "\n",
    "plt.scatter(val_outputs, dt_val_predictions)\n",
    "plt.xlabel(\"Realidad\")\n",
    "plt.ylabel(\"Predicción\")\n",
    "plt.title(\"Árbol de decisión binario - Conjunto de validación\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0e05e4-97ec-446b-877d-4010feaa3012",
   "metadata": {},
   "source": [
    "#### 3.3.4. Visualización del árbol\n",
    "Una de las ventajas de los modelos basados en árboles es su interpretabilidad. Scikit-learn permite visualizar los árboles de decisión. En el ejemplo siguiente podéis comprobar cómo se puede hacer. Debéis tener en cuenta que el árbol generado tiene una gran profundidad, por lo que, para poder visualizar algo, el ejemplo solo muestra los primeros 2 niveles (parámetro *manx_depth*). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de78afc-6850-47dc-9611-b814038f3732",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25,20))\n",
    "_ = tree.plot_tree(model_tree,\n",
    "                   feature_names=training_inputs.columns.values,\n",
    "                   max_depth=2,\n",
    "                   filled=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf909fc-9e42-45b0-9757-8a1dd2d46bfd",
   "metadata": {},
   "source": [
    "### 3.4. Creación y evaluación de otro modelo (*Random Forest*)\n",
    "\n",
    "Veamos ahora otro modelo de predicción, uno llamado *Random Forest*, que es un *ensemble* (conjunto) de *decision trees*. Cada uno se entrena con un conjunto aleatorio de los datos y de atributos. Al final, se promedia el resultado entre todos los árboles.\n",
    "\n",
    "Para esta tarea, tenemos la clase [RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) del Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e4fce1-13f8-4f02-b87f-e43c31d274c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import numpy as np\n",
    "except ImportError as err:\n",
    "    !pip install numpy\n",
    "    import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#n_estimators es el número de árboles de decisión que componen el ensemble    \n",
    "model_forest = RandomForestRegressor(n_estimators=80, random_state=SEED)\n",
    "\n",
    "# ravel() convierte el segundo parámetro en un array 1D, lo que necesita su fit, DecisionTreeRegressor acepta ambos (1D o 2D)\n",
    "#Esto es necesario realizarlo porque no todos los métodos de Scikit-learn están adaptados para trabajar con DataFrames de Pandas\n",
    "model_forest.fit(training_inputs, np.array(training_outputs).ravel()) \n",
    "rf_training_predictions = model_forest.predict(training_inputs)\n",
    "\n",
    "# La propia clase nos da un score para el modelo, pasándole las entradas y salidas\n",
    "print(\"Training score:\", model_forest.score(training_inputs, training_outputs))\n",
    "\n",
    "print(\"Primeras 5 observaciones del entrenamiento\")\n",
    "for pred,real in zip(rf_training_predictions[:5].round(-2),training_outputs.iloc[:5].values):\n",
    "    print(f\"Valor real: {real}, valor predicho: {pred}. Diferencia en absoluto{abs(real-pred)}. Porcentaje desvío: {(abs(real-pred)*100/real).round(2)}%\")\n",
    "\n",
    "rf_rmse_train = root_mean_squared_error(training_outputs, rf_training_predictions)\n",
    "print(f\"RMSE del conjunto de entrenamiento con modelo de random forest: {rf_rmse_train:.2f}\")\n",
    "\n",
    "# La propia clase nos da un score para el modelo, pasándole las entradas y salidas\n",
    "print(\"\\nVal score:\", model_forest.score(val_inputs, val_outputs))\n",
    "\n",
    "rf_val_predictions = model_forest.predict(val_inputs)\n",
    "rf_rmse_val = root_mean_squared_error(val_outputs, rf_val_predictions)\n",
    "print(f\"RMSE del conjunto de validación con el modelo random forest: {rf_rmse_val:.2f}\")\n",
    "\n",
    "plt.scatter(val_outputs, rf_val_predictions)\n",
    "plt.xlabel(\"Realidad\")\n",
    "plt.ylabel(\"Predicción\")\n",
    "plt.title(\"Random forest - Conjunto de validación\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5064ea78-07c4-47d9-b53a-63f7a5427e63",
   "metadata": {},
   "source": [
    "**Conclusión:** Como vemos, el modelo *Random Forest* da mejores resultados sobre el conjunto de validación.\n",
    "\n",
    "Al comparar los modelos sobre un conjunto de validación, podríamos seleccionar el que obtiene mejores métricas y evaluarlo contra el conjunto de Test para obtener la métrica general. **Importante**: solo podríamos evaluarlo contra el conjunto de Test si no fuésemos a comparar o probar más configuraciones. Mientras no hayamos tomado todas las decisiones sobre el modelo, el conjunto de Test seguirá reservado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebe29b2-8a36-4b3d-8d2c-86b6797bbc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Resultados de validación (valores inferiores son mejores): \")\n",
    "print(f\"RSME LR = { lr_rmse_val:.2f}.  RSME DT = {dt_rmse_val:.2f}.     RSME RF = {rf_rmse_val:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e917c7-daaf-4fe9-8500-8525bbead321",
   "metadata": {},
   "source": [
    "## 4. Validación cruzada (*Cross Validation*)\n",
    "\n",
    "\n",
    "La validación cruzada es una técnica que nos permite dividir un conjunto en varias partes (*folds*), por ejemplo K, y realizar K entrenamientos con sus correspondientes evaluaciones. Cada uno de los entrenamientos emplea todas las particiones menos una (K-1), y emplea la parte restante para evaluar. En cada entrenamiento se cambia la partición empleada para evaluar. Finalmente se estudia la lista de resultados (evaluaciones), y se asigna un resultado del modelo de alguna forma (por ejemplo, haciendo la media).\n",
    "\n",
    "El algoritmo se puede descomponer en los siguientes pasos:\n",
    "\n",
    "1. Se divide el conjunto en ‘K’ partes (*folds*) iguales.\n",
    "2. Se reserva una de las partes para evaluar.\n",
    "3. Las otras K-1 partes se usan para entrenar.\n",
    "4. Se evalúa el rendimiento del modelo en el *fold* reservado.\n",
    "5. Se repiten los pasos 2, 3 y 4 ‘K’ veces variando en cada iteración el\n",
    "*fold* de evaluación.\n",
    "\n",
    "\n",
    "La validación cruzada nos permite *simular* dos conjuntos cuando solo tenemos uno. Se aplica habitualmente cuando el número de observaciones es pequeño y no tenemos un dataset suficientemente grande como para dividirlo en subconjuntos suficientemente significativos.\n",
    "\n",
    "Si solo tenemos un dataset y aplicamos validación cruzada, podemos emplear el mismo dataset para entrenar y para testear. Si tenemos 2 subconjuntos (entrenamiento y test), podemos emplear la validación cruzada con el entrenamiento para entrenar y validar.\n",
    "\n",
    "Por supuesto, es posible hacerlo a mano, pero, afortunadamente, Scikit-learn nos ofrece herramientas para hacerlo de forma automática (la función \"[cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)\"). Es importante también destacar que muchas de las funciones de preprocesado y optimización de Scikit-learn tienen la posibilidad de evaluarse a través de la validación cruzada.\n",
    "\n",
    "Veamos ahora un ejemplo de cómo probar un modelo de los entrenados anteriormente con validación cruzada empleando [cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0bf9e1-5b1d-4d58-b3fc-cd4b23965ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Empleamos validación cruzada con la regresión lineal. 5 folds\n",
    "scores = cross_val_score(model_lin_reg, training_inputs, training_outputs, cv=5, scoring='neg_root_mean_squared_error')\n",
    "# Según la documentación: All scorer objects follow the convention that higher return values are better than lower return values.\n",
    "# Esa es la razón de que el RMSE venga en negativo. Lo pasamos a positivo para la correcta interpretación\n",
    "scores*=-1\n",
    "\n",
    "\n",
    "print(\"Resultados de la validación cruzada aleatoria estratificada para cada fold:\")\n",
    "for i, resultado in enumerate(scores):\n",
    "    print(f\"Fold {i}:{resultado:.2f}\")\n",
    "\n",
    "print(f\"Media={scores.mean():.2f}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf047bd-01e8-4c0a-8de7-5c759a79a6ec",
   "metadata": {},
   "source": [
    "En el ejemplo anterior elegimos realizar una validación cruzada con 5 particiones (*folds*), pero, por defecto, las particiones se realizan siguiendo el orden secuencial de las observaciones, sin aleatoriedad ni estratificación. Si queremos hacerlo de forma aleatoria en el atributo de salida, podemos emplear la clase [KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) para crear un objeto y dárselo al parámetro *cv*. Y si queremos hacerlo de forma estratificada sobre la variable a predecir (sólo aplicable en caso de ser una variable categórica), emplearemos la clase [StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b06927-a8d4-41c3-9501-7ffc619a11d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "skf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "scores = cross_val_score(model_lin_reg, training_inputs, training_outputs, cv=skf, scoring='neg_root_mean_squared_error')\n",
    "scores*=-1\n",
    "\n",
    "print(\"Resultados de la validación cruzada aleatoria estratificada para cada fold:\")\n",
    "for i, resultado in enumerate(scores):\n",
    "    print(f\"Fold {i}:{resultado:.2f}\")\n",
    "\n",
    "print(f\"Media={scores.mean():.2f}\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3ba378-b902-420e-8e6d-acbb52a60afe",
   "metadata": {},
   "source": [
    "**Nota:** Hasta ahora, simplemente hemos entrenado el modelo directamente, sin intentar optimizarlo. Existen diferentes técnicas que podemos emplear para intentar mejorar las métricas del modelo, entre ellas podemos destacar:\n",
    "* Técnicas para la reducción de la dimensionalidad.\n",
    "* Técnicas para la optimización de los hiperparámetros de los modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ae9e2e-7f5a-40fc-8cf4-31598cabbef9",
   "metadata": {},
   "source": [
    "## 5. Reducción de la dimensionalidad\n",
    "\n",
    "\n",
    "\n",
    "Trabajar con muchos atributos (dimensiones) puede parecer lo adecuado cuando tenemos que generar modelos y buscamos un alto rendimiento. Pero la realidad es que, cuando aumenta la dimensionalidad, el volumen del espacio de características aumenta exponencialmente, haciendo que los datos disponibles estén muy dispersos, lo que dificultará el trabajo de los modelos y que estos converjan y, como resultado, su rendimiento no será bueno. Es la llamada **maldición de la dimensionalidad**. \n",
    "\n",
    "El volumen y variabilidad de datos que necesitamos para entrenar en entornos de alta dimensionalidad es muy alto, y esto no siempre es fácil de obtener, por lo que, habitualmente, los modelos se ven beneficiados si reducimos la dimensionalidad. \n",
    "\n",
    "### 5.1. Selección de características\n",
    "\n",
    "Una posibilidad para reducir la dimensionalidad es aplicar técnicas de selección de las características (*Feature Selection*, FS), es decir, eliminar las características que proporcionen poca o ninguna información al modelo y escoger las más relevantes. La aplicación de estas técnicas tiene como consecuencia directa reducir las dimensiones, e, indirectamente, nos proporciona información sobre el problema con el que estamos trabajando (nos muestra las variables más relevantes). Las técnicas de selección de características se engloban en 3 grandes grupos: Filtros, Embebidos y Wrappers.\n",
    "\n",
    "**Nota**: *encontraréis una explicación más detallada de este apartado en la teoría de la materia*.\n",
    "\n",
    "#### 5.1.1. Filtros\n",
    "Técnicas de preprocesado que crean listas ordenadas de las características en base a algún tipo de métrica. El objetivo es eliminar las características con peores posiciones en el ranking (es necesario establecer un umbral).\n",
    "* Son métodos simples y rápidos.\n",
    "* Son robustos al sobreentrenamiento.\n",
    "* Son fácilmente interpretables.\n",
    "* Asignar el umbral no es sencillo.\n",
    "* No valoran la combinación de variables (puede que dos variables independientes no aporten información al modelo, pero combinadas sí).\n",
    "* Variables altamente correladas y redundantes no se eliminan (si dos variables tienen una muy alta correlación entre ellas, es decir, representan lo mismo, es probable que no tenga sentido mantener las dos).\n",
    "\n",
    "\n",
    "Scikit-learn proporcina varios métodos de filtrado para poder aplicar sobre nuestros datos. Dos posibles estrategias una vez aplicado el método es:\n",
    "1. Seleccionar las 'k' características mejor posicionadas en el ranking.\n",
    "2. Seleccionar un porcentaje de las mejores características según el orden del ranking.\n",
    "\n",
    "Dentro de los métodos de filtrado, uno de los más comunes es el filtrado a través de la correlación de Pearson. Veamos un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0b7ab5-18a0-4b81-aa8c-8751bc143707",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ejemplos con filtros empleando Scikit-learn\n",
    "from sklearn.feature_selection import SelectKBest, r_regression # r_regression es una función que mide la correlación Pearson\n",
    "\n",
    "def plot_selected_features(features, scores, title=None, err=None):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel(\"Características\")\n",
    "    plt.ylabel(\"Importancia\")\n",
    "    plt.title(title)\n",
    "    plt.bar(features,scores,orientation='vertical', yerr=err )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Seleccionamos las 'K' mejores características\n",
    "K=10\n",
    "\n",
    "# creamos un K-selector que usa la correlación Pearson para ordenar las características\n",
    "f_selector = SelectKBest(r_regression, k=K)\n",
    "\n",
    "# calculamos la correlación entre las variables de entrada y la salida, y seleccionamos las de mayor correlación Pearson\n",
    "# Necesitamos convertir el dataframe \"training_outputs\" en un array de 1D. training_outputs.values.ravel()\n",
    "# training_outputs.values devuelve los valores en forma de array multidimensional (2D)\n",
    "# training_outputs.values.ravel() \"aplana\" el array (lo convierte a 1D)\n",
    "f_selector.fit(training_inputs, training_outputs.values.ravel())\n",
    "\n",
    "\n",
    "print(\"Todas las características:\\n\"+\"\\n\".join(training_inputs.columns.values))\n",
    "\n",
    "indices_fs = f_selector.get_support(indices=True)\n",
    "print(f\"Índices de las {K} características seleccionadas:\", indices_fs, \"\\n\")\n",
    "\n",
    "selected_fs = f_selector.get_feature_names_out(training_inputs.columns)\n",
    "print(f\"Nombres de las {K} características seleccionadas:\\n\"+ \"\\n\".join(selected_fs))\n",
    "\n",
    "scores = f_selector.scores_\n",
    "print(\"Todos los scores:\\n\", scores, \"\\n\")\n",
    "\n",
    "selected_fs_tuplas = [(training_inputs.columns[i], scores[i]) for i in indices_fs]\n",
    "\n",
    "print(f\"Las {K} características con mayor correlación son:\")\n",
    "for t in sorted(selected_fs_tuplas, key = lambda t: t[1], reverse=True):  # ordenamos por score\n",
    "    print(f\"{t[0]}= {t[1]:.3f}\")\n",
    "\n",
    "# dibujamos los scores obtenidos para cada característica de entrada en su correlación con la salida\n",
    "plot_selected_features(training_inputs.columns.values, f_selector.scores_, title=\"FS. Filtro correlación Pearson\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab671925-b9ea-49c6-a803-a8ef79a1ff19",
   "metadata": {},
   "source": [
    "**Fijaos** que en el ejemplo anterior quizás no estemos seleccionando las que nos interesan, porque solo estamos **seleccionando las de mayor correlación positiva**. La correlación Pearson también nos da información interesante con la correlación negativa. Podemos modificar el código anterior para que tenga en cuenta el valor absoluto. Comprobaréis en el resultado que nos selecciona otras distintas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ca3dc1-1846-41c0-b87e-6294b061541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_r_regression(inputs, outputs):\n",
    "    return np.abs(r_regression(inputs, outputs))  \n",
    "\n",
    "# Seleccionamos las 'K' mejores características\n",
    "K=10\n",
    "\n",
    "# creamos un K-selector que usa el valor absoluto de la correlación Pearson \n",
    "f_selector = SelectKBest(abs_r_regression, k=K)\n",
    "\n",
    "# calculamos la correlación entre las variables de entrada y la salida, y seleccionamos las de mayor correlación Pearson\n",
    "f_selector.fit(training_inputs, training_outputs.values.ravel())\n",
    "\n",
    "#get_support(indices=True) devuelve los índices de las características seleccionadas\n",
    "indices_fs = f_selector.get_support(indices=True)\n",
    "#get_feature_names_out devuelve los nombres las características seleccionadas\n",
    "selected_fs = f_selector.get_feature_names_out(training_inputs.columns)\n",
    "#scores_ devuelve la puntuación de cada característica\n",
    "scores = f_selector.scores_\n",
    "\n",
    "selected_fs_tuplas = [(training_inputs.columns[i], scores[i]) for i in indices_fs]\n",
    "\n",
    "print(f\"Las {K} características con mayor correlación absoluta son:\")\n",
    "for t in sorted(selected_fs_tuplas, key = lambda t: t[1], reverse=True):\n",
    "    print(f\"{t[0]}= {t[1]:.3f}\")\n",
    "\n",
    "# dubujamos los scores obtenidos por cada característica de entrada en su correlación con la salida\n",
    "plot_selected_features(training_inputs.columns.values, f_selector.scores_, title=\"FS. Filtro correlación Pearson (ABS)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e5178c-3bbf-4b79-9c01-6a1bf6b349df",
   "metadata": {},
   "source": [
    "#### 5.1.2. Embebidos\n",
    "Los métodos de selección de características embebidos incorporan la selección de características en el propio proceso de entrenamiento de un modelo de ML. Hay dos tipos principales:\n",
    "1. **Aproximaciones basadas en regularización** (ej. Lasso-L1). Son algoritmos que tratan de minimizar una función de coste en la que participan términos de regularización. En el proceso de construcción del modelo se eliminan las características cuyo peso sea 0 ($\\beta_j = 0$).\n",
    "\n",
    "$$\\large F_{coste}=\\sum_{i=1}^N(y_i-\\sum(x_{ij}\\beta_j))^2 + \\lambda\\sum_{j=1}^P|\\beta_j|$$\n",
    "$$\\large L_1=\\lambda\\sum_{j=1}^P|\\beta_j|$$ \n",
    "\n",
    "\n",
    "**Importante**: con este método solo podríamos eliminar las características que sean 0. Aumentar el parámetro $\\lambda$ tiene como consecuencia que el modelo intente dar menos importancia a cada una de las características y, en consecuencia, puede asignar 0 a algunas de estas.\n",
    "\n",
    "En el ejemplo siguiente podéis jugar con el parámetro *alpha* que representa a $\\lambda$ en la ecuación. Si ponéis un valor igual a 1, *eliminará* alguna característica.\n",
    "\n",
    "Entrenando el modelo podéis analizar los coeficientes asociados a cada característica para poder descartar los que son 0, pero Scikit-learn proporciona una clase ([SelectFromModel](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html)) que nos permite seleccionar las características relevantes de forma automática.\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d2527c-9848-4986-bdd6-de945c1631ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ejemplo embebido basado en regularización\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn import linear_model\n",
    "\n",
    "# Modelo de regularización\n",
    "\n",
    "lasso_model = linear_model.Lasso(alpha=0.1, max_iter=1000, random_state=SEED)\n",
    "f_selector = SelectFromModel(estimator=lasso_model)\n",
    "f_selector.fit(training_inputs,training_outputs)\n",
    "selected_fs = f_selector.get_feature_names_out(training_inputs.columns)\n",
    "\n",
    "print(f\"Características originales: {len(training_inputs.columns)}. \\n Características seleccionadas: {len(selected_fs)}\")\n",
    "\n",
    "print(\"Características seleccionadas por el método de regularización: \\n\"+\" \\n\".join(selected_fs))\n",
    "\n",
    "\n",
    "plot_selected_features(training_inputs.columns.values, f_selector.estimator_.coef_, title=\"FS. Embebido modelo con regularización\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df1a25f-81d2-438e-a6a4-de8fa4cebf79",
   "metadata": {},
   "source": [
    "       \n",
    "2. **Aproximaciones basadas en la información extraída con la construcción del modelo**. El ejemplo más claro son los **modelos\n",
    "basados en árboles**, en los que la propia construcción del árbol otorga la importancia a cada de las características:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fc63c1-b4db-4626-a7c8-f8391fe7ec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# n_estimators es el número de árboles en el ensemble\n",
    "model = RandomForestRegressor(n_estimators=50)\n",
    "\n",
    "# training_outputs.values.ravel() no todos los métodos soportan un DataFrame\n",
    "model.fit(training_inputs, training_outputs.values.ravel())\n",
    "\n",
    "importances = model.feature_importances_ # importancia general de los atributos en el RandomForest\n",
    "std = np.std([importances for tree in model.estimators_],axis=0) # Podemos calcular la importancia para cada árbol y con ello la std\n",
    "\n",
    "# argsort devuelve analiza un array devuelve un array con los índices (posición de los elementos) ordenados de menor a mayor\n",
    "# [::-1] invierte un array\n",
    "print(\"Características ordenadas de mayor a menor importancia\")\n",
    "for idx in np.argsort(importances)[::-1]:\n",
    "    print(f\"{training_inputs.columns[idx]}: {importances[idx]:.3f}\")\n",
    "\n",
    "plot_selected_features(training_inputs.columns.values, importances, title=\"FS. Embebido RandomForest\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11947e5-0f03-443c-b82a-1a6ddd5e1f1d",
   "metadata": {},
   "source": [
    "#### 5.1.3. Wrappers\n",
    "\n",
    "Los *wrappers* son algoritmos de selección de características que integran un predictor durante el proceso de selección. Se prueban diferentes combinaciones de atributos para entrenar un modelo, y el subconjunto con mejores resultados es el seleccionado. Es inviable computacionalmente probar todas las posibles combinaciones, por lo que es necesario establecer un método de búsqueda en el espacio de características que genere los subconjuntos.\n",
    "* Son simples, universales, tienen buen rendimiento (el modelo resultante), PERO:\n",
    "    * Son computacionalmente muy costosos.\n",
    "    * Dependen en gran medida del modelo seleccionado.\n",
    "    * Pueden caer en el sobreentrenamiento.\n",
    "    * Es difícil configurar los modelos de forma justa (agregar el proceso de optimización de hiperparámetros produciría una sobrecarga muy importante).\n",
    "    * Cada método para seleccionar subconjuntos tiene problemáticas asociadas que es necesario valorar.\n",
    "\n",
    "Dos de los métodos más sencillos para establecer los conjuntos son:\n",
    "1. **Sequential forward selection (SFS) algorithm**:\n",
    "    1. Empezamos con un conjunto vacío FS_DATASET.\n",
    "    2. Añadimos a FS_DATASET la característica X que maximiza el rendimiento del modelo.\n",
    "    3. Repetimos el paso 'B' hasta que se cumple la condición de salida. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d097bbf-f856-494e-b8e4-0dacac381cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ejemplos wrappers\n",
    "\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn import tree\n",
    "\n",
    "# Features to select\n",
    "K=10\n",
    "model = tree.DecisionTreeRegressor()#modelo que usaremos para evaluar la bondad del subconjunto de características\n",
    "\n",
    "sfs = SequentialFeatureSelector(model, n_features_to_select=K, direction='forward')\n",
    "\n",
    "sfs.fit(training_inputs, training_outputs)\n",
    "selected_features = sfs.get_feature_names_out(training_inputs.columns)\n",
    "print(\"Características seleccionadas empleando SFS:\\n\" + \"\\n\".join(selected_features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4004b4-2de8-43eb-874b-08f5e9a58bb6",
   "metadata": {},
   "source": [
    "2. **Sequential backward selection (SBS) algorithm**:\n",
    "    1. Empezamos con un conjunto completo FS_DATASET (con todas las posibles características).\n",
    "    2. Eliminamos de FS_DATASET la característica X que menos reduce el rendimiento del predictor (la más irrelevante).\n",
    "    3. Repetimos el paso 'B' hasta que se cumple la condición de salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55618645-dcf9-4cee-81ce-2ab93edbb495",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ejemplos wrappers\n",
    "\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn import tree\n",
    "\n",
    "# Features to select\n",
    "K=10\n",
    "model = tree.DecisionTreeRegressor()#modelo que usaremos para evaluar la bondad del subconjunto de características\n",
    "\n",
    "sfs = SequentialFeatureSelector(model, n_features_to_select=K, direction='backward')\n",
    "\n",
    "sfs.fit(training_inputs, training_outputs)\n",
    "selected_features = sfs.get_feature_names_out(training_inputs.columns)\n",
    "print(\"Características seleccionadas empleando SBS:\\n\" + \"\\n\".join(selected_features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb863131-50ef-4be7-9f24-fbf9e51161c8",
   "metadata": {},
   "source": [
    "### 5.2. Reducción de dimensionalidad a través de proyecciones\n",
    "\n",
    "Este tipo de métodos generan un nuevo y más pequeño conjunto características (reduciendo la dimensionalidad), cada una de las cuales es una combinación de las variables de entrada. **No generan una selección de características**, sino que se realiza una transformación.\n",
    "\n",
    "Uno de los métodos más conocidos es el análisis de componentes principales.\n",
    "\n",
    "#### 5.2.1. Principal Component Analisys (PCA)\n",
    "El PCA permite reducir la dimensionalidad (características) perdiendo la menor cantidad de información (varianza). El objetivo es reducir un número elevado de características, posiblemente correlacionadas (información redundante), a un número menor de variables transformadas (componentes principales) que explique gran parte de la variabilidad de los datos. \n",
    "* Cada componente principal será una combinación lineal de las variables originales.\n",
    "* Las componentes principales serán independientes (no correlacionadas entre sí).\n",
    "* Los componentes se ordenan por la cantidad de varianza original que describen.\n",
    "* Podemos seleccionar las ‘n’ componentes principales que explican un alto porcentaje de varianza (ej. 95%) y, con ello, reducir la dimensionalidad.\n",
    "\n",
    "Scikit-learn tiene la clase [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) que permite aplicar este método a nuestros datos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08b874b-a1a8-4780-8fa2-b48b6b059c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# n_components, si ponemos un valor entero 'x', seleccionará las 'x' componentes principales\n",
    "# n_components, si ponemos un valor flotante 'y' entre 0 y 1 , seleccionará las componentes principales que expliquen el 'y'% de la varianza\n",
    "porcentaje_varianza = 0.99\n",
    "pca = PCA(n_components=porcentaje_varianza)\n",
    "training_inputs_pca = pca.fit_transform(training_inputs)\n",
    "\n",
    "print(f\"Número de características originales: {len(training_inputs.columns)}\")\n",
    "print(f\"Número de componentes principales seleccionadas: {training_inputs_pca.shape[1]}\")\n",
    "print(f\"Se redujo la dimensionalidad en {len(training_inputs.columns)-training_inputs_pca.shape[1]} dimensiones\")\n",
    "\n",
    "print(\"\\nVarianza explicada por cada una de las componentes principales: \")\n",
    "varianza_acumulada=0\n",
    "for i, valor in enumerate(pca.explained_variance_ratio_, start=1):\n",
    "    varianza_acumulada+=valor\n",
    "    print(f\"Componente principal {i}: {valor:.3f}. Varianza acumulada: {varianza_acumulada:.3f}\")\n",
    "\n",
    "\n",
    "# Las componentes principales son una combinación lineal de las características originales. \n",
    "# Veamos como se forma la componente principal 1\n",
    "first_pc = pca.components_[0] \n",
    "equation = \")+(\".join([f\"{coef:.4f} * {name}\" for coef, name in zip(first_pc, training_inputs.columns.values)])\n",
    "print(f\"\\nLa primera componente principal se forma como:\\nPC1 = ({equation})\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6008b51-bf63-489e-bafd-c0a7517aff0f",
   "metadata": {},
   "source": [
    "### 5.3. Seleccionar las características para nuestro modelo\n",
    "\n",
    "Tanto los métodos de selección de características, como los de reducción de la dimensionalidad a través de proyecciones (ej. PCA) nos proporcionan conjuntos de datos con menor dimensionalidad. Cada uno de ellos genera una respuesta diferente. **¿Cuál escoger?** A priori, no existe una respuesta correcta, es decir, la única opción sería probar los resultados de cada uno de estos métodos con el conjunto de validación, es decir, entrenar el modelo con cada una de estas alternativas (ej. el subconjunto de características generado por el filtro de correlación), y evaluar el modelo resultante contra el conjunto de validación. Una vez hayamos evaluado todas las alternativas, podríamos comparar las métricas resultantes y seleccionar el modelo (con la configuración asociada) que tuvo mejores valores.\n",
    "\n",
    "El siguiente ejemplo muestra cómo entrenar el modelo con el resultado del PCA, y cómo evaluarlo con el conjunto de validación:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a155cbca-2b4a-4aef-8dc0-e0340021c9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "model_forest = RandomForestRegressor(n_estimators=80, random_state=SEED)\n",
    "# ravel() convierte el segundo parámetro en un array 1D, lo que necesita su fit, DecisionTreeRegressor acepta ambos (1D o 2D)\n",
    "model_forest.fit(training_inputs_pca, np.array(training_outputs).ravel()) \n",
    "\n",
    "# La propia clase nos da un score para el modelo, pasándole las entradas y salidas\n",
    "print(\"Training score:\", model_forest.score(training_inputs_pca, training_outputs))\n",
    "\n",
    "rf_training_predictions_pca = model_forest.predict(training_inputs_pca)\n",
    "\n",
    "print(\"Primeras 5 observaciones del entrenamiento\")\n",
    "for pred, real in zip(rf_training_predictions_pca[:5].round(-2), training_outputs.iloc[:5].values):\n",
    "    print(f\"Valor real: {real}, valor predicho: {pred}. Diferencia en absoluto{abs(real-pred)}. Porcentaje desvío: {(abs(real-pred)*100/real).round(2)}%\")\n",
    "\n",
    "rf_rmse_train = root_mean_squared_error(training_outputs, rf_training_predictions_pca)\n",
    "print(f\"RMSE del conjunto de entrenamiento con modelo de random forest: {rf_rmse_train:.2f}\")\n",
    "\n",
    "val_inputs_pca = pca.transform(val_inputs)\n",
    "\n",
    "# La propia clase nos da un score para el modelo, pasándole las entradas y salidas\n",
    "print(\"\\nVal score:\", model_forest.score(val_inputs_pca, val_outputs))\n",
    "\n",
    "rf_val_predictions_pca = model_forest.predict(val_inputs_pca)\n",
    "rf_rmse_val_pca = root_mean_squared_error(val_outputs, rf_val_predictions_pca)\n",
    "print(f\"RMSE del conjunto de validación con el modelo random forest CON PCA: {rf_rmse_val_pca:.2f}\")\n",
    "\n",
    "plt.scatter(val_outputs, rf_val_predictions_pca)\n",
    "plt.xlabel(\"Realidad\")\n",
    "plt.ylabel(\"Predicción\")\n",
    "plt.title(\"Random forest - Conjunto de validación\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4d1ac7-f324-4ac8-b2bf-c3c026c87a43",
   "metadata": {},
   "source": [
    "En este caso concreto el uso del PCA no parece mejorar el entrenamiento respecto a emplear todas las características. Habría que probar también con el resultado de las diferentes técnicas de selección de características vistas anteriormente y compararlas. \n",
    "\n",
    "**Importante**: Las técnicas de reducción de dimensionalidad funcionan mejor si tenemos una gran número de dimensiones en las que existen características poco o nada relevantes o existen características redundantes. Nuestro problema de ejemplo no tiene una gran dimensionalidad y por ello no estará demasiado afectado por la *maldición de la dimensionalidad*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad9f866-fbaa-4d72-8992-dd975ef1ca59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RMSE del conjunto de validación con el modelo random forest CON PCA: {rf_rmse_val_pca:.2f}\")\n",
    "print(f\"RMSE del conjunto de validación con el modelo random forest SIN usar PCA: {rf_rmse_val:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b427465d-4627-443f-97a7-24e4a507a5c2",
   "metadata": {},
   "source": [
    "**EJERCICIO 2 PARA ENTREGAR EN EL AULA VIRTUAL**\n",
    "\n",
    "Utiliza el subconjunto de características generado por alguno de los métodos de selección de características vistos anteriormente (filtros, embebidos o wrappers) para entrenar un modelo y validarlo con el conjunto de validación. Compara las métricas con la salida del PCA, y con no aplicar la reducción de dimensionalidad.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5c695d-9cb3-4bdf-a918-01fe5a918046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJERCICIO 2\n",
    "# Utiliza el subconjunto de características generado por alguno de los métodos de selección de características \n",
    "# vistos anteriormente (filtros, embebidos o wrappers) para entrenar un modelo y validarlo con el conjunto de validación. \n",
    "# Compara las métricas con la salida del PCA y con no aplicar la reducción de dimensionalidad.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647e2ee9-cf0c-4706-aae6-9c99743ab280",
   "metadata": {},
   "source": [
    "## 6. Optimización de hiperparámetros\n",
    "Los algoritmos de ML tienen una serie de parámetros de configuración que afectan al entrenamiento del modelo y posteriormente al desempeño de éste (ej. *learning rate* en regresión lineal, número de estimadores en *Random Forest*, etc.). Los parámetros por defecto raras veces son los óptimos, y es necesario optimizarlos para cada problema concreto. Este es un proceso muy costoso computacionalmente, y que requiere muchas veces de recursos dedicados. Existen diferentes técnicas para realizar la optimización (ej. GRID, aleatoria, evolutiva, etc.). La más básica es la optimización basada en una búsqueda GRID.\n",
    "\n",
    "### 6.1. Grid Search\n",
    "\n",
    "La optimización GRID es la prueba exhaustiva de todas las combinaciones posibles de varios parámetros (Scikit-learn nos proporciona la clase **GridSearchCV** para esta tarea). Para aplicarla, lo primero que necesitamos es establecer los rangos de valores que queremos probar. La prueba exhaustiva es el equivalente a probar todas las combinaciones empleando bucles “for” anidados. Las evaluaciones de hiperparámetros se hacen y se comparan a través de un conjunto de validación (o empleando validación cruzada), pero nunca contra el conjunto de Test).\n",
    "\n",
    "El siguiente es un ejemplo de GridSearchCV sobre el modelo de árbol de decisión, donde exploramos:\n",
    "- Dos valores para el hiperparámetro *criterion*, empleado para decidir la forma de creación de nodos al constuir el árbol.\n",
    "- Cuatro valores para *max_features*, donde se indican los porcentajes de atributos que se incluirán en la creación del árbol (los atributos se eligen al azar).\n",
    "\n",
    "\n",
    "Emplearemos la validación cruzada con los datos de entrenamiento para seleccionar los hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e35d29-a0c1-4981-8520-4dfcf8814379",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# estos son los parámetros a analizar y los valores de cada uno a explorar\n",
    "param_grid = {\n",
    "    'criterion': [\"squared_error\", \"absolute_error\"], # para decidir la creación de nodos al constuir el árbol\n",
    "    'max_features': [0.25, 0.5, 0.75, 1.0]  # porcentajes de las features incluidas\n",
    "}\n",
    "\n",
    "\n",
    "# buscaremos los mejores parámetros para el modelo de árbol de decisión (model_tree)\n",
    "# cv puede ser un número fijo, un KFold, o una lista de tuplas (train,test) de índices POSICIONALES a probar \n",
    "# Por simplificación emplaremos un CV=5\n",
    "grid_search = GridSearchCV(estimator=model_tree, param_grid=param_grid, \n",
    "                           scoring=\"neg_root_mean_squared_error\", # criterio para seleccionar la mejor combinación de parámetros\n",
    "                           return_train_score=True,\n",
    "                           cv=5)\n",
    "\n",
    "\n",
    "\n",
    "results = grid_search.fit(training_inputs, training_outputs)\n",
    "\n",
    "print(f\"Mejor score en validación: {-grid_search.best_score_:.2f}\")\n",
    "print(f\"Params del mejor score en validación: {grid_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b56feb-efc7-4706-a984-d493c92d8b50",
   "metadata": {},
   "source": [
    "Y, a continuación, otro ejemplo de GridSearchCV con el modelo *Random Forest*, donde exploramos:\n",
    "- 4 valores para el hiperparámetro *max_features*, el número de atributos usados al construir cada árbol del modelo.\n",
    "- 5 para *n_estimators*, el número de árboles usados en el modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0d70a9-5557-4fc7-a06a-6285c4cc31aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ahora con Random Forest\n",
    "# estos son los parámetros a analizar y los valores de cada uno a explorar\n",
    "param_grid = {\n",
    "    'max_features': [2,4,6,8],\n",
    "    'n_estimators': [70,80,90,100,110]  \n",
    "}\n",
    "\n",
    "# buscaremos los mejores parámetros para el modelo de árbol de decisión (model_tree)\n",
    "# cv puede ser un número fijo, un KFold, o una lista de tuplas (train,test) de índices POSICIONALES a probar \n",
    "grid_search = GridSearchCV(estimator=model_forest, param_grid=param_grid, \n",
    "                           scoring=\"neg_root_mean_squared_error\", # criterio para seleccionar la mejor combinación de parámetros\n",
    "                           return_train_score=True,\n",
    "                           cv=5)\n",
    "\n",
    "\n",
    "results = grid_search.fit(training_inputs, np.array(training_outputs).ravel())\n",
    "\n",
    "print(f\"Mejor score en validación: {-grid_search.best_score_}\")\n",
    "print(f\"Params del mejor score en validación: {grid_search.best_params_}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d66b47c-b237-4ccf-9ba8-4a00844febc3",
   "metadata": {},
   "source": [
    "El mejor resultado puede variar de ejecución en ejecución, ya que hay cierta aleatoriedad en los conjuntos que procesa cada árbol.\n",
    "\n",
    "Si dibujásemos los resultados (información contenida en *results* en el anterior código), veríamos algo como la siguiente figura, que muestra que la evaluación se realiza para todas las combinaciones, y las zonas donde se producen mejores y peores modelos (la figura no tiene relación con los datos, es sólo un ejemplo).\n",
    "\n",
    "\n",
    "| ![alt text](img/gridSearch.png \"Grid Search\") | \n",
    "|:--:| \n",
    "| **Grid Search**: Búsqueda a través de diferentes valores de dos hiperparámetros. Para cada hiperparámetro se consideran 10 valores diferentes (100 combinaciones distintas). Los contornos azules indican las regiones con mejores resultados, mientras que los rojos son regiones con peores resultados. Fuente de la imagen [wikipedia](https://es.m.wikipedia.org/wiki/Archivo:Hyperparameter_Optimization_using_Grid_Search.svg)|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eefe89-95bd-413b-b639-ca1fcad2e38b",
   "metadata": {},
   "source": [
    "### 6.2. Random Search\n",
    "\n",
    "\n",
    "En este caso, realizamos una búsqueda aleatoria (no completa) en el espacio de combinaciones de los diferentes valores de los hiperparámetros. Con una búsqueda aleatoria suficientemente grande, esta aproximación tiene como ventajas:\n",
    "* Explora un espacio de búsqueda más amplio.\n",
    "* Es menos costoso computacionalmente.\n",
    "* Empiricamente [[5]](https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf?ref=broutonlab.com) ha demostrado ser más eficiente que la búsqueda exhaustiva.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f93689b-6367-4a8c-9fcd-663d78ad56d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemlo de Random Search con Random Forest\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# estos son los parámetros a analizar y los valores de cada uno a explorar\n",
    "param_distributions = {\n",
    "    'max_features': [2,4,6,8],\n",
    "    'n_estimators': [70,80,90,100,110]  \n",
    "}\n",
    "\n",
    "# buscaremos los mejores parámetros para el modelo de árbol de decisión (model_tree)\n",
    "# cv puede ser un número fijo, un KFold, o una lista de tuplas (train,test) de índices POSICIONALES a probar (este caso)\n",
    "# en este caso, cv será una lista de un único elemento, una tupla que recoge todos los índices POSICIONALES de entrenamiento y validación\n",
    "random_search = RandomizedSearchCV(estimator=model_forest, param_distributions=param_distributions,\n",
    "                           scoring=\"neg_root_mean_squared_error\", # criterio para seleccionar la mejor combinación de parámetros\n",
    "                           return_train_score=True, n_iter=10, # límite de combinaciones a probar (10 es el valor por defecto) \n",
    "                           cv=[(np.arange(len(training_inputs)),  # índices para entrenamiento\n",
    "                                np.arange(len(training_inputs),len(training_inputs) + len(val_inputs)) # índices para validación\n",
    "                               )])\n",
    "\n",
    "# x e y tienen los índices originales del dataframe en su columna idx, pero esto no se usará por el GridSearchCV.\n",
    "# si el primer índice del cv-train es '0', GridSearchCV cogerá el valor de la primera posición de x, no el índice idx=0.\n",
    "x = pd.concat([training_inputs, val_inputs])\n",
    "y = pd.concat([training_outputs, val_outputs])\n",
    "\n",
    "results = random_search.fit(x, np.array(y).ravel())\n",
    "\n",
    "print(f\"Mejor score en validación: {-random_search.best_score_}\")\n",
    "print(f\"Params del mejor score en validación: {random_search.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f5ca92-6555-493e-9930-27f19f4744b2",
   "metadata": {},
   "source": [
    "Limitando nuestra búsqueda a solo  10 combinaciones, es posible que el resultado óptimo no sea el encontrado pero podríamos aumentar el número de iteraciones para que el resultado final sea más confiable.\n",
    "\n",
    "Si repitiésemos la figura anterior para el caso del Random Search, veríamos que las combinaciones probadas muestran una forma aleatoria, no la rejilla anterior.\n",
    "\n",
    "| ![alt text](img/randomSearch.png \"Random Search\")| \n",
    "|:--:| \n",
    "| **Random Search**: Búsqueda aleatoria entre diferentes combinaciones de valores para dos hiperparámetros. En este ejemplo se evalúan 100 opciones aleatorias diferentes.Los contornos azules indican las regiones con mejores resultados, mientras que los rojos son regiones con peores resultados. Fuente de la imagen [wikipedia](https://commons.wikimedia.org/wiki/File:Hyperparameter_Optimization_using_Random_Search.svg)|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66be196-6eeb-4a66-9cca-ae7e5f2b4418",
   "metadata": {},
   "source": [
    "**EJERCICIO 3 PARA ENTREGAR EN EL AULA VIRTUAL**\n",
    "\n",
    "En este punto nos queda unir todo lo que hemos visto con el objetivo de seleccionar un modelo y testearlo. En este ejercicio deberéis realizar:\n",
    "1. Seleccionar 2 modelos:\n",
    "    * Un Random Forest.\n",
    "    * Un modelo de regresión presente en Scikit-learn que no se haya empleado en esta práctica (ANN, SVMs, etc.).\n",
    "2. Reducir la dimensionalidad empleando PCA y desarrollar los modelos.\n",
    "3. Optimizar los modelos con una búsqueda de Random Search empleando CV=5.\n",
    "    * Como mínimo, es necesario optimizar 2 hiperparámetros.\n",
    "4. Comparar los resultados de los modelos optimizados sobre el conjunto de validación.\n",
    "5. Seleccionar el mejor modelo en base a sus métricas de validación y testearlo con el conjunto de Test.\n",
    "6. Mostrar las métricas generales generadas por el Test.\n",
    "\n",
    "\n",
    "**Nota 1**: Una parte de los apartados están ya realizados, sobre todo los relacionados con el Random Forest.  Fijaros que el ejemplo del Random Search no se está empleando PCA, por lo que tenéis que adaptarlo.\n",
    "\n",
    "\n",
    "**Nota 2**: Todo el código necesario para el desarrollo del ejercicio debe aparecer en las celdas siguientes (incluso si algún apartado puede reaprovecharse de celdas anteriores). El objetivo es desarrollar el flujo de trabajo completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b4bb09-9296-435c-8c2a-ce035d6b2c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJERCICIO 3\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
