{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14f7f923-6c97-4dc1-a0dd-aa1404f35f1b",
   "metadata": {},
   "source": [
    "# Unidad 03. Regresión lineal de una variable\n",
    "\n",
    "El objetivo de esta práctica es comprender los fundamentos del entrenamiento de un modelo de ML simple. En particular, por su simplicidad, nos centraremos en un modelo de regresión lineal de una sola variable. \n",
    "\n",
    "En la vida real es muy improbable que tengáis que implementar modelos desde cero, ya que existe una gran variedad de librerías de ML disponibles, pero enfrentarse a la implementación de los conceptos más básicos os ayudará a entender cómo funcionan internamente los modelos y ver que no existe \"magia\" en el ML.\n",
    "\n",
    "**Importante**: este Notebook esta desarrollado con fines docentes y no con el objetivo de optimizar el proceso. En el desarrollo y entrenamiento de modelos de ML (offline) es vital aprovecharnos de la vectorización para la optimización de recursos (operaciones en matrices o vectores completos en lugar de procesar los elementos individualmente mediante bucles). La vectorización es clave pero también dificulta el aprendizaje, ya que ofusca el código.\n",
    "\n",
    "**Este Notebook no contiene ejercicios para entregar** pero es importante que dediqueis tiempo para entender los conceptos. \n",
    "\n",
    "## Referencias útiles para la práctica\n",
    "1. API Matplotlib: [https://matplotlib.org/stable/api/index](https://matplotlib.org/stable/api/index)\n",
    "2. API Numpy: [https://numpy.org/doc/stable/reference/](https://numpy.org/doc/stable/reference/)\n",
    "3. Dataset para el ejercicio: [https://www.kaggle.com/datasets/camnugent/california-housing-prices](https://www.kaggle.com/datasets/camnugent/california-housing-prices).\n",
    "4. [Curso](https://www.coursera.org/learn/machine-learning-course/home/welcome) de Machine Learning de Anxdrew NG en Coursera.\n",
    "   * Este es un curso muy recomendable para el que quiera entender los conceptos básicos del ML.\n",
    "   * La Unidad 03 es una adaptación de diferentes prácticas del curso a nuestro problema.\n",
    "\n",
    "\n",
    "## Modelo de Regresión lineal de una variable\n",
    "\n",
    "Un modelo de regresión lineal de una variable busca establecer una relación lineal entre dos variables: una variable independiente (input) y una variable dependiente (output).  El objetivo es encontrar la línea recta que mejor se ajuste a los datos, permitiendo predecir el valor de la variable dependiente a partir de la variable independiente.\n",
    "\n",
    "La función del modelo que representa la regresión lineal simple se muestra a continuación:\n",
    "\n",
    "\n",
    "\n",
    "$$ f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{1}$$\n",
    "\n",
    "Esta función tiene dos parámetros $w$ y $b$ y asigna un valor \"y\" (output) en función de una variable de entrada \"x\" (input). El ajuste de los parámetros $w$ y $b$ se realiza durante el proceso de entrenamiento del modelo.\n",
    "\n",
    "## Conjunto de datos\n",
    "\n",
    "Usaremos el conjunto de datos que hemos visto en las unidades anteriores, pero en este caso solo nos centraremos en 2 variables, para ajustarnos a las condiciones de la regresión lineal. En la Unidad 01 vimos que entre la variable a predecir *median_house_value* y *median_income* había una correlación lineal bastante alta. EL objetivo de esta Unidad será desarrollar una regresión lineal entre esas 2 variables.\n",
    "\n",
    "\n",
    "**Recordad** que durante la Unidad 01 descubrimos que la *median_income* era una representación escalada del ingreso promedio para los hogares en una manzana, cuyo valor representaba decenas de miles de dolares (ej. 35=350.000$). Las 2 variables están en escalas muy diferentes. Para facilitar las gráficas y el seguimiento de la práctica, realizaremos un escalado sobre la característica *median_house_value* dividiendo su valor por 1.000.\n",
    "\n",
    "**Nota**: aunque ya tenemos los datos preprocesados de la Unidad 02, vamos a trabajar con los datos originales para que su interpretación sea más sencilla. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c69ca9-e1bb-4a9c-b535-48f75bcda2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import kagglehub\n",
    "except ImportError as err:\n",
    "    !pip install kagglehub\n",
    "    import kagglehub\n",
    "    \n",
    "try:\n",
    "    from rich import print\n",
    "except ImportError as err:\n",
    "    !pip install rich\n",
    "    from rich import print\n",
    "\n",
    "import pandas as pd\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ImportError as err:\n",
    "    !pip install pandas\n",
    "    import pandas as pd\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "except ImportError as err:\n",
    "    !pip install sklearn\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except ImportError as err:\n",
    "    !pip install matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "# Descarga del dataset\n",
    "path = kagglehub.dataset_download(\"camnugent/california-housing-prices\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "dataset=pd.read_csv(path+\"/housing.csv\")#Carga datos desde un CSV y devuelve un DataFrame \n",
    "\n",
    "#Generación de los datasets de entrenamiento y test\n",
    "SEED=1234 #Semilla para asegurarnos que siempre se generen los mismos valores aleatorios\n",
    "trainset, testset=train_test_split(dataset, test_size=0.3, train_size=0.7, random_state=SEED, shuffle=True, stratify=dataset[\"ocean_proximity\"])\n",
    "\n",
    "#Para esta Unidad solo nos interesan las variables \"median_income\" y \"median_house_value\"\n",
    "trainset=trainset[[\"median_income\",\"median_house_value\" ]].copy()\n",
    "testset=testset[[\"median_income\",\"median_house_value\" ]].copy()\n",
    "\n",
    "#En unidades anteriores habóiamos descubierto que \"median_house_value\" tenía un límite superior\n",
    "#que agrupaba todas las manzanas con precio medio de vivienda mayor que dicho límite\n",
    "#para no condicionar el modelo, esas observaciones son eliminadas\n",
    "#Eliminación de las observaciones con  median_house_value == max\n",
    "#Eliminación de observaciones de Entrenamiento\n",
    "trainset=trainset[trainset[\"median_house_value\"]!=dataset[\"median_house_value\"].max()]\n",
    "#escalamos median_house_value en el mismo rango que median_income\n",
    "trainset[\"median_house_value\"]=trainset[\"median_house_value\"].apply(lambda X: X/1000)\n",
    "#Eliminación de observaciones de Test\n",
    "testset=testset[testset[\"median_house_value\"]!=dataset[\"median_house_value\"].max()]\n",
    "\n",
    "\n",
    "#Por motivos docentes y de visualización\n",
    "#escalamos \"median_house_value\" en el mismo rango que \"median_income\"\n",
    "#Ver *Nota* superior\n",
    "testset[\"median_house_value\"]=testset[\"median_house_value\"].apply(lambda X: X/1000)\n",
    "\n",
    "\n",
    "#Ploteamos los datos para visualizar la correlación entre las dos variables\n",
    "trainset.plot(x=\"median_income\", y=\"median_house_value\", kind=\"scatter\")\n",
    "plt.show()\n",
    "print(trainset.corr())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a416376-12af-4ca6-82e2-4eac7eb185e3",
   "metadata": {},
   "source": [
    "Empezaremos con lo más básico, cogiendo 2 puntos de los datos de entrenamiento para construir un modelo entre ellos. Vamos a trabajar desde la base, es decir, empleando [arrays](https://numpy.org/doc/stable/reference/generated/numpy.array.html) de numpy.\n",
    "\n",
    "**Nota**: Las observaciones seleccionadas están deliberadamente escogidas por motivos docentes. Tenéis que tener en cuenta que este no es un problema que se pueda resolver con una regresión lineal de 1 variable, por lo que he buscado valores que ajusten correctamente.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f95860-de8e-4b60-9deb-c5af6c891f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import numpy as np\n",
    "except ImportError as err:\n",
    "    !pip install numpy\n",
    "    import numpy as np\n",
    "\n",
    "# Las observaciones seleccionadas están deliberadamente escogidas por motivos docentes.\n",
    "pto1=trainset.iloc[1697]#Devuelve una Serie de Pandas\n",
    "pto2=trainset.iloc[4032] #Devuelve una Serie de Pandas\n",
    "\n",
    "#Almacenamos en un array las entradas (inputs o 'x'), es decir, los valores de \"median_income\"\n",
    "x_train = np.array([pto1.loc[\"median_income\"],pto2.loc[\"median_income\"]])\n",
    "\n",
    "#Almacenamos en un array las salidas (outputs, labels, o 'y'), es decir, los valores de \"median_house_value\"\n",
    "y_train = np.array([pto1.loc[\"median_house_value\"],pto2.loc[\"median_house_value\"]])\n",
    "\n",
    "#Mostramos los puntos escogidos\n",
    "print(f\"x_train = {x_train}\")\n",
    "print(f\"y_train = {y_train}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1978a370-f997-4086-b986-1c44eaae5212",
   "metadata": {},
   "source": [
    "#### Notación\n",
    "- Usaremos $m$ para denotar el número de ejemplos que tenemos para entrenar, es decir, el número de observaciones de nuestro conjunto de entrenamiento (2 en este caso). Podemos emplear <code>len()</code> para obtener el valor o el método [shape()](https://numpy.org/doc/stable/reference/generated/numpy.shape.html) de los arrays de numpy.\n",
    "\n",
    "\n",
    "- Usaremos la notación (x$^{(i)}$, y$^{(i)}$) para denotar la  $i$-esima observación del conjunto de entrenamiento.\n",
    "\n",
    "    -    **Recordad** que en Python (y en Numpy) los índices empiezan en cero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71322267-2e23-499b-a454-ac000d0f6537",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Número de observaciones del entrenamiento: {len(x_train)}\")\n",
    "i = 0 # i-esima observación\n",
    "x_i = x_train[i]\n",
    "y_i = y_train[i]\n",
    "print(f\"La observación {i} es (x^({i}), y^({i})) = ({x_i}, {y_i})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf55962-512f-4172-8ffa-3c659239ecc3",
   "metadata": {},
   "source": [
    "Visualicemos los datos de las observaciones seleccionadas en un gráfico. Veremos que estos puntos pueden encajar facilmente con una regresión lineal.\n",
    "\n",
    "**Recordad** que los puntos fueron deliberadamente escogidos. Otros puntos del dataset de entrenamiento original no mostarían el mismo resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121b7b62-1f0f-4c37-9f8e-ac34c6f5324e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotea los puntos\n",
    "plt.figure()\n",
    "plt.scatter(x_train, y_train, marker='x', c='r')\n",
    "# Especifica el título\n",
    "plt.title(\"Valor medio de la vivienda\")\n",
    "\n",
    "# Añade el eje y\n",
    "plt.ylabel('median house value')\n",
    "# Añade el eje x\n",
    "plt.xlabel('median income')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fadfffe-c21d-43c9-9b75-48c901607536",
   "metadata": {},
   "source": [
    "## Modelo de regresión lineal de una variable\n",
    "\n",
    "Recordamos la función del modelo para la regresión lineal de una sola variable:\n",
    "\n",
    "$$ f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{1}$$\n",
    "\n",
    "Esta función solo permite representar lineas rectas (es la ecuación de una recta). Cambiando los valores $w$ y $b$ generamos diferentes tipos de recta, que se podrán ajustar mejor o peor a nuestros datos (el objetivo del entrenamiento es precisamente encontrar los mejores valores para $w$ y $b$). La clave es que no podemos escoger unos valores de $w$ y $b$ diferentes para cada observación, si no que necesitamos encontrar los valores que mejor se ajustan, de media, a todas las observaciones del conjunto de entrenamiento.\n",
    "\n",
    "\n",
    "Si escogemos 2 valores cualquiera para  $w$ y $b$ podremos usar nuestro modelo para predecir la $y$ en base a las entradas $x$.\n",
    "\n",
    "- Para $x^{(0)}$, `f_wb = w * x[0] + b`\n",
    "- Para $x^{(1)}$, `f_wb = w * x[1] + b`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca70911-9497-429b-8977-7d673c8cf67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcula_salida_modelo(x, w, b):\n",
    "    m = len(x)\n",
    "    print(m)\n",
    "    predicciones = np.zeros(m)#genera una matriz de ceros con numpoy\n",
    "    for i in range(m):\n",
    "        #función del modelo que se aplica sobre los datos de entrada para unos valores w, b concretos\n",
    "        predicciones[i] = w * x[i] + b \n",
    "    return predicciones\n",
    "\n",
    "\n",
    "def plot_resultados_modelo(inputs,predicciones, labels):\n",
    "\n",
    "    # Genera un plot con las predicciones\n",
    "    plt.plot(inputs, predicciones, c='b',label='Predicciones')\n",
    "    # Añade al plot los puntos reales de entrenamiento\n",
    "    plt.scatter(inputs, labels, marker='x', c='r',label='Valores reales')\n",
    "\n",
    "    \n",
    "    # Añade un título al gráfico\n",
    "    plt.title(\"Valor medio de la vivienda\")\n",
    "    # Añade el eje y\n",
    "    plt.ylabel('median house value')\n",
    "    # Añade el eje x\n",
    "    plt.xlabel('median income')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "w = 100# seleccionamos un valor para w\n",
    "b = 100# seleccionamos un valor para b \n",
    "print(f\"w: {w}\")\n",
    "print(f\"b: {b}\")\n",
    "\n",
    "predicciones = calcula_salida_modelo(x_train, w, b)\n",
    "plot_resultados_modelo(x_train, predicciones,y_train)\n",
    "for i, (x, y,pred) in enumerate(zip(x_train, y_train, predicciones)):\n",
    "    print(f\"x({i})={x} - valor real: {y} valor predicho: {pred:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a36dd73-8722-4116-9f7b-9f7e71221c8b",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "Prueba a jugar con los valores de $w$ y $b$ para ver como varía la recta hasta conseguir unos valores que ajusten bien.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bb8edc-95b2-4f19-9415-802931dae316",
   "metadata": {},
   "outputs": [],
   "source": [
    "#w = ?\n",
    "#b = ?\n",
    "\n",
    "\n",
    "predicciones = calcula_salida_modelo(x_train, w, b)\n",
    "plot_resultados_modelo(x_train, predicciones,y_train)\n",
    "for i, (x, y,pred) in enumerate(zip(x_train, y_train, predicciones)):\n",
    "    print(f\"x({i})={x} - valor real: {y} valor predicho: {pred:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f63b9f-35a5-4ba1-986b-61d5f4d3480b",
   "metadata": {},
   "source": [
    "Una vez escogemos unos valores para los parámetros de nuestro modelo que ajustan bien con los datos de entrenamiento, podemos usar el modelo  para predecir con nuevos datos con los que nunca se entrenó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa68297-588a-4a29-b9a7-90b8a9033b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 200\n",
    "b = 100\n",
    "\n",
    "x_punto_test=3#probamos con un valor \n",
    "\n",
    "precio_medio_vivienda = w * x_punto_test + b \n",
    "precio_medio_vivienda*=1000 #lo devolvemos a la escala original\n",
    "\n",
    "print(f\"Precio de una vivienda en el que la familia tiene un ingreso de {x_punto_test*10000}$: {precio_medio_vivienda}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340448cb-d9c8-4af1-8f9e-62b0b24eb4ab",
   "metadata": {},
   "source": [
    "## Función de coste\n",
    "\n",
    "Tenemos una función con 2 parámetros  ($w$ y $b$)  que debemos definir para ajustar dicho modelo pero, \n",
    "**¿Cómo escogemos los valores $w$ y $b$?**\n",
    "$$ f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{1}$$\n",
    "\n",
    "\n",
    "Obviamente no podemos estar probando con todas las posibles combinaciones \"a mano\". Necesitamos entrenar el modelo y que se ajusten automáticamente. Para poder decidir que una combinación es mejor que otra, es necesario poder establecer un **coste** de la combinación. De esta forma podremos comparar combinaciones y escoger entre las diferentes opciones. \n",
    "\n",
    "\n",
    "De forma general podemos representar la función de coste para los párametros $w$ y $b$ como: $J(w,b)$\n",
    "\n",
    "\n",
    "Existen diferentes funciones de coste pero una habitual en ML es el *Mean Squared Error* (MSE).\n",
    "\n",
    "$$MSE=\\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2\\tag{2}$$\n",
    "\n",
    "y particularmente, en problemas de regresión,  es habitual introducir un factor $\\frac{1}{2}$, quedando la función de coste como: \n",
    "\n",
    "  $$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2 \\tag{3}$$ \n",
    "\n",
    " donde:\n",
    " -  $f_{w,b}(x^{(i)}) = wx^{(i)} + b $ (función del modelo).\n",
    " - $f_{w,b}(x^{(i)})$ es nuestra predicción para la entrada $x^i$ empleando unos parámetros ($w,b$) específicos. Por simplicidad es habitual representar la predicción como: $\\hat{y}^i$\n",
    "- $(f_{w,b}(x^{(i)}) -y^{(i)})^2$ es la diferencia al cuadrado entre el valor real y la predicción.   \n",
    "- Todas esas diferencias a lo largo de las $m$ observaciones son sumadas y divididas por `2m` para producir el coste final, $J(w,b)$. \n",
    "\n",
    "\n",
    "**Fijaos** que el coste se calcula sobre todos las observaciones de entrenamiento, por lo que el objetivo es obtener los parámetros $w,b$ que mejor se comportan para todas las observaciones .\n",
    "\n",
    " **Importante**: Cuánto más cerca estén las predicciones de los valores reales, menor coste. **Debemos buscar una forma de minimizar la función de coste** para obtener la mejor solución.\n",
    "\n",
    "\n",
    "$$\\underset{w, b}{\\text{minimize}}J(w,b)\\tag{4}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd1bb58-1803-4523-9b2f-e15b85aa22e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculo_coste(x, y, w, b): \n",
    "    \"\"\"\n",
    "    Calcula la función de coste para la regresión lineal\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray (m,)): Entradas, m observaciones \n",
    "      y (ndarray (m,)): salidas\n",
    "      w,b (scalar)    : parámetros del modelo  \n",
    "    \n",
    "    Returns\n",
    "        coste_total (float)\n",
    "    \"\"\"\n",
    "    # observaciones\n",
    "    m = len(x)\n",
    "    cost_sum = 0 \n",
    "    for i in range(m): #recorremos todas las observaciones y sumamos su coste\n",
    "        pred = w * x[i] + b   \n",
    "        cost_sum += (pred - y[i]) ** 2    \n",
    "    return (1 / (2 * m)) * cost_sum #promediamos el coste \n",
    "\n",
    "\n",
    "#Comprobamos los diferentes costes que tendríamos para el mismo conjunto de entrada\n",
    "#cuando varíamos los parámetros w, b\n",
    "#Podéis probar con otras configuraciones\n",
    "w_pruebas=[50,100,150]#posibles valores para w\n",
    "b_pruebas=[50,100,150]#posibles valores para b\n",
    "\n",
    "print(\"Para los datos de entrenamiento:\")\n",
    "for i,(x,y) in enumerate(zip(x_train, y_train)):\n",
    "    print(f\"(x^{i},y^{i})=({x},{y})\")\n",
    "\n",
    "\n",
    "for w, b in zip(w_pruebas,b_pruebas):\n",
    "    print(f\"El coste total con w={w} y b={b} es: {calculo_coste(x_train, y_train, w, b):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbbb27d-1268-49cb-86a3-75ff207ee6f7",
   "metadata": {},
   "source": [
    "### Visualización de la función de coste \n",
    "\n",
    "Para mejorar la comprensión de la función de coste y en que consiste su minimzación, vamos a simplificar el problema y suponer que el valor de $b$ es fijo y que solo tenemos que  buscar el valor óptimo de $w$, es decir, nuestra función solo tiene un parámetro \"libre\". Para cada posible valor de $w$, podemos calcular su función de coste ($J(w)$) y graficar dicho valor. A medida que vayamos calculando valores y graficándolos, iremos dibujando la curva del coste. \n",
    "\n",
    "\n",
    "Si solo tenemos 1 variable \"libre\" y le damos todos los posibles valores, la función de coste que podríamos pintar con los resultados tendría  forma convexa con un mínimo global. Nuestro objetivo minimizando dicha función de coste ($J(w)$) es encontrar dicho mínimo global.  Necesitamos un algoritmo que nos permita movernos por la función de coste hasta alcanzar el mínimo.\n",
    "\n",
    "Veamos la representación gráfica de esta explicación. Fijaos en la figura inferior como evoluciona el coste (parte derecha) si movemos el valor de $w$ (parte izquierda). \n",
    "- La figura de la parte derecha refleja en azul la forma de la función de coste si graficásemos todos sus valores (es una aproximación).\n",
    "- La figura de la parte derecha muestra el coste de una configuración concreta (valor de $w$) dentro de la función de coste. El objetivo es conseguir que el coste concreto se situe en el mínimo de la función.\n",
    "- La figura de la parte izquierda muestra el coste de cada predicción (línea azul) respecto al valor real (puntos rojos). Cuanto más lejos el valor de la predicción del real, mayor coste.\n",
    "\n",
    "**Nota**: Podéis jugar con el valor fijo de $b$.\n",
    "\n",
    "**Nota 2**: el gráfico es interactivo, podéis modificar el valor de $w$. El refresco puede ser algo lento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e710047-747c-4bc8-a1e4-47990c9a3a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "from lab_utils_uni import plt_intuition, plt_stationary, plt_update_onclick\n",
    "\n",
    "#Esta función está en un fichero de Python con métodos útiles para la práctica\n",
    "#No es necesario que la entendáis\n",
    "#Se emplea solo por motivos docentes\n",
    "plt_intuition(x_train,y_train, fixed_b=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325769fd-5a6e-4ccd-a6ba-bf951f417f07",
   "metadata": {},
   "source": [
    "Aumentemos la dificultad y dejemos \"libres\" los 2 parámetros $w$ y $b$. En este caso, para poder graficar la función de coste, necesitamos un gráfico en 3 dimensiones o emplear un gráfico de contornos (aka [isolineas](https://es.wikipedia.org/wiki/Isol%C3%ADnea)). \n",
    "\n",
    "- El gráfico de la derecha (gráfico de contornos) nos permite seleccionar una combinación de los parámetros $w$ y $b$\n",
    "- El gráfico de la izquierda nos permite visualizar la recta y el coste de la selección de parámetros actual.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fef1ec8-5e84-434e-8e68-09c4c20b8b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all') \n",
    "fig, ax, dyn_items = plt_stationary(x_train, y_train)\n",
    "updater = plt_update_onclick(fig, ax, x_train, y_train, dyn_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8d557d-adb3-44d7-b84d-aa4d75d12c3b",
   "metadata": {},
   "source": [
    "## Gradiente descendente\n",
    "\n",
    "Ahora que entendemos como funciona una regresión lineal y como se calcula el coste de los posibles valores ($w,b$), debemos encontrar una forma de buscarlos de forma automática. Esta claro que no podemos ir probando todas las posibles combinaciones de forma manual. \n",
    "\n",
    "¿Cómo minimizamos la función de coste? Empleando el algoritmo de **descenso de gradiente**. \n",
    "\n",
    "$$\\underset{w, b}{\\text{minimize}}J(w,b)\\tag{4}$$\n",
    "\n",
    "\n",
    "Este algoritmo define la forma de actualizar  los parámetros $w$, $b$.\n",
    "\n",
    "$$\\begin{align*} \\text{Repetir}&\\text{ hasta que converja:} \\; \\lbrace \\newline\n",
    "\\;  w &= w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{5}  \\; \\newline \n",
    " b &= b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "donde:\n",
    "- $\\alpha$ es la tasa de aprendizaje o *learning rate*.\n",
    "- $\\frac{\\partial J(w,b)}{\\partial w}$ es la derivada parcial de la función de coste respecto a $w$.\n",
    "- $\\frac{\\partial J(w,b)}{\\partial b}$ es la derivada parcial de la función de coste respecto a $b$.\n",
    "\n",
    "La actualización de parámetros implica la derivada parcial de la función de coste en función de $w$ y de $b$. **No es necesario aprendais a hacer la derivada parcial**, a continuación os proporciono el resultado de dicha derivada parcial:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(w,b)}{\\partial w}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\tag{4}\\\\\n",
    "  \\frac{\\partial J(w,b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{5}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "**Importante**: Los parámetros $w$, $b$ deben ser actualizados simultaneamente. Esto significa que las derivadas parciales deben calcularse antes de actualizar ninguno de los parámetros.\n",
    "\n",
    "Os resumo los pasos del algoritmo: \n",
    "1. Inicializamos los valores de $w$ y $b$, típicamente con valores aleatorios.\n",
    "2. Calculamos las predicciones.\n",
    "3. Calculamos las derivadas parciales.\n",
    "4. Ajustamos los parámetros $w$ y $b$ en función del gradiente.\n",
    "5. Repetimos hasta cumplir la condición de salida.\n",
    "\n",
    "\n",
    "\n",
    "El tamaño y dirección del \"paso\" para actualizar los parámetros $w$ y $b$ viene marcado por el resultado de la derivada parcial:\n",
    "- **Dirección**: Si la derivada es positiva, debemos disminuir el parámetro; si es negativa, debemos aumentarlo.\n",
    "- **Magnitud**: Qué tan grande es el cambio necesario. Valores mayores significan que el coste cambia rápidamente en esa dirección.\n",
    "\n",
    "El hiperparámetro $\\alpha$ (tasa de aprendizaje o *learnig rate*) permite aumentar o reducir el \"paso\" en la actualización de los parámetros. Este hiperparámetro es algo que deberemos escoger y ajustar al entrenar los modelos y que suele ser clave:\n",
    "- Un $\\alpha$ pequeño puede hacer que no lleguemos a un buen ajuste final del modelo en las iteraciones que le permitamos entrenar.\n",
    "-  Un $\\alpha$ demasiado grande puede resultar en un entrenamieno divergente. \n",
    "\n",
    "\n",
    "Vamos a implementar el descenso de gradiente para una sola variable con la idea de entender los fundamentos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed32ddc-da00-460c-ad74-b7f8d81a5aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculo_gradiente(x, y, w, b): \n",
    "    \"\"\"\n",
    "    Calcula el gradiente para la regresión lineal\n",
    "    Args:\n",
    "      x (ndarray (m,)): datos de entrada, m ejemplos \n",
    "      y (ndarray (m,)): valores de salida\n",
    "      w,b (scalar)    : parámetros del modelo  \n",
    "    Returns\n",
    "      dj_dw (scalar): El gradiente del coste respecto a w\n",
    "      dj_db (scalar): El gradiente del coste respecto a b     \n",
    "     \"\"\"\n",
    "    \n",
    "    # observaciones\n",
    "    m = len(x)    \n",
    "\n",
    "    #valores que devolveremos inicializados a cero\n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "\n",
    "    #calculo de los gradientes\n",
    "    for i in range(m):  \n",
    "        f_wb = w * x[i] + b #cálculo de la predicción\n",
    "        dj_dw_i = (f_wb - y[i]) * x[i] \n",
    "        dj_db_i = f_wb - y[i] \n",
    "        dj_db += dj_db_i\n",
    "        dj_dw += dj_dw_i \n",
    "    dj_dw = dj_dw / m \n",
    "    dj_db = dj_db / m \n",
    "        \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9ac4d5-be43-4428-aa71-63c32d28df77",
   "metadata": {},
   "source": [
    "Para una mejor comprensión de los gradientes y la función de coste, vamos a fijar el valor de parámetro $b$ y probar con diferentes puntos para $w$. El gradiente nos devolverá:\n",
    "- **Dirección**: Si la derivada es positiva, debemos disminuir el parámetro; si es negativa, debemos aumentarlo.\n",
    "- **Magnitud**: Qué tan grande es el cambio necesario. Valores mayores significan que el coste cambia rápidamente en esa dirección.\n",
    "\n",
    "**Nota**: En el mínimo global, la derivada será 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a36070-4513-4223-88a0-40d2dda10aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab_utils_uni import plt_gradients\n",
    "plt.close('all') \n",
    "plt_gradients(x_train,y_train, calculo_coste, calculo_gradiente, fixed_b=100, tested_points=[50,100,150,200, 300])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d62c74c-477c-4939-8a3c-e167ce3bac96",
   "metadata": {},
   "source": [
    "### Implementación del gradiente descendente.\n",
    "Una vez comprendido como los gradientes nos ayudarán a saber la dirección y la magnitud de las actualizaciones de nuestros parámetros $w$ y $b$, ya solo necesitamos implementar el algoritmo del gradiente descendente para automatizar el proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda889fa-4187-4747-af85-c0a201092929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradiente_descendente(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function): \n",
    "    \"\"\"\n",
    "    Desarrolla el algoritmo de gradiente descendente para ajustar w y b. \n",
    "    Actualiza los valores w y b realizando num_iters cálculos de gradiente\n",
    "    con una tasa de aprendizaje alpha\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray (m,))  : Datos entrada, m observacines \n",
    "      y (ndarray (m,))  : valores de salida\n",
    "      w_in,b_in (scalar): Valores iniiales de w y b  \n",
    "      alpha (float):     Learning rate o tasa de aprendizaje\n",
    "      num_iters (int):   número de iteraciones (épocas) para ejecutar el gradiente descendente.\n",
    "      cost_function:     función que calcula el coste\n",
    "      gradient_function: función que calcula el radiente\n",
    "      \n",
    "    Returns:\n",
    "      w (scalar): valor calculado de w una vez ejecutado el algoritmo de gradiente descendente.\n",
    "      b (scalar): valor calculado de b una vez ejecutado el algoritmo de gradiente descendente.\n",
    "      J_history (List): histórico de valores de coste \n",
    "      p_history (list): histórico de valores de (w,b)\n",
    "      \"\"\"\n",
    "    \n",
    "    # array para almancear los valores históricos de coste y (w,b)\n",
    "    #Los usaremos para graficar\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    b = b_in\n",
    "    w = w_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calcula el gradiente empleando la función pasada como argumento\n",
    "        dj_dw, dj_db = gradient_function(x, y, w , b)     \n",
    "\n",
    "        # Actualiza los parámetros empleando la ecuación 5\n",
    "        b = b - alpha * dj_db                            \n",
    "        w = w - alpha * dj_dw                            \n",
    "\n",
    "        ###Esta parte del algoritmo es opcional, es solo por histórico y visualización\n",
    "        coste=cost_function(x, y, w , b)\n",
    "        J_history.append(coste )#almacena el coste en el histórico\n",
    "        p_history.append([w,b])#almacena los valores de w y b en el histórico\n",
    "\n",
    "        #Mostramos el avance cada 10 iteraciones\n",
    "        if i%10==0:\n",
    "            print(f\"Iteración {i}. Coste {coste}.\",\n",
    "                  f\"dj_dw: {dj_dw: 0.2f}, dj_db: {dj_db: 0.2f}  \",\n",
    "                  f\"w: {w: 0.2f}, b:{b: 0.2f}\"\n",
    "                 )\n",
    "\n",
    " \n",
    "    return w, b, J_history, p_history "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85102b3-5d07-47ab-a0c7-9223c268e3be",
   "metadata": {},
   "source": [
    "Probemos el algoritmo con nuestros datos:\n",
    "\n",
    "**Nota**: si jugais con el número de iteraciones o con los valores de los parámetros de entrada, conseguiréis ajustar mejor el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec00171-597d-417f-b031-a721b64e9976",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_init=10#valores iniciales\n",
    "b_init=10#valores iniciales\n",
    "iteraciones=1000\n",
    "alpha=0.01\n",
    "\n",
    "\n",
    "w_calculado, b_calculado, J_history, p_history =gradiente_descendente(x_train, y_train, w_init, b_init, alpha, iteraciones, calculo_coste, calculo_gradiente)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1017cf-0233-4557-a44d-02b6707c6b30",
   "metadata": {},
   "source": [
    "Probemos el modelo con los resultados del algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1429fd57-55d6-4d05-8b62-2da66fd5c531",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicciones = calcula_salida_modelo(x_train, w_calculado, b_calculado)\n",
    "plt.close('all') \n",
    "plot_resultados_modelo(x_train, predicciones,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30ea858-d670-4397-a020-dde85f9510c4",
   "metadata": {},
   "source": [
    "### Coste vs iteraciones\n",
    "Un gráfico del coste frente a las iteraciones es una medida ineresante para valorar el progreso en el descenso de gradiente. El coste siempre debería disminuir en ejecuciones exitosas. El cambio en el coste muy rápido al comienzo y luego tiende a desacelerar, por lo que, a menudo,  es útil graficar el descenso inicial en una escala diferente a la del descenso final. En los gráficos a continuación, observe la escala del coste en los ejes y el paso de iteración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a862bbd8-1e12-4b1b-a7e4-52fcc0efc95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot coste vs iteraciones versus iteration  \n",
    "plt.close('all') \n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(9,4))\n",
    "ax1.plot(J_history[:100])\n",
    "\n",
    "\n",
    "\n",
    "ax2.plot(100 + np.arange(len(J_history[100:])), J_history[100:])\n",
    "ax1.set_title(\"Coste vs. iteración(incio)\");  ax2.set_title(\"Coste vs. iteración (final)\")\n",
    "ax1.set_ylabel('Coste')            ;  ax2.set_ylabel('Coste') \n",
    "ax1.set_xlabel('Iteración')  ;  ax2.set_xlabel('Iteración') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a678bb0-0328-42ca-8f28-3d6090a3c9cd",
   "metadata": {},
   "source": [
    "Podemos también visualizar la evolución de los parámetros $w$ y $b$ durante la ejecución del algoritmos de gradiente descendente respecto al gráfico de contornos. Veremos como la evolución de los valores se acerca, en cada iteración, al centro del gráfico, dónde se encuentra el mínimo global."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e590f301-2ec8-4e76-9b86-90accece27a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab_utils_uni import plt_contour_wgrad\n",
    "fig, ax = plt.subplots(1,1, figsize=(10, 6))\n",
    "plt_contour_wgrad(x_train, y_train, p_history, ax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
