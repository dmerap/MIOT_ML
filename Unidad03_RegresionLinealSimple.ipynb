{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bd759c9-9b54-4c0d-8d83-dce977d6e1b0",
   "metadata": {},
   "source": [
    "![alt text](img/MIoT_ML.png \"MIoT_ML\")\n",
    "# Unidad 03. Regresión lineal de una variable\n",
    "\n",
    "El objetivo de esta práctica es comprender los fundamentos del entrenamiento de un modelo de ML simple. En particular, por su simplicidad, nos centraremos en un modelo de regresión lineal de una sola variable. \n",
    "\n",
    "En la vida real es muy improbable que tengáis que implementar modelos desde cero, ya que existen una gran variedad de librerías de ML disponibles, pero enfrentarse a la implementación de los conceptos más básicos os ayudará a entender cómo funcionan internamente los modelos, y ver que no existe \"magia\" en el ML.\n",
    "\n",
    "**Importante**: este Notebook está desarrollado con fines docentes, y no con el objetivo de optimizar el proceso. En el desarrollo y entrenamiento de modelos de ML (offline) es vital aprovecharnos de la vectorización para la optimización de recursos (operaciones en matrices o vectores completos en lugar de procesar los elementos individualmente mediante bucles). La vectorización es clave, pero también dificulta el aprendizaje, ya que hace menos legible el código.\n",
    "\n",
    "\n",
    "El Notebook contiene varios ejercicios sencillos. Debéis desarrollarlos durante la clase y enviarlos por el aula virtual del curso, en la tarea correspondiente.\n",
    "\n",
    "## Referencias útiles para la práctica\n",
    "1. API Matplotlib: [https://matplotlib.org/stable/api/index](https://matplotlib.org/stable/api/index)\n",
    "2. API Numpy: [https://numpy.org/doc/stable/reference/](https://numpy.org/doc/stable/reference/)\n",
    "3. Dataset para el ejercicio: [https://www.kaggle.com/datasets/camnugent/california-housing-prices](https://www.kaggle.com/datasets/camnugent/california-housing-prices).\n",
    "4. [Curso](https://www.coursera.org/learn/machine-learning-course/home/welcome) de Machine Learning de Anxdrew NG en Coursera.\n",
    "   * Esta unidad contiene algunos ejemplos inspirados y/o adaptados de algunas de las prácticas del curso de Machine Learning de Andrew NG\n",
    "\n",
    "\n",
    "## 1. Modelo de regresión lineal de una variable\n",
    "\n",
    "Un modelo de regresión lineal de una variable busca establecer una relación lineal entre dos variables: una variable independiente (input) y una variable dependiente (output).  El objetivo es encontrar la línea recta que mejor se ajuste a los datos, permitiendo predecir el valor de la variable dependiente a partir de la variable independiente.\n",
    "\n",
    "La función del modelo que representa la regresión lineal simple se muestra a continuación:\n",
    "\n",
    "\n",
    "\n",
    "$$ f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{1}$$\n",
    "\n",
    "Esta función tiene dos parámetros $w$ y $b$ y asigna un valor \"y\" (output) en función de una variable de entrada \"x\" (input). El ajuste de los parámetros $w$ y $b$ se realiza durante el proceso de entrenamiento del modelo.\n",
    "\n",
    "## 2. Conjunto de datos\n",
    "### 2.1. Cargar y seleccionar los datos\n",
    "\n",
    "Usaremos el conjunto de datos que hemos visto en las unidades anteriores, pero, en este caso, solo nos centraremos en dos variables, para ajustarnos a las condiciones de la regresión lineal. En la Unidad 01 vimos que entre la variable a predecir *median_house_value* y *median_income* había una correlación lineal bastante alta. EL objetivo de esta Unidad será desarrollar una regresión lineal entre esas 2 variables.\n",
    "\n",
    "\n",
    "**Recordad** que durante la Unidad 01 descubrimos que la *median_income* era una representación escalada de la mediana de los ingresos de los hogares en una manzana, cuyo valor representaba decenas de miles de dolares (ej. 35=350.000$). Las dos variables están en escalas muy diferentes. Para facilitar las gráficas y el seguimiento de la práctica, realizaremos un escalado sobre la característica *median_house_value* dividiendo su valor por 1.000.\n",
    "\n",
    "**Nota**: aunque ya tenemos los datos preprocesados de la Unidad 02, vamos a trabajar con los datos originales para que su interpretación sea más sencilla. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d3be56-a980-4486-9fb6-4746c70002c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import kagglehub\n",
    "except ImportError as err:\n",
    "    !pip install kagglehub\n",
    "    import kagglehub\n",
    "    \n",
    "try:\n",
    "    from rich import print\n",
    "except ImportError as err:\n",
    "    !pip install rich\n",
    "    from rich import print\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ImportError as err:\n",
    "    !pip install pandas\n",
    "    import pandas as pd\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "except ImportError as err:\n",
    "    !pip install sklearn\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except ImportError as err:\n",
    "    !pip install matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "# Descarga del dataset\n",
    "path = kagglehub.dataset_download(\"camnugent/california-housing-prices\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "dataset = pd.read_csv(path+\"/housing.csv\") # Carga datos desde un CSV y devuelve un DataFrame \n",
    "\n",
    "# Generación de los datasets de entrenamiento y test\n",
    "SEED=1234 # Semilla para asegurarnos que siempre se generen los mismos valores aleatorios\n",
    "trainset, testset = train_test_split(dataset, test_size=0.3, train_size=0.7, random_state=SEED, shuffle=True, stratify=dataset[\"ocean_proximity\"])\n",
    "\n",
    "# Para esta Unidad solo nos interesan las variables \"median_income\" y \"median_house_value\"\n",
    "trainset = trainset[[\"median_income\",\"median_house_value\" ]].copy()\n",
    "testset = testset[[\"median_income\",\"median_house_value\" ]].copy()\n",
    "\n",
    "# En unidades anteriores habíamos descubierto que \"median_house_value\" tenía un límite superior\n",
    "# que agrupaba todas las manzanas donde la mediana del precio de la vivienda superaba dicho límite\n",
    "# para no condicionar el modelo, esas observaciones son eliminadas\n",
    "# Eliminación de las observaciones con  median_house_value == max\n",
    "# Eliminación de observaciones de Entrenamiento\n",
    "trainset = trainset[trainset[\"median_house_value\"] != dataset[\"median_house_value\"].max()]\n",
    "# Eliminación de observaciones de Test\n",
    "testset = testset[testset[\"median_house_value\"] != dataset[\"median_house_value\"].max()]\n",
    "\n",
    "# Por motivos docentes y de visualización\n",
    "# escalamos \"median_house_value\" en el mismo rango que \"median_income\"\n",
    "# Ver *Nota* superior\n",
    "trainset[\"median_house_value\"] = trainset[\"median_house_value\"].apply(lambda X: X/1000)\n",
    "testset[\"median_house_value\"] = testset[\"median_house_value\"].apply(lambda X: X/1000)\n",
    "\n",
    "# Ploteamos los datos para visualizar la correlación entre las dos variables\n",
    "trainset.plot(x=\"median_income\", y=\"median_house_value\", kind=\"scatter\", alpha=0.1)\n",
    "plt.show()\n",
    "print(\"Correlación entre ambas variables\")\n",
    "print(trainset.corr())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4252662-8ad9-4d80-a1ef-f1df2906606e",
   "metadata": {},
   "source": [
    "### 2.2. Modelo básico a partir de 2 puntos\n",
    "\n",
    "Empezaremos con lo más básico, cogiendo 2 puntos de los datos de entrenamiento para construir un modelo entre ellos. Vamos a trabajar desde la base, es decir, empleando [arrays](https://numpy.org/doc/stable/reference/generated/numpy.array.html) de numpy.\n",
    "\n",
    "**Nota**: Las observaciones seleccionadas están deliberadamente escogidas para acercarse a la recta que mejor describe la relación entre ambas variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f95860-de8e-4b60-9deb-c5af6c891f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import numpy as np\n",
    "except ImportError as err:\n",
    "    !pip install numpy\n",
    "    import numpy as np\n",
    "\n",
    "# En este punto podríamos seleccionar varias observaciones del conjunto de entrenamiento\n",
    "# Nuestro caso de uso no se puede representar fácilmente con una regresión lineal entre solo 2 variables\n",
    "# Por motivos docentes y para poder mostrar mejor los ejercicios\n",
    "# he seleccionado deliberadamente 2 observaciones presentes en el dataset\n",
    "#datos_seleccionados = pd.DataFrame(data={'median_income': [1.0298, 1.9891], 'median_house_value': [300, 500]})\n",
    "datos_seleccionados = pd.DataFrame(data={'median_income': [2, 4], 'median_house_value': [92.6, 202.6]})\n",
    "pto1 = datos_seleccionados.iloc[0] # Devuelve una Serie de Pandas con la fila 0\n",
    "pto2 = datos_seleccionados.iloc[1] # Devuelve una Serie de Pandas con la fil 1\n",
    "\n",
    "# Almacenamos en un array las entradas (inputs o 'x'), es decir, los valores de \"median_income\"\n",
    "x_train = np.array([pto1.loc[\"median_income\"], pto2.loc[\"median_income\"]])\n",
    "\n",
    "# Almacenamos en un array las salidas (outputs, labels, o 'y'), es decir, los valores de \"median_house_value\"\n",
    "y_train = np.array([pto1.loc[\"median_house_value\"], pto2.loc[\"median_house_value\"]])\n",
    "\n",
    "# se podría haber hecho directamente sin calcular los puntos con\n",
    "#x_train = np.array(datos_seleccionados[\"median_income\"], dtype=float)\n",
    "#y_train = np.array(datos_seleccionados[\"median_house_value\"], dtype=float)\n",
    "              \n",
    "# Mostramos los puntos escogidos\n",
    "print(f\"x_train = {x_train}\")\n",
    "print(f\"y_train = {y_train}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7559f8c5-ff42-4976-a25b-577a40888bae",
   "metadata": {},
   "source": [
    "#### Notación\n",
    "- Usaremos $m$ para denotar el número de ejemplos que tenemos para entrenar, es decir, el número de observaciones de nuestro conjunto de entrenamiento (2 en este caso). Podemos emplear <code>len()</code> para obtener el valor o el método [shape()](https://numpy.org/doc/stable/reference/generated/numpy.shape.html) de los arrays de numpy.\n",
    "\n",
    "\n",
    "- Usaremos la notación (x$^{(i)}$, y$^{(i)}$) para denotar la  $i$-esima observación del conjunto de entrenamiento.\n",
    "\n",
    "    -    **Recordad** que en Python (y también en Numpy) los índices empiezan en cero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71322267-2e23-499b-a454-ac000d0f6537",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Número de observaciones del entrenamiento: {len(x_train)}\")\n",
    "i = 0 # i-esima observación\n",
    "x_i = x_train[i]\n",
    "y_i = y_train[i]\n",
    "print(f\"La observación {i} es (x^({i}), y^({i})) = ({x_i}, {y_i})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9507fc-86ce-405c-b557-0f9406ff7e30",
   "metadata": {},
   "source": [
    "Visualicemos en un gráfico los datos de las observaciones seleccionadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121b7b62-1f0f-4c37-9f8e-ac34c6f5324e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotea los puntos\n",
    "plt.figure()\n",
    "\n",
    "plt.scatter(trainset[\"median_income\"], trainset[\"median_house_value\"], alpha=0.1)\n",
    "plt.scatter(x_train, y_train, marker='x', c='r', label=\"Valores reales\")\n",
    "\n",
    "plt.title(\"Mediana del valor de la vivienda\") # Especifica el título\n",
    "plt.ylabel('median house value') # Añade el eje y\n",
    "plt.xlabel('median income') # Añade el eje x\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368d1d14-56fe-442c-b4ad-28d43a283eef",
   "metadata": {},
   "source": [
    "## 3. Modelo de regresión lineal de una variable\n",
    "\n",
    "Recordamos la función del modelo para la regresión lineal de una sola variable:\n",
    "\n",
    "$$ f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{1}$$\n",
    "\n",
    "Esta función solo permite representar líneas rectas (es la ecuación de una recta). Cambiando los valores $w$ y $b$ generamos diferentes rectas, que se podrán ajustar mejor o peor a nuestros datos (el objetivo del entrenamiento es precisamente encontrar los mejores valores para $w$ y $b$). Lógicamente, no podemos escoger unos valores de $w$ y $b$ diferentes para cada observación, sino que necesitamos encontrar los valores que mejor se ajustan, de media, a todas las observaciones del conjunto de entrenamiento.\n",
    "\n",
    "\n",
    "Si escogemos 2 valores cualesquiera para  $w$ y $b$ podremos usar nuestro modelo para predecir la $y$ en base a las entradas $x$.\n",
    "\n",
    "- Para $x^{(0)}$, `f_wb = w * x[0] + b`\n",
    "- Para $x^{(1)}$, `f_wb = w * x[1] + b`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca70911-9497-429b-8977-7d673c8cf67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# esta función recibe las 'x' de todas las observaciones, y devuelve todas las 'y' (las predicciones)\n",
    "def calcula_salida_modelo(x, w, b):\n",
    "    m = len(x)\n",
    "    predicciones = np.zeros(m) # genera un array de ceros con numpy, de la longitud del resultado\n",
    "    for i in range(m):\n",
    "        # función del modelo que se aplica sobre los datos de entrada para unos valores w, b concretos (recibidos como parámetros)\n",
    "        predicciones[i] = w * x[i] + b \n",
    "    return predicciones\n",
    "\n",
    "# esta función dibuja las predicciones (una línea azul por 'plot') y los valores reales (x rojas por 'scatter')\n",
    "def plot_resultados_modelo(inputs, predicciones, labels):\n",
    "    plt.clf() # reset de la figura \n",
    "    # Genera un plot con las predicciones\n",
    "    plt.plot(inputs, predicciones, c='b', label='Predicciones')\n",
    "    # Añade al plot los puntos reales de entrenamiento\n",
    "    plt.scatter(inputs, labels, marker='x', c='r', label='Valores reales')\n",
    "\n",
    "    plt.title(\"Valor medio de la vivienda\")  # Añade un título al gráfico\n",
    "    plt.ylabel('median house value') # Añade el eje y\n",
    "    plt.xlabel('median income') # Añade el eje x\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# seleccionamos valores aleatorios para w y b como punto de partida (los resultados serán malos)\n",
    "w = 100 \n",
    "b = 100\n",
    "print(f\"w: {w}, b: {b}\")\n",
    "\n",
    "predicciones = calcula_salida_modelo(x_train, w, b)  # calculamos las predicciones\n",
    "for i, (x, y, pred) in enumerate(zip(x_train, y_train, predicciones)):\n",
    "    print(f\"x({i})={x} - valor real: {y} valor predicho: {pred:.2f}\")\n",
    "\n",
    "plot_resultados_modelo(x_train, predicciones, y_train)  # dibujamos las predicciones y los valores reales\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dde35ae-14da-4b54-ac34-ac69172f935d",
   "metadata": {},
   "source": [
    "Vemos que las predicciones (azul) están lejos de los valores reales (x rojas). Lógico, la recta ($w$,$b$) fue elegida al azar.\n",
    "\n",
    "**Ejercicio**\n",
    "\n",
    "Prueba a jugar con los valores de $w$ y $b$ para ver cómo varía la recta hasta conseguir unos valores que ajusten bien.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56298e83-e0fa-4767-97f3-985368ef8ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### modifica w y b hasta generar la recta que une los dos puntos\n",
    "w = 100\n",
    "b = 100\n",
    "\n",
    "predicciones = calcula_salida_modelo(x_train, w, b)\n",
    "print(f\"w: {w}, b: {b}\")\n",
    "for i, (x, y,pred) in enumerate(zip(x_train, y_train, predicciones)):\n",
    "    print(f\"x({i})={x} - valor real: {y} valor predicho: {pred:.2f}\")\n",
    "plot_resultados_modelo(x_train, predicciones, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58766a4d-3b26-4dc2-91ad-4a2ea0e68577",
   "metadata": {},
   "source": [
    "\n",
    "**EJERCICIO 1 PARA ENTREGAR EN EL AULA VIRTUAL**\n",
    "\n",
    "La función \"calcula_salida_modelo\" definida anteriormente de forma tradicional puede ser simplificada usando un estilo de programación más propio de Python, haciendo uso de sus capacidades para procesar listas y arrays. Porporciona otra versión de la función \"calcula_salida_modelo\" que sea más sintética.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01c4665-47b6-445b-99de-481ef4b9c618",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EJERCICIO 1 PARA ENTREGAR EN EL AULA VIRTUAL \n",
    "# Escribe otra versión de la función calcula_salida_modelo (mostrada a continuación) que sea más sintética\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d7bbcb-5975-4c77-9099-e443493d61f0",
   "metadata": {},
   "source": [
    "Una vez escogemos unos valores para los parámetros de nuestro modelo que ajustan bien con los datos de entrenamiento, podemos usar el modelo  para predecir con nuevos datos con los que nunca se entrenó. A continuación mostramos las prediciones para dos entradas con sus respectivos valores reales, y vemos que una se aproxima bastante (3.4), mientras que la otra no tanto (4.4). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa68297-588a-4a29-b9a7-90b8a9033b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fijamos los valores de w y b que hemos calculado a mano, y que sí se ajustan a los puntos seleccionados\n",
    "w = 55\n",
    "b = -17\n",
    "\n",
    "x_test = np.array([3.4, 4.4]) # probamos con otros valores reales del dataset\n",
    "y_test = np.array([155.5, 174.1])  \n",
    "\n",
    "y_prediccion = w * x_test + b  # calculamos las predicciones para esos dos valores\n",
    "\n",
    "# imprimimos el resultado para los nuevos puntos, devolviendo sus valores a la escala original\n",
    "print(f\"Precio de una vivienda en el que la familia tiene un ingreso de {x_test[0]*10000}$: {y_prediccion[0]*1000}$\")\n",
    "print(f\"Precio de una vivienda en el que la familia tiene un ingreso de {x_test[1]*10000}$: {y_prediccion[1]*1000}$\")\n",
    "\n",
    "trainset.plot(x=\"median_income\", y=\"median_house_value\", kind=\"scatter\", alpha=0.1)\n",
    "plt.scatter(x_train, y_train, marker='x', c='r', label=\"Puntos iniciales\") # valor real de los puntos iniciales  \n",
    "plt.scatter(x_test, y_test, marker='o', c='r', label='Valores reales') # valor real de los nuevos puntos\n",
    "plt.scatter(x_test, y_prediccion, marker='o', c='cyan', label='Valores predichos')  # valor predicho para los nuevos puntos\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b325e76a-5cec-4076-b41f-99fb304387d3",
   "metadata": {},
   "source": [
    "## 4. Función de coste\n",
    "\n",
    "### 4.1. Definición de la función de coste\n",
    "\n",
    "Tenemos una función con 2 parámetros  ($w$ y $b$)  que debemos definir para ajustar dicho modelo pero, \n",
    "**¿cómo escogemos los valores $w$ y $b$?**\n",
    "$$ f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{1}$$\n",
    "\n",
    "\n",
    "Obviamente, no podemos estar probando con todas las posibles combinaciones \"a mano\". Necesitamos entrenar el modelo, y que se ajusten automáticamente. Para poder decidir que una combinación es mejor que otra, es necesario poder establecer un **coste** de la combinación. De esta forma, podremos comparar combinaciones y escoger entre las diferentes opciones. \n",
    "\n",
    "\n",
    "De forma general, podemos representar la función de coste para los párametros $w$ y $b$ como: $J(w,b)$\n",
    "\n",
    "\n",
    "Existen diferentes funciones de coste pero una habitual en ML es el *Mean Squared Error* (MSE).\n",
    "\n",
    "$$MSE=\\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2\\tag{2}$$\n",
    "\n",
    "y particularmente, en problemas de regresión,  es habitual introducir un factor $\\frac{1}{2}$, quedando la función de coste como: \n",
    "\n",
    "  $$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2 \\tag{3}$$ \n",
    "\n",
    " donde:\n",
    " -  $f_{w,b}(x^{(i)}) = wx^{(i)} + b $ (función del modelo).\n",
    " - $f_{w,b}(x^{(i)})$ es nuestra predicción para la entrada $x^i$ empleando unos parámetros ($w,b$) específicos. Por simplicidad, es habitual representar la predicción como: $\\hat{y}^i$\n",
    "- $(f_{w,b}(x^{(i)}) -y^{(i)})^2$ es la diferencia al cuadrado entre el valor real y la predicción.   \n",
    "- Todas esas diferencias a lo largo de las $m$ observaciones son sumadas y divididas por `2m` para producir el coste final, $J(w,b)$. \n",
    "\n",
    "\n",
    "**Fijaos** que el coste se calcula sobre todas las observaciones de entrenamiento, por lo que el objetivo es obtener los parámetros $w,b$ que mejor se comportan para todas las observaciones.\n",
    "\n",
    " **Importante**: Cuanto más cerca estén las predicciones de los valores reales, menor coste. **Debemos buscar una forma de minimizar la función de coste** para obtener la mejor solución.\n",
    "\n",
    "\n",
    "$$\\underset{w, b}{\\text{minimize }} J(w,b)\\tag{4}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd1bb58-1803-4523-9b2f-e15b85aa22e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculo_coste(x, y, w, b): \n",
    "    \"\"\"\n",
    "    Calcula la función de coste para la regresión lineal\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray (m,)): Entradas, m observaciones \n",
    "      y (ndarray (m,)): salidas\n",
    "      w,b (scalar)    : parámetros del modelo  \n",
    "    \n",
    "    Returns\n",
    "        coste_total (float)\n",
    "    \"\"\"\n",
    "    # observaciones\n",
    "    m = len(x)\n",
    "    cost_sum = 0 \n",
    "    for i in range(m): # recorremos todas las observaciones y sumamos su coste\n",
    "        pred = w * x[i] + b   \n",
    "        cost_sum += (pred - y[i]) ** 2    \n",
    "    return (1 / (2 * m)) * cost_sum # promediamos el coste \n",
    "\n",
    "\n",
    "# Comprobamos los diferentes costes que tendríamos para el mismo conjunto de entrada\n",
    "# cuando variamos los parámetros w, b, probando con (50,50), (100,100) y (150,150)\n",
    "# Podéis probar con otras configuraciones\n",
    "w_pruebas = [50,100,150] # posibles valores para w\n",
    "b_pruebas = [50,100,150] # posibles valores para b\n",
    "\n",
    "print(\"Para los datos de entrenamiento:\")\n",
    "for i,(x,y) in enumerate(zip(x_train, y_train)):\n",
    "    print(f\"(x^{i},y^{i})=({x},{y})\")\n",
    "\n",
    "for w, b in zip(w_pruebas, b_pruebas):\n",
    "    print(f\"El coste total con w={w} y b={b} es: {calculo_coste(x_train, y_train, w, b):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648f9ff9-4ae3-47ff-9036-df048420db7e",
   "metadata": {},
   "source": [
    "### 4.2. Visualización de la función de coste \n",
    "\n",
    "Para mejorar la comprensión de la función de coste, y en qué consiste su minimización, vamos a simplificar el problema y suponer que el valor de $b$ es fijo y que solo tenemos que  buscar el valor óptimo de $w$; es decir, nuestra función solo tiene un parámetro \"libre\". Para cada posible valor de $w$, podemos calcular su función de coste ($J(w)$) y graficar dicho valor. A medida que vayamos calculando valores y graficándolos, iremos dibujando la curva del coste para cada $w$. \n",
    "\n",
    "\n",
    "Si solo tenemos 1 variable \"libre\" y le damos todos los posibles valores, la función de coste que podríamos pintar con los resultados tendría  forma convexa con un mínimo global. Nuestro objetivo minimizando dicha función de coste ($J(w)$) es encontrar dicho mínimo global.  Necesitamos un algoritmo que nos permita movernos por la función de coste hasta alcanzar el mínimo.\n",
    "\n",
    "Veamos la representación gráfica de esta explicación. Fijaos en la figura inferior cómo evoluciona el coste (parte derecha) si movemos el valor de $w$ (parte izquierda). \n",
    "- La figura de la parte derecha refleja en azul la forma de la función de coste si graficásemos todos sus valores (es una aproximación). También se muestra el coste de una configuración concreta (valor de $w$) dentro de la función de coste (el punto rojo). El objetivo es conseguir que el coste concreto (el punto rojo) se situe en el mínimo de la función (esto se produce alrededor de w=55, como habíamos visto)\n",
    "- La figura de la parte izquierda muestra el valor de cada predicción (línea azul) respecto al valor real (puntos rojos). Cuanto más lejos esté el valor de la predicción del valor real, mayor coste.\n",
    "\n",
    "**Nota**: Podéis jugar con el valor fijo de $b$.\n",
    "\n",
    "**Nota 2**: el gráfico es interactivo, podéis modificar el valor de $w$. El refresco puede ser algo lento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e710047-747c-4bc8-a1e4-47990c9a3a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "try:\n",
    "    from lab_utils_uni import plt_intuition, plt_stationary, plt_update_onclick\n",
    "except ModuleNotFoundError:  # en el caso del Colab, no estarán los ficheros auxiliares\n",
    "    repo_path = \"/content/MIOT_ML\"\n",
    "    if not os.path.exists(repo_path):\n",
    "      !git clone http://github.com/dmerap/MIOT_ML  # clonamos el repositorio en el Colab para usar los ficheros auxiliares\n",
    "    else:\n",
    "      print(\"El repositorio ya existe\")\n",
    "    import sys\n",
    "    sys.path.append(repo_path)\n",
    "    from lab_utils_uni import plt_intuition, plt_stationary, plt_update_onclick\n",
    "\n",
    "plt.close()\n",
    "# Esta función está en un fichero de Python con métodos útiles para la práctica\n",
    "# No es necesario que la entendáis. Se emplea solo por motivos docentes\n",
    "plt_intuition(x_train, y_train, fixed_b=-17)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6ccfeb-b74f-463b-8611-488122871cd9",
   "metadata": {},
   "source": [
    "Aumentemos la dificultad y dejemos \"libres\" los 2 parámetros $w$ y $b$. En este caso, para poder graficar la función de coste, necesitamos un gráfico en 3 dimensiones o emplear un gráfico de contornos (aka [isolíneas](https://es.wikipedia.org/wiki/Isol%C3%ADnea)). \n",
    "\n",
    "- El gráfico de la derecha (gráfico de contornos) nos permite seleccionar una combinación de los parámetros $w$ y $b$. Vemos que el centro de los contornos se haya alrededor de $w$=55 y $b$=-17, como habíamos visto.\n",
    "- El gráfico de la izquierda nos permite visualizar la recta y el coste de la selección de parámetros actual.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fef1ec8-5e84-434e-8e68-09c4c20b8b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import ipympl\n",
    "except ImportError as err:\n",
    "    !pip install ipympl\n",
    "    import ipympl\n",
    "\n",
    "## START-Estas líneas se necesitan para ejecutar el Notebook en Colab\n",
    "## IMPORTANTE: la primera vez que se instala ipympl en Colab el gráfico no es interactivo\n",
    "## Debéis ejecutar nuevamente la celda\n",
    "try:\n",
    "    from google.colab import output\n",
    "    print(\"El Notebook se está ejecutando en Colab\")\n",
    "    output.enable_custom_widget_manager()\n",
    "except ModuleNotFoundError:\n",
    "    print(\"El Notebook no se está ejecutando en Colab\")\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "plt.close('all') \n",
    "fig, ax, dyn_items = plt_stationary(x_train, y_train)\n",
    "print(\"\")\n",
    "updater = plt_update_onclick(fig, ax, x_train, y_train, dyn_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed43db3-b617-440b-9593-cc85e664d1f2",
   "metadata": {},
   "source": [
    "## 5. Gradiente descendente\n",
    "\n",
    "### 5.1. Definición\n",
    "\n",
    "Ahora que entendemos cómo funciona una regresión lineal y cómo se calcula el coste de los posibles valores ($w,b$), debemos encontrar una forma de buscarlos de forma automática. Está claro que no podemos ir probando todas las posibles combinaciones de forma manual. \n",
    "\n",
    "¿Cómo minimizamos la función de coste? Empleando el algoritmo de **descenso de gradiente**. \n",
    "\n",
    "$$\\underset{w, b}{\\text{minimize }}J(w,b)\\tag{4}$$\n",
    "\n",
    "\n",
    "Este algoritmo define la forma de actualizar  los parámetros $w$, $b$ empleando la derivada parcial de la función de coste respecto de sus variables ($w$ y $b$). Esta derivada indica la dirección y magnitud del cambio necesario para reducir el error.\n",
    "\n",
    "$$\\begin{align*} \\text{Repetir}&\\text{ hasta que converja:} \\; \\lbrace \\newline\n",
    "\\;  w &= w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{5}  \\; \\newline \n",
    " b &= b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "donde:\n",
    "- $\\alpha$ es la tasa de aprendizaje o *learning rate*.\n",
    "- $\\frac{\\partial J(w,b)}{\\partial w}$ es la derivada parcial de la función de coste respecto a $w$.\n",
    "- $\\frac{\\partial J(w,b)}{\\partial b}$ es la derivada parcial de la función de coste respecto a $b$.\n",
    "\n",
    "La actualización de parámetros implica la derivada parcial de la función de coste respecto de $w$ y de $b$. **No es necesario que aprendais a hacer la derivada parcial**, a continuación os proporciono el resultado de dicha derivada parcial:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(w,b)}{\\partial w}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\tag{4}\\\\\n",
    "  \\frac{\\partial J(w,b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{5}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "**Importante**: Los parámetros $w$, $b$ deben ser actualizados simultaneamente. Esto significa que las derivadas parciales deben calcularse antes de actualizar ninguno de los parámetros.\n",
    "\n",
    "Os resumo los pasos del algoritmo: \n",
    "1. Inicializamos los valores de $w$ y $b$, típicamente con valores aleatorios.\n",
    "2. Calculamos las predicciones.\n",
    "3. Calculamos las derivadas parciales.\n",
    "4. Ajustamos los parámetros $w$ y $b$ en función del gradiente.\n",
    "5. Repetimos hasta cumplir la condición de salida.\n",
    "\n",
    "\n",
    "\n",
    "El tamaño y dirección del \"paso\" para actualizar los parámetros $w$ y $b$ viene marcado por el resultado de la derivada parcial:\n",
    "- **Dirección**: Si la derivada es positiva, debemos disminuir el parámetro; si es negativa, debemos aumentarlo.\n",
    "- **Magnitud**: Qué tan grande es el cambio necesario. Valores mayores significan que el coste cambia rápidamente en esa dirección.\n",
    "\n",
    "El hiperparámetro $\\alpha$ (tasa de aprendizaje o *learning rate*) permite aumentar o reducir el \"paso\" en la actualización de los parámetros. Este hiperparámetro es algo que deberemos escoger y ajustar al entrenar los modelos, y que suele ser clave:\n",
    "- Un $\\alpha$ pequeño puede hacer que no lleguemos a un buen ajuste final del modelo en las iteraciones que le permitamos entrenar.\n",
    "-  Un $\\alpha$ demasiado grande puede resultar en un entrenamieno divergente. \n",
    "\n",
    "\n",
    "Vamos a implementar el descenso de gradiente para una sola variable con la idea de entender los fundamentos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed32ddc-da00-460c-ad74-b7f8d81a5aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculo_gradiente(x, y, w, b): \n",
    "    \"\"\"\n",
    "    Calcula el gradiente para la regresión lineal\n",
    "    Args:\n",
    "      x (ndarray (m,)): datos de entrada, m ejemplos \n",
    "      y (ndarray (m,)): valores de salida\n",
    "      w,b (scalar)    : parámetros del modelo  \n",
    "    Returns\n",
    "      dj_dw (scalar): El gradiente del coste respecto a w\n",
    "      dj_db (scalar): El gradiente del coste respecto a b     \n",
    "     \"\"\"\n",
    "    \n",
    "    # observaciones\n",
    "    m = len(x)    \n",
    "\n",
    "    # valores que devolveremos inicializados a cero\n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "\n",
    "    # calculo de los gradientes\n",
    "    for i in range(m):  \n",
    "        f_wb = w * x[i] + b # cálculo de la predicción\n",
    "        dj_dw_i = (f_wb - y[i]) * x[i]  # un componente del sumatorio para w\n",
    "        dj_db_i = f_wb - y[i]  # un componente del sumatorio para b\n",
    "        dj_db += dj_db_i # añadimos el componente al sumatorio de w\n",
    "        dj_dw += dj_dw_i # añadimos el componente al sumatorio de b\n",
    "    dj_dw = dj_dw / m  # paso final de dividir entre m\n",
    "    dj_db = dj_db / m \n",
    "        \n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9ac4d5-be43-4428-aa71-63c32d28df77",
   "metadata": {},
   "source": [
    "Para una mejor comprensión de los gradientes y la función de coste, vamos a fijar el valor de parámetro $b$ y probar con diferentes puntos para $w$. El gradiente nos devolverá:\n",
    "- **Dirección**: Si la derivada es positiva, debemos disminuir el parámetro; si es negativa, debemos aumentarlo.\n",
    "- **Magnitud**: Qué tan grande es el cambio necesario. Valores mayores significan que el coste cambia rápidamente en esa dirección.\n",
    "\n",
    "**Nota**: En el mínimo global, la derivada será 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a36070-4513-4223-88a0-40d2dda10aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab_utils_uni import plt_gradients\n",
    "plt.close('all') \n",
    "plt_gradients(x_train,y_train, calculo_coste, calculo_gradiente, fixed_b=-17, tested_points=[-200, -150, -100, -50, 0,50,100,150,200, 300])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dba6aea-f38f-441c-a763-5d5cfd91f9a7",
   "metadata": {},
   "source": [
    "### 5.2. Implementación del gradiente descendente\n",
    "Una vez comprendido cómo los gradientes nos ayudarán a saber la dirección y la magnitud de las actualizaciones de nuestros parámetros $w$ y $b$, ya solo necesitamos implementar el algoritmo del gradiente descendente para automatizar el proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda889fa-4187-4747-af85-c0a201092929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradiente_descendente(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function): \n",
    "    \"\"\"\n",
    "    Desarrolla el algoritmo de gradiente descendente para ajustar w y b. \n",
    "    Actualiza los valores w y b realizando num_iters cálculos de gradiente\n",
    "    con una tasa de aprendizaje alpha\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray (m,))  : Datos entrada, m observacines \n",
    "      y (ndarray (m,))  : valores reales de salida\n",
    "      w_in,b_in (scalar): Valores iniciales de w y b  \n",
    "      alpha (float):      Learning rate o tasa de aprendizaje\n",
    "      num_iters (int):    número de iteraciones (épocas) para ejecutar el gradiente descendente.\n",
    "      cost_function:      función que calcula el coste\n",
    "      gradient_function:  función que calcula el radiente\n",
    "      \n",
    "    Returns:\n",
    "      w (scalar): valor calculado de w una vez ejecutado el algoritmo de gradiente descendente.\n",
    "      b (scalar): valor calculado de b una vez ejecutado el algoritmo de gradiente descendente.\n",
    "      J_history (List): histórico de valores de coste \n",
    "      p_history (list): histórico de valores de (w,b)\n",
    "      \"\"\"\n",
    "    \n",
    "    # array para almancear los valores históricos de coste y (w,b)\n",
    "    # Los usaremos para graficar\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    b = b_in\n",
    "    w = w_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calcula el gradiente empleando la función pasada como argumento\n",
    "        dj_dw, dj_db = gradient_function(x, y, w , b)     \n",
    "\n",
    "        # Actualiza los parámetros empleando la ecuación 5\n",
    "        b = b - alpha * dj_db                            \n",
    "        w = w - alpha * dj_dw                            \n",
    "\n",
    "        ### Esta parte del algoritmo es opcional, es solo para guardar el histórico de resultados intermedios y visualizarlos\n",
    "        coste = cost_function(x, y, w , b)\n",
    "        J_history.append(coste ) # almacena el coste en el histórico\n",
    "        p_history.append([w,b]) # almacena los valores de w y b en el histórico\n",
    "\n",
    "        # Mostramos el avance cada 10 iteraciones\n",
    "        if i%10==0:\n",
    "            print(f\"Iteración {i}. Coste {coste}.\",\n",
    "                  f\"dj_dw: {dj_dw: 0.2f}, dj_db: {dj_db: 0.2f}  \",\n",
    "                  f\"w: {w: 0.2f}, b:{b: 0.2f}\"\n",
    "                 )\n",
    "\n",
    " \n",
    "    return w, b, J_history, p_history "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f440f1-04e5-46a0-bb6b-d572120bcd8c",
   "metadata": {},
   "source": [
    "Probemos el algoritmo con nuestros datos:\n",
    "\n",
    "**Nota**: si jugáis con el número de iteraciones, o con los valores de los parámetros de entrada, conseguiréis ajustar mejor el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec00171-597d-417f-b031-a721b64e9976",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_init = 10 # valor inicial de w\n",
    "b_init = 10 # valor inicial de b\n",
    "iteraciones = 3000\n",
    "alpha = 0.01\n",
    "\n",
    "w_calculado, b_calculado, J_history, p_history = gradiente_descendente(x_train, y_train, w_init, b_init, alpha, iteraciones, calculo_coste, calculo_gradiente)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc769061-c368-4cdd-b306-aba5958092fa",
   "metadata": {},
   "source": [
    "Vemos que, partiendo de  $w$=10 y $b$=10, en 3000 iteraciones, los valores finales ($w$=54,27 y $b$=-14,98) son aproximadamente los previstos.\n",
    "Probemos el modelo con los resultados del algoritmo ($w$ y $b$) para los dos puntos por los que empezamos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3275168-4e48-4228-887f-6dbc0203dbdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1429fd57-55d6-4d05-8b62-2da66fd5c531",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicciones = calcula_salida_modelo(x_train, w_calculado, b_calculado)\n",
    "plt.close('all') \n",
    "plot_resultados_modelo(x_train, predicciones,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45eae906-b3b1-4dc0-a020-1835ed9f03fc",
   "metadata": {},
   "source": [
    "### 5.3. Coste vs iteraciones\n",
    "Un gráfico del coste frente a las iteraciones es una medida interesante para valorar el progreso en el descenso de gradiente. El coste siempre debería disminuir en ejecuciones exitosas. El cambio en el coste es muy rápido al comienzo y luego tiende a desacelerar, por lo que, a menudo,  es útil graficar el descenso inicial en una escala diferente a la del descenso final. En los siguientes gráficos, observe la escala del coste en los ejes y el paso de iteración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a862bbd8-1e12-4b1b-a7e4-52fcc0efc95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot coste vs iteraciones  \n",
    "plt.close('all') \n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, constrained_layout=True, figsize=(9,4))\n",
    "\n",
    "ax1.plot(J_history[:100])\n",
    "ax1.set_title(\"Coste vs. iteración(inicio)\")\n",
    "ax1.set_ylabel('Coste')\n",
    "ax1.set_xlabel('Iteración')\n",
    "\n",
    "ax2.plot(100 + np.arange(len(J_history[100:])), J_history[100:])\n",
    "ax2.set_title(\"Coste vs. iteración (final)\")\n",
    "ax2.set_ylabel('Coste') \n",
    "ax2.set_xlabel('Iteración') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31acda7b-7a14-4d79-8b5a-391ce403ef1b",
   "metadata": {},
   "source": [
    "Podemos también visualizar la evolución de los parámetros $w$ y $b$ durante la ejecución del algoritmo de gradiente descendente respecto al gráfico de contornos. Veremos como la evolución de los valores se acerca, en cada iteración, al centro del gráfico, donde se encuentra el mínimo global."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e590f301-2ec8-4e76-9b86-90accece27a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab_utils_uni import plt_contour_wgrad\n",
    "fig, ax = plt.subplots(1,1, figsize=(10, 6))\n",
    "plt_contour_wgrad(x_train, y_train, p_history, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cdb981-d3aa-4bd0-a17a-5a761ef63a39",
   "metadata": {},
   "source": [
    "\n",
    "**EJERCICIO 2 para entregar en el aula virtual**\n",
    "\n",
    "En el entrenamiento previo hemos usado solamente los dos puntos seleccionados desde el inicio, de ahí los buenos resultados obtenidos respecto a la recta que los une.\n",
    "\n",
    "Entrena ahora con todo el conjunto de entrenamiento del dataset y comenta los resultados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0dfa9f-cbf7-470e-9a7a-4e6c4fe52951",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EJERCICIO 2 para entregar en el aula virtual\n",
    "# Escribe el código para entrenar con todo el conjunto de entrenamiento\n",
    "# Ejecútalo y comenta los resultados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a68149-a3a2-4e36-9dc0-1e184baf391d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
