{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "963473bc-9091-4bf9-9462-f9c2373752c9",
   "metadata": {},
   "source": [
    "# Unidad 1 - Análisis y exploración de datos\n",
    "\n",
    "El objetivo principal de esta práctica es que os familiaricéis con los conceptos y las aproximaciones clave para la exploración de datos en el desarrollo de modelos de Aprendizaje Automático (ML). Para ello,  la mayor parte del contenido se dedica a explicar estos aspectos,  apoyándose en ejemplos concretos que ilustran su aplicación a un problema real.  Es crucial que dediquéis tiempo a leer y comprender el material,  en lugar de simplemente ejecutar el código.  Os invitamos a experimentar modificando y variando el código proporcionado para que podáis explorar las distintas opciones y profundizar en su funcionamiento.\n",
    "\n",
    "\n",
    "El Notebook contiene varios ejercicios sencillos. Debéis desarrollarlos durante la clase y enviarlos por el aula virtual de cada universidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65d2c59-914a-4176-88a7-f6ee68172c0d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Referencias útiles para la práctica\n",
    "1. API Pandas: [https://pandas.pydata.org/docs/reference/index.html](https://pandas.pydata.org/docs/reference/index.html)\n",
    "2. API Scikit-learn: [https://scikit-learn.org/stable/api/index.html](https://scikit-learn.org/stable/api/index.html)\n",
    "3. API Seaborn: [https://seaborn.pydata.org/api.html](https://seaborn.pydata.org/api.html)\n",
    "4. Dataset para el ejercicio: [https://www.kaggle.com/datasets/camnugent/california-housing-prices](https://www.kaggle.com/datasets/camnugent/california-housing-prices)\n",
    "5. Géron, Aurélien. Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow. \" O'Reilly Media, Inc.\", 2022. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65285829-2bd2-42e1-9f0c-55fc20ca1458",
   "metadata": {},
   "source": [
    "## Flujo de trabajo básico en problemas de Aprendizaje Automático (*ML workflow*)\n",
    "A la hora de enfrentarnos a un nuevo problema de Aprendizaje Automático (ML), existen una serie de pasos típicos y comunes que debemos afrontar:\n",
    "1. Entender el problema y su contexto.\n",
    "2. Obtener los datos (histórico).\n",
    "3. Explorar, analizar y entender los datos.\n",
    "4. Preparar los datos para los modelos.\n",
    "5. Seleccionar, optimizar y entrenar los modelos ML.\n",
    "6. Evaluar y presentar el modelo seleccionado.\n",
    "7. Desplegar, monitorizar y mantener la solución.\n",
    "\n",
    "La mayoría de los cursos de ML se centran en los pasos 5 y 6, pero la realidad es que entender el problema, su contexto y preparar adecuadamente los datos para desarrollar los modelos, es esencial para obtener buenos resultados y suele implicar el mayor porcentaje de tiempo que dedicamos al desarrollo de modelos. En esta práctica nos centraremos, a modo de introducción, en una primera iteració de los pasos del 1 al 3.\n",
    "\n",
    "\n",
    "## Entender el problema\n",
    "\n",
    "De poco vale saber realizar modelos de ML si no entemos la finalidad y el objetivo de estos. Lo primero que necesitamos tener claro es nuestro problema, su contexto y lo que queremos modelar. En la mayor parte de los problemas del mundo real, el conocimiento experto es clave para poder conseguir buenos resultados, ya que es el que nos va a permitir poder interpretar los datos y los resultados y trabajar en consecuencia. \n",
    "\n",
    "En nuestro problema el conocimiento experto viene dado por el enunciado y vosotros tendréis que analizar los datos para profundizar en dicho conocimiento y contexto antes de empezar a modelar.\n",
    "\n",
    "\n",
    "### Descripción del problema\n",
    "**¡Bienvenido a Machine Learning Housing Corporation!**\n",
    "\n",
    "Tu primera tarea es utilizar datos del censo para construir un modelo que prediga los precios de la vivienda en el estado de California. Estos datos incluyen métricas como la población, el ingreso medio y el precio medio de la vivienda para cada *bloque* (área geográfica) de California. Los bloques son las unidades geográficas más pequeña para las cuales la Oficina del Censo de EE. UU. publica datos (un *bloque* generalmente tiene una población entre 600 y 3.000 personas). *Nos referiremos a estos bloques como **manzanas** durante el ejercicio* \n",
    "\n",
    "Para realizar el ejercicio dispones de un conjunto de datos obtenido del Censo de EE. UU. para el estado de California con las siguientes variables/características/atributos:\n",
    "\n",
    "\n",
    "- **longitude**: Coordenada geográfica de longitud. Un valor más alto indica que está más al oeste.\n",
    "- **latitude**: Coordenada geográfica de latitud. Un valor más alto indica que está más al norte.\n",
    "- **housingMedianAge**: Edad promedio de los habitantes en una manzana.\n",
    "- **totalRooms**: Número total de habitaciones en una manzana.\n",
    "- **totalBedrooms**: Número total de dormitorios en una manzana.\n",
    "- **population**: Número total de personas que residen en una manzana.\n",
    "- **medianIncome**:Ingreso promedio para los hogares en una manzana. \n",
    "- **oceanProximity**: Ubicación de la casa con respecto al océano/mar.\n",
    "- **medianHouseValue**: Valor promedio de la vivienda en una manzana (variable a predecir).\n",
    "\n",
    "![alt text](img/california.png \"California\")\n",
    "\n",
    "### Contexto del problema\n",
    "\n",
    "Más allá de los trabajos académicos, lo habitual es que el desarrollo de un modelo no sea el objetivo final. Típicamente se hará con un fin empresarial o comercial y es importante entender cómo espera la empresa utilizar este modelo y beneficiarse de él. Conocer el objetivo final es importante porque determinará cómo se enmarca el problema, qué algoritmos se seleccionarán, qué medida de rendimiento se utilizará para evaluar el modelo y cuánto esfuerzo se dedicará a ajustarlo.\n",
    "\n",
    "En el marco de este práctica, y tras consultarlo con el gerente de *Machine Learning Housing Corporation*, se nos informa que el resultado del modelo (predicción del precio medio de la vivienda) será la entrada de un modelo posterior (junto con otras entradas), que determinará si vale la pena invertir o no en una zona concreta. Acertar en esa predicción de inversión será clave para los ingresos de nuestra organización. \n",
    "\n",
    "En este punto es clave entender cómo se gestiona actualmente el sistema de inversión y cómo se estima el precio de la vivienda en una zona concreta. Es importante porque la medida actual será nuestra métrica base contra la que compararnos (debemos generar un sistema de predicción que, al menos, mejore el actual). Desde la dirección se nos informa que actualmente es un grupo de expertos el que analiza de forma minuciosa y valora la zona empleando un sistema de reglas complejo. La empresa considera que este sistema es muy costoso y que requiere mucho tiempo, además que sus estimaciones no son precisas (errores medios del 30%).\n",
    "\n",
    "Para poder desarrollar el sistema de predicción del precio medio de la vivienda en una manzana geográfica, nuestra empresa considera que el conjunto de datos obtenido del Censo de EE. UU. para el estado de California es el apropiado, ya que son muy completos y actualizados.\n",
    "\n",
    "\n",
    "\n",
    "## Obtener los datos\n",
    "Para poder analizar los datos lo primero que necesitamos es descargarlos y cargarlos. En el repositorio de la materia encontraréis el dataset que emplearemos para esta práctica (*housing.csv*) pero Kaggle permite descargar directamente la última versión empleando la librería *kagglehub*. \n",
    "También podéis acceder a la web de Kaggle y descargar el dataset desde allí: [https://www.kaggle.com/datasets/camnugent/california-housing-prices](https://www.kaggle.com/datasets/camnugent/california-housing-prices).\n",
    "\n",
    "Para cargar los datos y posteriormente analizarlos, emplearemos la librería [Pandas](https://pandas.pydata.org/).\n",
    "\n",
    "[Pandas](https://pandas.pydata.org/) es una librería de Python de código abierto que proporciona herramientas de análisis y manipulación de datos de alto rendimiento y que podemos considerar el *standar de facto* en proyectos de ML. En esencia, Pandas permite:\n",
    "\n",
    "- **Organizar datos**: Similar a una hoja de cálculo de Excel, pero con multitud de extras. Pandas permite crear estructuras de datos como DataFrames (tablas) y Series (listas) para almacenar y organizar los datos de forma eficiente.\n",
    "- **Limpiar y transformar datos**: Entre otras cosas, Pandas permite limpiar datos (*cleaning data*), gestionar valores faltantes (*data imputation*) y transformar y reorganizar la información para prepararla para el análisis posterior.\n",
    "- **Analizar datos**: Pandas ofrece múltiples funciones para realizar análisis estadísticos, calcular estadísticas descriptivas, agrupar datos, y mucho más.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2b35f4-53e8-4e43-bcdf-2213c38badce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los datos\n",
    "# This package allows to directly download the  CSV from Kaggle\n",
    "try:\n",
    "    import kagglehub\n",
    "except ImportError as err:\n",
    "    !pip install kagglehub\n",
    "    import kagglehub\n",
    "\n",
    "# this library is only to improve the redability of some structures\n",
    "# https://rich.readthedocs.io/en/stable/introduction.html\n",
    "try:\n",
    "    from rich import print\n",
    "except ImportError as err:\n",
    "    !pip install rich\n",
    "    from rich import print\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"camnugent/california-housing-prices\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "#file name: housing.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9ad959-2a70-42ab-8406-8ef40790247c",
   "metadata": {},
   "source": [
    "Una vez descargado, lo podremos cargar con Pandas para poder analizarlo. Pandas permite cargar datos desde multitud de [fuentes](https://pandas.pydata.org/docs/reference/io.html), aunque de forma bastante habitual os encontraréis que los datos os los proporcionarán en CSVs.\n",
    "<code>read_csv(path)</code> es un [método](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) de Pandas que carga los datos desde un fichero CSV y devuelve un DataFrame, que es una de las estructuras de datos básicas de la librería. Un DataFrame es una estructura de datos tabular, similar a una hoja de cálculo, que se utiliza para organizar datos en filas y columnas. El método [read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) tiene muchos parámetros que podemos configurar según el problema, pero en este caso dejaremos los valores por defecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be26c75-8d82-47dc-919c-790d07f93a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ImportError as err:\n",
    "    !pip install pandas\n",
    "    import pandas as pd\n",
    "dataset=pd.read_csv(path+\"/housing.csv\")#Carga datos desde un CSV y devuelve un DataFrame \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa3609b-96dc-4983-aada-51c950f6903e",
   "metadata": {},
   "source": [
    "## Explorar, visualizar y entender los datos\n",
    "\n",
    "A falta de conocimiento experto que nos ayude, deberemos explorar minuciosamente los datos para poder obtener un conocimiento profundo de estos. Lo primero que podemos hacer es mostrar algunas de las filas del dataset para ver que forma tienen los datos. Para ello usaremos el método <code>head()</code> del DataFrame, que muestra las primeras filas (recordad que cada fila representa una manzana)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecef5d18-6148-4a54-b0b0-098b0275116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc91819-68e0-471c-afb8-251fecc8d762",
   "metadata": {},
   "source": [
    "A la hora de empezar a enfrentarnos a los datos, podemos empezar por conocer el tamaño del dataset, el tipo de datos o si los datos contienen nulos.\n",
    "\n",
    "Conocer el **tipo de datos** de las variables en un dataset es crucial para el ML porque determina cómo se procesa y analiza la información. Algunos algoritmos solo funcionan con tipos de datos específicos (numéricos, categóricos), y comprenderlos permite la correcta selección de métodos,  la aplicación de transformaciones adecuadas y un mejor rendimiento del modelo.\n",
    "\n",
    "Los **valores nulos** en un dataset pueden causar problemas al entrenar un modelo de ML, ya que muchos algoritmos no pueden manejarlos directamente. Gestionarlos (eliminándolos o imputándolos) ayuda a prevenir errores, mejorar la precisión del modelo y asegurar resultados más confiables.\n",
    "\n",
    "El método <code>info()</code> nos proporciona un poco más de información. Entre otras cosas nos dice el tipo de datos (en caso de que lo pueda identificar automáticamente). Si conocemos bien los datos, es posible indicar el tipo de cada atributo al cargarlo con <code>read_csv()</code> a través del parámetro <code>dtype</code>.\n",
    "\n",
    "Fijaos que todos los tipos son numéricos, excepto *ocean_proximity* que es de tipo *Object* (una clase genérica de Python). En este caso, si vemos el fichero de datos original, podremos ver que es un tipo categórico (cadena de texto). \n",
    "\n",
    "En el caso de los nulos, se puede observar que del total de filas del dataset, solo un atributo tiene valores faltantes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bdc00e-d446-479c-b220-f8b456cdb057",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ef0954-7688-4aa6-af7c-e22368f490d1",
   "metadata": {},
   "source": [
    "¿Cuántos nulos tienen los atributos del datasset? Se puede calcular automáticamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fd38c8-249f-46ca-b9b8-cd1991d41b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los valores nulos \n",
    "#isnull genera una matriz de booleanos siendo True un nulo. \n",
    "#Los boleanos pueden interpretarse como enteros True=1, False=0\n",
    "#La suma de un dataframe, por defecto, es por columnas\n",
    "dataset.isnull().sum() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f22b649-5a31-4ef0-b6d4-35b2eb091338",
   "metadata": {},
   "source": [
    "Nos interesa conocer el contenido de las variables no numéricas. Una variable de texto que tenga valores diferentes para cada una de las observaciones no nos aporta información pero una variable categórica, en la que todas las observaciones tomen uno de los 'n' posibles valores (siendo 'n' un valor pequeño), si que resulta de utilidad. \n",
    "\n",
    "\n",
    "Comprobemos los valores (categorías) que contiene *ocean_proximity* y cómo se distribuyen las observaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9905e508-3625-46c3-81e1-a9756417f923",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtener los valores de una varaible categórica\n",
    "\n",
    "#dataset['ocean_proximity'].unique()\n",
    "print(\"Valores únicos de ocean_proximity: \", \", \".join(dataset['ocean_proximity'].unique()))\n",
    "\n",
    "#Obtener el número de instancias para cada categoría\n",
    "#dataset['ocean_proximity'].value_counts()\n",
    "print(dataset['ocean_proximity'].value_counts()) #Obtiene los valores únicos de una columna de un DataFrame y las observaciones de cada uno\n",
    "\n",
    "\n",
    "#Obtener el porcentaje de instancias para cada categoría\n",
    "#print(dataset['ocean_proximity'].value_counts()/len(dataset)*100)#Obtiene el porcentaje de observaciones para cada categoría\n",
    "#Lo visualizamos de una forma más apropiada\n",
    "aux=dataset['ocean_proximity'].value_counts()/len(dataset)*100\n",
    "print(\"Porcentaje de observaciones por categoría de 'ocean_proximity'\")\n",
    "for cat in dataset['ocean_proximity'].unique():\n",
    "    print(f\"{cat}: {round(aux[cat],2)}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8787e276-296c-4831-8032-49f98b1f1741",
   "metadata": {},
   "source": [
    "## Datos duplicados\n",
    "Si detectamos observaciones duplicadas en nuestro conjunto de datos, lo habitual es que queramos eliminarlas al comienzo, ya que no queremos influir en la analítica o sobreentrenar los modelos con datos repetidos.  Los DataFrame de Pandas tienen un método (<code>[duplicated()</code>](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.duplicated.html)) para identificar filas duplicadas. En nuestro caso podemos comprobar que no tenemos dicho problema. Si existiese, Pandas nos proporciona un método para eliminarlos [<code>drop_duplicates</code>](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop_duplicates.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57b63d3-14f8-44b5-b358-0c016aa5e0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Filas duplicadas: {dataset.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cb7683-59db-4de1-a12e-70109e55929b",
   "metadata": {},
   "source": [
    "### Reservar un conjunto de Test \n",
    "\n",
    "Puede resultar contraintuitivo reservar voluntariamente una parte de los datos al comienzo de un proyecto. Es natural pensar que antes de tomar decisiones sobre qué algoritmos o técnicas utilizar, deberías analizar a fondo los datos. Aunque esta intuición tiene sentido, también es crucial considerar cómo funciona nuestro cerebro. Si examinas el conjunto de test desde el inicio, podrías identificar patrones aparentemente interesantes que, en realidad, no generalizan bien. Esto puede llevarte a aplicar técnicas de preprocesado o elegir un modelo de aprendizaje automático inapropiado. Como consecuencia, la estimación del error de generalización usando el conjunto de test será demasiado optimista, y el sistema implementado no tendrá el rendimiento esperado. A este fenómeno se le conoce como sesgo de espionaje de datos, y evitarlo es una de las razones clave para separar los datos correctamente desde el principio.\n",
    "\n",
    "Veremos en las clases de teoría cuándo debemos separar los datos en 2 conjuntos(Entrenamiento y Test) o en 3 conjuntos (Entrenamiento, Validación y Test) así como las diferentes formas de repartir los datos (Secuencial, Aleatorio y Aleatorio Estratificado). Por simplificación, en este ejemplo nos centraremos en la división en dos conjuntos. \n",
    "\n",
    "#### División secuencial\n",
    "En una primera aproximación, podemos decidir dividir secuencialmente el dataset, escogiendo, por ejemplo, el primer 70% de los datos para el entrenamiento y el 30% restante para el test (posteriormente este conjunto lo podremos dividir en validación y test si es necesario). Esta aproximación puede ser muy problemática si los datos tienen, en su ordenamiento, dependencias temporales, geográficas, etc. Imaginaos que el dataset tioene datos censales ordenados por año y los datos que usamos para entrenar están centrados en unos años concretos del pasado, mientras que los datos del último año son los que usamos para testear. El problema y el precio de la vivienda ha evolucionado con el tiempo, por lo que los datos del pasado pueden no ser buenos para generalizar una solución actual.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5dd5cb-e59b-488d-a299-72973ee12d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def seleccionar_datos_secuenciales(train_ratio=0.7):\n",
    "    #Función para generar dos conjuntos de datos secuenciales\n",
    "    train_size=int(len(dataset)*train_ratio)\n",
    "    print(f\"Observaciones del conjunto de entrenamiento: {train_size}\")\n",
    "    print(f\"Observaciones del conjunto de test: {len(dataset)-train_size}\")\n",
    "    return dataset[:train_size], dataset[train_size:]\n",
    "\n",
    "\n",
    "trainset, testset=seleccionar_datos_secuenciales()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd01849-f073-48b7-952d-f2a39b70773e",
   "metadata": {},
   "source": [
    "#### División de datos aleatoria\n",
    "Dados los inconvenientes potenciales de repartir los datos de manera secuencial, una posible aproximación es repartirlos de forma aleatoria. Existen diferentes formas de hacerlo pero una de las más habituales es el método <code>train_test_split()</code> proporcionado por la librería sklearn. \n",
    "\n",
    "**IMPORTANTE**: La **repetibilidad** es clave en ML. Cada vez que ejecutemos nuestros analísis y entrenamientos de modelos, deberíamos de conseguir los mismos resultados. A la hora de repartir los datos de forma aleatoria, también debemos de asegurarnos que, tras cada ejecución obtenemos el mismo reparto. En informática no tenemos números aleatorios puros, lo que tenemos son números pseudoaleatorios.  Los números pseudoaleatorios se generan mediante algoritmos deterministas, es decir, siguen una fórmula que, dada una semilla inicial, produce una secuencia de números que parecen aleatorios, pero en realidad son predecibles si se conoce el algoritmo y la semilla. Dado esta característica, es **importante establecer la semilla** que nos permitirá repetir una secuencia aleatoria en el futuro.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc1ad95-085f-4cbf-b2cd-d27009e9cc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Demostración del uso de una semilla para generar números aleatorios\n",
    "try:\n",
    "    import numpy as np\n",
    "except ImportError as err:\n",
    "    !pip install numpy\n",
    "    import numpy as np\n",
    "\n",
    "SEED=1234 #Semilla para asegurarnos que siempre se generen los mismos valores aleatorios\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"Números aleatorios SIN semilla:\",np.random.rand(5))\n",
    "\n",
    "print(10*\"-\")\n",
    "for i in range(3):\n",
    "    np.random.seed(SEED)  # Establecer la semilla\n",
    "    print(\"Números aleatorios CON semilla:\",np.random.rand(5))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2546d8-1fc3-4093-9ab2-203472b1b72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generación de datasets aleatorios con sklearn\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "except ImportError as err:\n",
    "    !pip install sklearn\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainset, testset=train_test_split(dataset, test_size=0.3, train_size=0.7, random_state=SEED, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971204e7-f57d-4314-a381-c4284a344250",
   "metadata": {},
   "source": [
    "#### División estratificada\n",
    "La división \"aleatoria\" pura puede llevar a repartos en conjuntos en los que no se respete la distribución de alguna variable o característica relevante. Imaginemos que tenemos un problema de clasificación de correos electrónicos, en los que la mayoría de nuestros datos son correos válidos, mientras que un porcentaje muy pequeño es de correos Spam. Una división aleatoria podría llevar a que el conjunto de datos de Test no contuviese ningún correo de Spam. Para evitar esto, aparece la división aleatoria estratificada, que permite mantener la distribución de datos, en una variable concreta, entre el conjunto original y los de Test y Entrenamiento.  El método  <code>train_test_split</code> nos permite agregar el parámetro *stratify* para indicar la variable de referencia para realizar la distribución de datos (típicamente esta variable es la variable objetivo en clasificación).\n",
    "\n",
    "Para probar cómo funciona la división estratificada usaremos el método <code>train_test_split</code> y nos centraremos en la variable categórica *ocean_proximity*. Probaremos a dividir los datos SIN y CON estratificación y comprobaremos como se reparten las categorías de *ocean_proximity* en los conjuntos resultantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f527c871-e47d-4935-a480-1fe6d82d61b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribución si estratificado\n",
    "print(\"DATASET ORIGINAL\")\n",
    "print(dataset['ocean_proximity'].value_counts()/len(dataset)*100)#Obtiene el porcentaje de observaciones para cada categoría\n",
    "print(\"ENTRENAMIENTO\")\n",
    "print(trainset['ocean_proximity'].value_counts()/len(trainset)*100)#Obtiene el porcentaje de observaciones para cada categoría\n",
    "print(\"TEST\")\n",
    "print(testset['ocean_proximity'].value_counts()/len(testset)*100)#Obtiene el porcentaje de observaciones para cada categoría\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f790487-4de3-4f7d-bcf4-4e1591fc741d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribución con  estratificado\n",
    "trainset, testset=train_test_split(dataset, test_size=0.3, train_size=0.7, random_state=SEED, shuffle=True, stratify=dataset[\"ocean_proximity\"])\n",
    "print(\"DATASET ORIGINAL\")\n",
    "print(dataset['ocean_proximity'].value_counts()/len(dataset)*100)#Obtiene el porcentaje de observaciones para cada categoría\n",
    "print(\"ENTRENAMIENTO\")\n",
    "print(trainset['ocean_proximity'].value_counts()/len(trainset)*100)#Obtiene el porcentaje de observaciones para cada categoría\n",
    "print(\"TEST\")\n",
    "print(testset['ocean_proximity'].value_counts()/len(testset)*100)#Obtiene el porcentaje de observaciones para cada categoría\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b17bd9-10d0-4da9-98c2-51d8b0cece34",
   "metadata": {},
   "source": [
    "\n",
    "**IMPORTANTE**\n",
    "A partir de este momento toda la analítica la realizaremos sobre el conjunto de entrenamiento (trainset) para evitar el *sesgo de espionaje*. **Recordad** establecer la semilla para asegurar la repetibilidad y que una nueva ejecución de este Notebook lleve a los mismos repartos.\n",
    "\n",
    "\n",
    "### Estadística básica\n",
    "Podemos generar una estadística básica para conocer mejor los datos empleando el método [describe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html) sobre el dataframe que representa nuestro conjunto de entrenamiento. \n",
    "\n",
    "Las filas de **count**, **mean**, **min** y **max**  son autoexplicativas pero es necesario tener en cuenta que los valores nulos son ignorados (variable *total_bedrooms*).  La fila **std** muestra la desviación estándar y las filas 25%, 50% y 75% simbolizan los **percentiles** correspondientes. Un percentil indica el valor por debajo del cual cae un porcentaje dado de observaciones del dataset. Por ejemplo, el 25% de las manzanas tienen un edad media inferior a 18, mientras que el 50% es inferior a 29 y el 75% es inferior a 37. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c8aee9-7fbd-46dd-96ae-db07aad6d5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0d7942-d1bb-439e-a123-5baae3f11126",
   "metadata": {},
   "source": [
    "**¿Os llama algo la atención?**\n",
    "\n",
    "Con estos datos ya llama la atención que la media del salario sea un valor entre 0.5 y 15 (**recordad** que el  punto es el separador decimal) o que el valor medio de la casa tenga un máximo de 500.001,00$ (parece poco respecto a los valores de EE.UU)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1129ac2-f6a7-40aa-bb1e-c39b32954211",
   "metadata": {},
   "source": [
    "#### Histogramas\n",
    "Los histogramas pueden aportarnos una perspectiva rápida de cómo están distribuidos los datos y eso nos puede proporcionar información útil y resaltar posibles anomalías. Pandas permite generar gráficos básicos a través de Matplotlib. \n",
    "\n",
    "Podemos crear un histograma para una sola variable o automaticamente para todas a través del método <code>hist()</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e506f2-a938-4053-9d82-43db1430af4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except ImportError as err:\n",
    "    !pip install matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "#Histograma para la variable 'population'\n",
    "trainset['population'].hist(bins=50, figsize=(12, 8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873b93b1-0fc2-47c2-a38a-2f4d2a3b41e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histograma para todas las variables\n",
    "dataset.hist(bins=50, figsize=(12, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6027fa-c81d-4247-aa96-5de69d58c14d",
   "metadata": {},
   "source": [
    "Otra librería interesante para generar gráficos, que también se basa en Matplotlib, es **Seaborn**.\n",
    "\n",
    "[Seaborn](https://seaborn.pydata.org/index.html) es una librería de Python para visualización de datos estadísticos. Construida sobre matplotlib, proporciona una interfaz de alto nivel para crear gráficos atractivos e informativos. Ofrece una gran variedad de opciones para visualizar distribuciones, relaciones entre variables y patrones en datos multidimensionales, facilitando el análisis exploratorio y la comunicación de resultados.\n",
    "\n",
    "Alternativa ejecutada con **Seaborn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cb0a62-9ad6-40e5-8ea5-ae30103b781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import seaborn as sns\n",
    "except ImportError as err:\n",
    "    !pip install seaborn\n",
    "    import seaborn as sns\n",
    "\n",
    "\n",
    "for col in dataset.columns:\n",
    "    sns.histplot(data=dataset[col], bins=50)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bc3e57-b506-4523-978f-c91265a1c19f",
   "metadata": {},
   "source": [
    "**¿Qué os llama la atención de los histogramas?**\n",
    "\n",
    "A la vista de los histogramas y teniendo en cuenta también los valores generados por el método *describe()* llaman la atención varias cosas:\n",
    "1. Unidades del salario medio (*median_income*): Los datos no vienen en miles de dolares, están escalados. Los datos reprensentan decenas de miles de dolares, es decir, el valor 3 significa 30.0000,00$. El escalado del atributo no supone un problema.\n",
    "2. Limites del salario medio (*median income*): El salario tiene un límite inferior y superior en el que se agrupan salarios. Sobre todo es claro en la parte superior donde se agrupan bastantes valores en la cola.\n",
    "3. El valor medio de las casas tiene un límite superior (*media_house_value*), lo que es un problema para poder modelar, ya que es la variable objetivo (un modelo pensaría que no se puede sobrepasar dicho límite).\n",
    "4. La media de edad (*housing_median_age*) también está limitado.\n",
    "5. Los atributos están en escalas muy diferentes,lo que puede dificultar o impedir aprender a los modelos de ML. En este caso podremos aplicar algún tipo de normalización para solucionarlo.\n",
    "6. Muchos de los histogramas están sesgados a la derecha: se extienden mucho más hacia la derecha de la mediana que hacia la izquierda. Esto puede dificultar la detección de patrones pero podremos intentar transformar estos atributos para que tengan distribuciones más simétricas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e612e1f-fba0-4490-982a-273951e5ee6b",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "**Preguntas**\n",
    "- ¿Qué porcentaje de los datos de entrenamiento tienen el límite mínimo del salario (*median_income*) asociado?\n",
    "- ¿Qué porcentaje de los datos de entrenamiento tienen el límite máximo del salario (*median_income*) asociado?\n",
    "- ¿Qué porcentaje de los datos de entrenamiento tienen como valor de la casa (*median_house_value*) el límite máximo?\n",
    "- ¿Qué porcentaje de los datos de entrenamiento tienen como valor medio de la edad de los habitantes (*housing_median_age*) el límite máximo?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e78644f-205a-4505-8070-d656c736e170",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EJERCICIO ###\n",
    "#Genera el código necesario para responder a las preguntas formuladas\n",
    "#Pista: Podéis comparar un valor concreto contra un (sub) dataset para generar una matriz de booleanos que se pueda sumar\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c61fd53-5390-4105-ace2-c5640a39ee02",
   "metadata": {},
   "source": [
    "### Generación de Hipótesis\n",
    "\n",
    "A medida que se van explorando los datos es habitual empezar a plantearse hipótesis de trabajo que debemos intentar validar o desechar. Este proceso es iterativo pero algo que facilmente nos podemos plantear es que la localización de la vivienda tendrá una importancia alta en su precio.\n",
    "\n",
    "#### Importancia de la localización de la vivienda en el precio\n",
    "En cualquier ciudad o región, la localización es relevante a la hora de valorar una vivienda. Si la vivienda está cerca de una gran ciudad o cerca de la playa suele ser relevante. Este dataset dispone de información geográfica, por lo que podemos visualizar los datos empleando su latitud y longitud e intentar validar nuestra hipótesis.\n",
    "\n",
    "Primero intentaremos simplemente posicionar las observaciones en base a sus coordenadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcf02d9-b4ce-4b92-883a-7f84791b9e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset.plot(x=\"longitude\", y=\"latitude\", kind=\"scatter\", figsize=(10, 7))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d6ee28-3262-43d5-ae96-c3414af20f29",
   "metadata": {},
   "source": [
    "Visualizar los datos nos permite observar la forma de California en el resultado, aunque todavía no podemos obtener ninguna información relevante. Jugando con la visualización, podemos intentar descubrir las zonas más pobladas (típicamente más caras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a3389c-0a37-4875-8cb2-a38f70ead2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha es un parámetro de matplotlib que se lo pasamos a través del método plot del DataFrame\n",
    "#Recordad que Matplotlib está \"por debajo\"\n",
    "#alpha es un nivel de transparencia. Las zonas en las que se acumulan muchos datos\n",
    "#se ven más oscuras\n",
    "\n",
    "trainset.plot(x=\"longitude\", y=\"latitude\", kind=\"scatter\", alpha=0.2, figsize=(10, 7))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b0e5dc-4f84-4cbf-934d-f6c82f5a1692",
   "metadata": {},
   "source": [
    "Como es habitual, la gente tiende a agruparse en áreas concretas, seguramente son áreas cercanas a ciudades importantes. Las ciudades con más población en California son:\n",
    "- Los Ángeles\n",
    "- San Diego\n",
    "- San josé\n",
    "- San Francisco\n",
    "\n",
    "Si buscamos las [coordenadas](https://www.latlong.net/) del centro de cada ciudad y las visualizamos junto al resto de los datos podremos comprobar si son zonas de alta densidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b311c2-e08f-4cfb-bedb-1cac15c26d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Info obtenida de: https://www.latlong.net/\n",
    "LA={\"lat\":34.052235, \"lon\":-118.243683} # Los Angeles\n",
    "SD={\"lat\":32.715736, \"lon\":-117.161087} # San Diego\n",
    "SJ={\"lat\":37.335480, \"lon\":-121.893028} # San Jose\n",
    "SF={\"lat\":37.773972, \"lon\":-122.431297} # San Francisco\n",
    "\n",
    "\n",
    "\n",
    "ax=trainset.plot(x=\"longitude\", y=\"latitude\", kind=\"scatter\", alpha=0.2, figsize=(10, 7))\n",
    "ax.scatter(LA[\"lon\"],LA[\"lat\"], color='red', marker='x', label=\"LA\") \n",
    "ax.scatter(SD[\"lon\"],SD[\"lat\"], color='orange', marker='x', label=\"SD\") \n",
    "ax.scatter(SJ[\"lon\"],SJ[\"lat\"], color='yellow', marker='x', label=\"SJ\") \n",
    "ax.scatter(SF[\"lon\"],SF[\"lat\"], color='black', marker='x', label=\"SF\") \n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deec3a0a-45db-4bc1-8b0c-97ffa0c93ac6",
   "metadata": {},
   "source": [
    "Efectivamente, las grandes ciudades están situadas en zonas altamente pobladas. Para comprobar si las zonas más densas son las más caras, necesitamos incorporar color al gráfico representando el precio medio de la vivienda. Además vamos a generar puntos más o menos grandes en función de la población total de la manzana (observación)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c8a23a-6ab7-4f9b-befc-d7c38d427569",
   "metadata": {},
   "outputs": [],
   "source": [
    "#El parámetro 's' representa el tamaño del marcador en el plot\n",
    "#El parámetro 'c' representa el color\n",
    "#El parámetro cmap indica el mapa de color a emplear\n",
    "ax= trainset.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True,\n",
    "s=trainset[\"population\"] / 100, label=\"population\",\n",
    "c=\"median_house_value\", cmap=\"jet\", colorbar=True,\n",
    "legend=True, sharex=False, figsize=(10, 7))\n",
    "\n",
    "\n",
    "ax.scatter(LA[\"lon\"],LA[\"lat\"], color='black', marker='x',s=100) \n",
    "ax.scatter(SD[\"lon\"],SD[\"lat\"], color='black', marker='x', s=100) \n",
    "ax.scatter(SJ[\"lon\"],SJ[\"lat\"], color='black', marker='x',s=100) \n",
    "ax.scatter(SF[\"lon\"],SF[\"lat\"], color='black', marker='x',s=100) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740558df-13b4-4926-9368-38e82c736f80",
   "metadata": {},
   "source": [
    "**Ejercicio**\n",
    "\n",
    "Aunque los datos parecen claros, repite la gráfica mostrando solamente las manzanas con precio medio superior a la media (*median_house_value*), para comprobar dónde están localizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d405a846-439f-44f0-9284-cd51d0fcc58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EJERCICIO ####\n",
    "#Genera el código necesario para mostrar la gráfica solo con las manzanas que tienen un precio medio superior a la media\n",
    "#Pista: podéis generar una máscara booleana de que observaciones cumplen una condición \n",
    "#y luego generar un sub dataframe en base a esa máscara\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01f725c-803a-4d87-b17d-4bb88f266100",
   "metadata": {},
   "source": [
    "Ya sabemos que estar cerca de una ciudad grande tiende a subir los precios. Esto nos puede ayudar a pensar en nuevas variables que podamos crear (**ingeniería de variables**) que reflejen esto y que podamos incluirlas en el modelo (ej. distancia con LA).\n",
    "\n",
    "#### Importancia de la localización de la vivienda en el precio\n",
    "\n",
    "\n",
    "\n",
    "Pensando en la localización, también podemos considerar que la distancia a  la costa puede ser determinante en el precio.\n",
    "Vamos a analizarlo está vez con *seaborn*, que facilita la forma de graficar con variables categóricas.\n",
    "- Primero graficaremos las observaciones y le daremos color según su categoría en *ocean_proximity*.\n",
    "- Segundo cruzaremos las zonas con los precios medios de las viviendas.\n",
    "\n",
    "\n",
    "Claramente se observa que las zonas próximas al mar son más caras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea5d0e2-145a-4742-9ae6-88420eb1207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primero graficaremos las observaciones y le daremos color según su categoría en ocean_proximity.\n",
    "sns.scatterplot(data=trainset, x=\"longitude\", y=\"latitude\", hue=\"ocean_proximity\")\n",
    "plt.show()\n",
    "\n",
    "#Segundo cruzaremos las zonas con los precios.\n",
    "#El parámetro \"hue\" nos proporciona el color (usamos otra paleta por que seaborn no tiene jet)\n",
    "#El parámetro style nos permite poner formas diferentes en los marcadores\n",
    "sns.scatterplot(data=trainset, x=\"longitude\", y=\"latitude\", hue=\"median_house_value\", style=\"ocean_proximity\", palette=\"OrRd\")\n",
    "plt.show()\n",
    "#Las zonas más caras tienen marcadores diferentes (parámetro style)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5813a6-e152-444b-9d79-c25478fb3762",
   "metadata": {},
   "source": [
    "### Búsqueda de Correlaciones entre variables\n",
    "\n",
    "Es adecuado y típico analizar la correlación entre las variables de nuestro dataset para intuir la importancia de cada una de ellas para el futuro modelo. La mayoría de las librerias nos proporcionan una forma fácil de calcular el **coeficiente de correlación de Pearson**. \n",
    "\n",
    "**Antención**: el coeficiente de correlación de Pearson solo identifica correlaciones lineales (ej. si *'x'* sube entonces *'y'* sube). No tiene en cuenta correlaciones no lineales, las cuales son habituales en muchos problemas reales.\n",
    "\n",
    "La siguiente figura (fuente [wikipedia](https://es.wikipedia.org/wiki/Coeficiente_de_correlaci%C3%B3n_de_Pearson)) muestra el valor de correlación de Pearson ante dos variables (x e y). Fijaos como la fila inferior tiene siempre valor cero, a pesar de que claramente las variables no son independientes\n",
    "\n",
    "\n",
    "![alt text](img/ejCorrelacion.png \"Correlacion Pearson\")\n",
    "\n",
    "\n",
    "\n",
    "Analicemos en nuestro caso las características más correlacionadas con nuestra variable objetivo (*median_house_value*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511a8218-e0f5-41b8-bf4e-52e5187403df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_corr=trainset.corr(numeric_only=True)\n",
    "print(data_corr[\"median_house_value\"].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c448be8-619b-4cf8-8c05-7503d8df4f32",
   "metadata": {},
   "source": [
    "El coeficiente de correlación varía de –1 a 1. Cuando está cerca de 1, significa que hay una fuerte correlación positiva; por ejemplo, el valor promedio de la vivienda tiende a subir cuando el ingreso promedio sube. Cuando el coeficiente está cerca de –1, significa que hay una fuerte correlación negativa; puedes ver una pequeña correlación negativa entre la latitud y el valor promedio de la vivienda (es decir, los precios tienden a bajar ligeramente cuando vas hacia el norte). Finalmente, los coeficientes cercanos a 0 significan que no hay correlación lineal.\n",
    "\n",
    "Interpretación de los coeficientes positivos:\n",
    "- 0 Nula\n",
    "- $>$0.0 – 0.2 Muy baja\n",
    "- $>$0.2 – 0.4 Baja\n",
    "- $>$0.4 – 0.6 Moderada\n",
    "- $>$0.6 – 0.8 Alta\n",
    "- $>$0.8 – $<$1.0 Muy alta\n",
    "- 1.0 Perfecta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bdb2c9-faf5-47b6-af6d-39d7a3c8bb62",
   "metadata": {},
   "source": [
    "Otra forma de analizar la correlación suele ser la matriz de correlación, ya que nos permite calcular la correlación entre todas las variables de forma visual. Emplearemos en este caso **Seaborn**, por la simplicidad de usar el <code>heatmap()</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b42bb06-85ce-4541-87cc-f32ddcd73e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un heatmap con la correlación de Pearson a través de Seaborn\n",
    "sns.heatmap(data_corr, annot=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d55e1f0-7c92-4133-b5ff-b84a3c4fffca",
   "metadata": {},
   "source": [
    "Otra modo de analizar la correlación de atributos es empleando el método <code>scatter_matrix()</code> de Pandas.\n",
    "\n",
    "Cada persona encontrará los gráficos con los que se encuentre más cómodo. También es habitual que algunos gráficos faciliten la interpretación más que otros, dependiendo del problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f23e82c-aff8-4570-8aea-c8ad22a024d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes=[\"median_house_value\", \"median_income\", \"total_rooms\",\"housing_median_age\"]\n",
    "pd.plotting.scatter_matrix(trainset[attributes], figsize=(12, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38feed5d-340a-445e-b072-272cb167ebec",
   "metadata": {},
   "source": [
    "La característica con la correlación más alta y más interesante para profundizar en el análisis es *median_income*.\n",
    "\n",
    "**Ejercicio**: \n",
    "  \n",
    "Genera un scatterplot entre *median_income* y *median_house_value* (puedes emplear Seaborn o Pandas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1107b934-1c85-4acd-9163-e58ad49908de",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ejercicio ###\n",
    "#Genera y analiza un scatterplot entre *median_income* y *median_house_value* (puedes emplear Seaborn o Pandas).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d514ab-7c22-453b-bb51-450cd5a43b91",
   "metadata": {},
   "source": [
    "El diagrama muestra gráficamente el nivel de correlación y se puede ver claramente el límite superior en el precio como una línea horizontal en el precio de 500k. Este *plot* muestra, de forma menos clara,  otras líneas horizontales en torno a los 450k, 350k, 280k, etc. Quizás nos interese eliminar las manzanas que tienen esos límites para prevenir que los modelos \"aprendan\" esos \"artefactos\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40883be5-044f-4318-851b-c7c8cabe9db7",
   "metadata": {},
   "source": [
    "#### Ingeniería de variables\n",
    "\n",
    "A medida que exploramos los datos y los entendemos mejor, nos pueden surgir nuevas hipótesis de trabajo y que esto nos lleve a la creación de nuevas variables (**ingeniería de variables**). Esta parte puede ser más o menos compleja. Lo más simple puede ser buscar combinaciones de características ya existentes que juntas proporcionen más información que por separado. Vamos a generar diferentes combinaciones de atributos y vamos a analizar su correlación para, con suerte, mejorar las métricas.\n",
    "\n",
    "\n",
    "Por ejemplo, el número total de habitaciones en una manzana no es muy útil si no sabes cuántos hogares hay. Lo que realmente nos interesa conocer  es el **número de habitaciones por hogar**. Del mismo modo, el número total de  dormitorios por sí solo no es muy útil y probablemente lo que nos interese es compararlo con el número de habitaciones/habitáculos del hogar, es decir, el **ratio de dormitorios vs habitaciones**. Por último, las **personas por hogar** también parece una combinación de atributos interesante de observar. \n",
    "\n",
    "**Ejercicio**\n",
    "\n",
    "Genera los siguientes atributos nuevos y calcula la correlación de todas los atributos del dataset con la variable objetivo. \n",
    "- $rooms\\_per\\_house=\\frac{total\\_rooms}{households}$.  Media de habitaciones por casa  \n",
    "- $bedrooms\\_ratio=\\frac{total\\_bedrooms}{total\\_rooms}$. Ratio de dormitorios vs habitaciones\n",
    "- $people\\_per\\_house=\\frac{population}{households}$. Media de habitantes por casa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaacd55-e5f7-4b98-81df-b1793f5d4b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ejercicio ###\n",
    "#Genera los siguientes atributos nuevos y calcula la correlación de todas los atributos del dataset con la variable objetivo. \n",
    "# rooms_per_house\n",
    "# bedrooms_ratio\n",
    "# people_per_house\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f077bd07-f4ce-474e-b4d1-844b2817fa6b",
   "metadata": {},
   "source": [
    "Parece que la generación de estas nuevas variables ha aportado mas información. \n",
    "El nuevo atributo *bedrooms_ratio* está  más correlacionado con el valor promedio de la vivienda que el número total de habitaciones o dormitorios (variables que forman el ratio). Aparentemente, las casas con una proporción de dormitorio/habitación más baja tienden a ser más caras. El número de habitaciones por hogar también es más informativo que el número total de habitaciones en un distrito.  Obviamente, cuanto más grandes son las casas, más caras son.\n",
    "\n",
    "Esta ronda de exploración no tiene que ser absolutamente exhaustiva. Necesitamos comenzar a entender el problema  y obtener información que  ayude a obtener un primer prototipo razonablemente bueno. **Este es un proceso iterativo** y, una vez que tengas un prototipo, será necesario analizar su salida  para obtener más información y volver a este paso de exploración."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
