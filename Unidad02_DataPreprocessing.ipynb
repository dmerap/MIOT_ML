{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "033cf518-6486-4895-aaeb-357544cfd583",
   "metadata": {},
   "source": [
    "![alt text](img/MIoT_ML.png \"MIoT_ML\")\n",
    "\n",
    "# Unidad 2: Preparación de los datos. Limpieza y preprocesamiento\n",
    "\n",
    "El objetivo principal de esta práctica es que os familiaricéis con los conceptos y las aproximaciones clave para la limpieza y preparación de los datos (preprocesado) para el desarrollo de modelos de Aprendizaje Automático (ML). Para ello,  la mayor parte del contenido se dedica a explicar estos aspectos, apoyándose en ejemplos concretos que ilustran su aplicación a un problema real. Es crucial que dediquéis tiempo a leer y comprender el material,  en lugar de simplemente ejecutar el código. Os invitamos a experimentar modificando y variando el código proporcionado para que podáis explorar las distintas opciones y profundizar en su funcionamiento.\n",
    "\n",
    "\n",
    "El Notebook contiene varios ejercicios sencillos. Debéis desarrollarlos durante la clase y enviarlos por el aula virtual del curso, en la tarea correspondiente.\n",
    "\n",
    "## Referencias útiles para la práctica\n",
    "1. API Pandas: [https://pandas.pydata.org/docs/reference/index.html](https://pandas.pydata.org/docs/reference/index.html)\n",
    "2. API Scikit-learn: [https://scikit-learn.org/stable/api/index.html](https://scikit-learn.org/stable/api/index.html)\n",
    "3. API Seaborn: [https://seaborn.pydata.org/api.html](https://seaborn.pydata.org/api.html)\n",
    "4. Dataset para el ejercicio: [https://www.kaggle.com/datasets/camnugent/california-housing-prices](https://www.kaggle.com/datasets/camnugent/california-housing-prices)\n",
    "5. Géron, Aurélien. Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow. \" O'Reilly Media, Inc.\", 2022. \n",
    "   \n",
    "## 1. Flujo de trabajo básico en problemas de Aprendizaje Automático (*ML workflow*)\n",
    "A la hora de enfrentarnos a un nuevo problema de Aprendizaje Automático (ML), existen una serie de pasos típicos y comunes que debemos afrontar:\n",
    "1. Entender el problema y su contexto.\n",
    "2. Obtener los datos (histórico).\n",
    "3. Explorar, analizar y entender los datos.\n",
    "4. Preparar los datos para los modelos.\n",
    "5. Seleccionar, optimizar y entrenar los modelos ML.\n",
    "6. Evaluar y presentar el modelo seleccionado.\n",
    "7. Desplegar, monitorizar y mantener la solución.\n",
    "\n",
    "En la Unidad 01 analizamos el problema y los datos disponibles. Solo ha sido una primera iteración, y probablemente sea necesario \"regresar\" a esa etapa, pero, por lo de pronto, prepararemos los datos para poder realizar un primer prototipo de modelo. Nos centraremos, por lo tanto, en el paso 4 del flujo de trabajo básico.\n",
    "\n",
    "¿Qué operaciones haremos durante esta práctica?\n",
    "\n",
    "* Limpieza y preprocesado:\n",
    "    * Gestión de duplicados.\n",
    "    * Eliminación de observaciones.\n",
    "    * Gestión de datos faltantes o nulos (*data imputation*).\n",
    "    * Discretización de variables.\n",
    "    * Gestión de variables categóricas.\n",
    "    * Escalado de las variables.\n",
    "    * Transformación de la distribución de las variables.\n",
    "* Desarrollo de un *pipeline* completo de limpieza y preprocesado.\n",
    "\n",
    "\n",
    "## 2. Carga, división y limpieza de datos\n",
    "\n",
    "### 2.1. Carga de los datos\n",
    "\n",
    "Para dar continuidad a la unidad anterior, es necesario volver a cargar y asegurarse de repartir los datos en los mismos conjuntos (**repetibilidad**).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a85c80-4376-4ea3-a5d3-3896be04e13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los datos\n",
    "# This package allows to directly download the  CSV from Kaggle\n",
    "try:\n",
    "    import kagglehub\n",
    "except ImportError as err:\n",
    "    !pip install kagglehub\n",
    "    import kagglehub\n",
    "\n",
    "# this library is only to improve the redability of some structures\n",
    "# https://rich.readthedocs.io/en/stable/introduction.html\n",
    "try:\n",
    "    from rich import print\n",
    "except ImportError as err:\n",
    "    !pip install rich\n",
    "    from rich import print\n",
    "\n",
    "## pandas library\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ImportError as err:\n",
    "    !pip install pandas\n",
    "    import pandas as pd\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"camnugent/california-housing-prices\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "# file name: housing.csv\n",
    "\n",
    "dataset = pd.read_csv(path+\"/housing.csv\") # Carga datos desde un CSV y devuelve un DataFrame d\n",
    "dataset.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0349f582-3f4e-46ba-b699-ca5b5f588ad0",
   "metadata": {},
   "source": [
    "### 2.2. Datos duplicados\n",
    "Si detectamos observaciones duplicadas, y consideramos que son errores, deberíamos eliminarlas, ya que no queremos sobreentrenar los modelos con datos repetidos ni queremos influir a las métricas de Test con observaciones duplicadas.  Los DataFrame de Pandas tienen un método (<code>[duplicated()</code>](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.duplicated.html)) para identificar filas duplicadas. En nuestro caso podemos comprobar que no tenemos dicho problema. Si existiese, Pandas nos proporciona un método para eliminarlos [<code>drop_duplicates</code>](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop_duplicates.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83032617-d262-4416-9f14-f24268892232",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Filas duplicadas: {dataset.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef381ec-daff-454c-8ea2-3d338efc7706",
   "metadata": {},
   "source": [
    "### 2.3. División de los datos\n",
    "Una vez definido el conjunto de datos que vamos a emplear, debemos ajustar los conjuntos de entrenamiento y test respetando el reparto de la Unidad 01. Durante la primera unidad, la analítica se realizó sobre el conjunto de entrenamiento y en esta segunda unidad, las operaciones de limpieza y preprocesado de los datos se desarrollarán pensando también en el conjunto de entrenamiento.\n",
    "\n",
    "**Importante**: La división de datos debe realizarse sobre el conjunto de datos con exactamente las mismas observaciones. Si modificamos el conjunto, se modificarán los subconjuntos resultantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8046d5-863b-47db-95c3-13077bcb896f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "except ImportError as err:\n",
    "    !pip install sklearn\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "SEED=1234 # Semilla para asegurarnos que siempre se generen los mismos valores aleatorios\n",
    "trainset, testset=train_test_split(dataset, test_size=0.3, train_size=0.7, random_state=SEED, shuffle=True, stratify=dataset[\"ocean_proximity\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0d74d2-a1ac-4e00-b9e7-0e5f7cddeb0d",
   "metadata": {},
   "source": [
    "### 2.4. Eliminación de observaciones\n",
    "\n",
    "Durante el análisis de datos inicial en la Unidad 01, descubrimos que varias características tenían un límite superior. En particular, esto ocurría con la variable objetivo *median_house_value*. Esto puede ser un problema, ya que los modelos *aprenderían* que los inmuebles no pueden superar ese valor. Para el objetivo empresarial de nuestra empresa *Machine Learning Housing Corporation* eso sería un error, por lo que parece adecuado eliminar las observaciones asociadas a ese límite superior para evitar el problema con los modelos que desarrollemos. \n",
    "\n",
    "Esta operación no la realizamos antes para no influir en la división de los conjuntos, ya que modificaremos el número total de observaciones, pero es una modificación que debemos aplicar a todo el dataset, debido a que no queremos entrenar con esos datos pero tampoco testearlo contra ellos, para no desvirtuar las métricas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b6f78d-015a-4dea-bd1f-897381801bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_medianHouseValue = dataset[\"median_house_value\"].max()\n",
    "\n",
    "# Eliminación de observaciones de Entrenamiento\n",
    "print(f\"Número original de observaciones en el conjunto de entrenamiento: {len(trainset)}\")\n",
    "trainset = trainset[trainset[\"median_house_value\"] != max_medianHouseValue]\n",
    "print(f\"Número de observaciones en el conjunto de entrenamiento una vez eliminadas las manzanas con límite superior: {len(trainset)}\")\n",
    "\n",
    "# Eliminación de observaciones de Test\n",
    "print(f\"Número original de observaciones en el conjunto de Test: {len(testset)}\")\n",
    "testset = testset[testset[\"median_house_value\"] != max_medianHouseValue]\n",
    "print(f\"Número de observaciones en el conjunto de Test una vez eliminadas las manzanas con límite superior: {len(testset)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad6faca-28da-4985-ad54-73c3686717f8",
   "metadata": {},
   "source": [
    "**IMPORTANTE**\n",
    "\n",
    "Durante este Notebook vamos a ir viendo diferentes operaciones de limpieza y preprocesado de datos. Vamos a hacer los cálculos y análisis necesarios sobre el conjunto de entrenamiento y veremos el resultado de aplicarlo sobre ese conjunto, PERO **no vamos a consolidar los cambios**. Al final del Notebook crearemos un *pipeline* de operaciones y las ejecutaremos todas juntas. El *pipeline* que generemos será el que posteriormente usemos para aplicar las mismas operaciones sobre el conjunto de Test (o sobre cualquier otro).\n",
    "\n",
    "\n",
    "## 3. Gestión de nulos - *Data imputation*\n",
    "\n",
    "### 3.1. Observaciones con valores desconocidos\n",
    "\n",
    "Trabajar con atributos de los que desconocemos el valor es un problema a la hora de desarrollar modelos, ya que la mayoría de ellos no son capaces de gestionarlos. Las estrategias más habituales para tratarlos son:\n",
    "\n",
    "1. Eliminar todas las observaciones que contengan algún valor desconocido entre sus atributos.\n",
    "    - Requiere tener una gran cantidad de datos, ya que podemos estar reduciendo los datos de forma significativa.\n",
    "2. Eliminar las características que tienen valores desconocidos para todas las observaciones (manteniendo el resto).\n",
    "    - Esta aproximación requiere que los valores desconocidos estén principalmente en ciertas características, y que éstas no tengan excesiva relevancia para el modelo.\n",
    "3. Generar los valores desconocidos de forma estadística empleando, por ejemplo, la media, mediana, moda, etc. del atributo que contiene los nulos.\n",
    "    - Dependerá del contexto del problema la medida estadística a emplear. Esta es una solución rápida y bastante habitual.\n",
    "4. Generar los datos empleando otro modelo del ML que aprenda del resto del dataset.\n",
    "    - Puede ser una solución interesante, pero claramente es más compleja y requiere tiempo.\n",
    "5. Asignarle un valor de forma directa.\n",
    "    - Esta aproximación puede ser válida para asignar un valor que represente los valores desconocidos y así poder trabajar con ellos.\n",
    "   \n",
    "**Nota**: Los alternativas 3, 4 y 5 entran dentro de lo denominado como **Data Imputation**.\n",
    "\n",
    "\n",
    "**Importante**: La estrategia que se seleccione se aplicará sobre todos los datos, PERO cualquier cálculo o decisión relacionado con *Data Imputation* se realizará sobre el conjunto de entrenamiento y se aplicará sobre el conjunto de Test sin analizar su contenido. Ejemplo: si decidimos usar la media para cubrir el valor de los atributos nulos, dicha media se calculará sobre el conjunto de entrenamiento, y se aplicará sobre el conjunto de entrenamiento y sobre el de Test. **Los datos de Test nunca se emplearán para el cálculo**.\n",
    "\n",
    "\n",
    "**Recordatorio**: En nuestro problema, solo la variable *total_bedrooms* contenía valores nulos. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8d07c7-f35d-42d0-83aa-4599b7e6242c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los valores nulos \n",
    "# isnull genera una matriz de booleanos siendo True un nulo. \n",
    "# Los boleanos pueden interpretarse como enteros True=1, False=0\n",
    "# La suma de un dataframe, por defecto, es por columnas\n",
    "trainset.isnull().sum() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b4489e-cf82-4cf0-9f4d-c5e337787f66",
   "metadata": {},
   "source": [
    "\n",
    "**Alternativas para la gestión de nulos:**\n",
    "\n",
    "**Nota**: probaremos las diferentes opciones sobre un dataset auxiliar para no modificar los datasets originales.\n",
    "\n",
    "1. Eliminar **todas las observaciones** que contengan algún valor desconocido entre sus atributos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16b87cf-88bc-40f5-a5fe-1b7618804642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta alternativa se podría aplicar al conjunto completo, \n",
    "# ya que no requiere cálculos sobre los valores de las observaciones\n",
    "# Genero un dataset nuevo con el resultado\n",
    "trainset_sinnulos = trainset.dropna(subset=\"total_bedrooms\") # devuelve un dataframe con las observaciones con nulos eliminadas\n",
    "print(f\"Observaciones del dataset original: {len(trainset)}. Número de atributos: {len(trainset.columns)}\")\n",
    "print(f\"Observaciones del dataset modificado: {len(trainset_sinnulos)}. Número de atributos: {len(trainset_sinnulos.columns)}\")\n",
    "print(\"Valores nulos del dataset:\")\n",
    "trainset_sinnulos.isnull().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231b0fef-951f-4b1c-a797-5e234377d2da",
   "metadata": {},
   "source": [
    "**Alternativas para la gestión de nulos:**\n",
    "\n",
    "2. **Eliminar las características** que tienen valores desconocidos para todas las observaciones (manteniendo el resto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc8fa1b-abb3-4734-bb1b-bd73f03434c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta alternativa se podría aplicar al conjunto completo, \n",
    "# ya que no requiere cálculos sobre los valores de las observaciones\n",
    "# Genero un dataset nuevo con el resultado\n",
    "# axis=1 significa que elimine columnas. axis=0 elimina filas\n",
    "\n",
    "trainset_sinnulos = trainset.drop(labels=\"total_bedrooms\", axis=1) # devuelve un dataframe con las columnas eliminadas\n",
    "\n",
    "print(f\"Observaciones del dataset original: {len(trainset)}. Número de atributos: {len(trainset.columns)}\")\n",
    "print(f\"Observaciones del dataset modificado: {len(trainset_sinnulos)}. Número de atributos: {len(trainset_sinnulos.columns)}\")\n",
    "print(\"Valores nulos del dataset:\")\n",
    "trainset_sinnulos.isnull().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9e40ef-1280-4cad-bbbb-bed197a21b91",
   "metadata": {},
   "source": [
    "**Alternativas para la gestión de nulos** \n",
    "\n",
    "**Aproximaciones basadas en *Data Imputation***\n",
    "\n",
    "3. Generar los valores desconocidos de forma estadística empleando, por ejemplo, la media, mediana, moda, etc. del atributo que contiene los nulos.\n",
    "\n",
    "**Nota**: *Solo desarrollaremos la alternativa 3 como representante de las aproximaciones de *data imputation*.  La aproximación \"5\" es directa, y la aproximación \"4\" implicaría el desarrollo de un modelo ML, y aún no hemos llegado a este punto en las prácticas.*\n",
    "\n",
    "\n",
    " **Recordatorio**: Los cálculos o modelos para realizar este tipo de aproximaciones deben realizarse sobre el conjunto de entrenamiento. Nunca debéis emplear el conjunto de Test.\n",
    "\n",
    "### 3.2. Imputación manual con la mediana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfb9676-94cd-400c-a43b-2ec1a8f297c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculo la mediana sobre el conjunto de test\n",
    "mediana_total_bedrooms = trainset[\"total_bedrooms\"].median()\n",
    "print(f\"Mediana de habitaciones por manzana: {mediana_total_bedrooms}\")\n",
    "\n",
    "# Aplico el valor de la mediana sobre los datos de entrenamiento\n",
    "# Genero una copia del dataset porque no vamos a consolidar los cambios hasta el final\n",
    "# El parámetro implace=True de la función fillna modificaría el dataframe directamente, en lugar de devolver una copia\n",
    "trainset_sinnulos = trainset.copy()\n",
    "trainset_sinnulos[\"total_bedrooms\"] = trainset[\"total_bedrooms\"].fillna(mediana_total_bedrooms) # fillna() es un método del DataFrame\n",
    "print(f\"Observaciones del dataset de entrenamiento: {len(trainset)}. Número de atributos: {len(trainset.columns)}\")\n",
    "print(f\"Observaciones del dataset modificado: {len(trainset_sinnulos)}. Número de atributos: {len(trainset_sinnulos.columns)}\")\n",
    "print(\"Valores nulos del dataset:\")\n",
    "print(trainset_sinnulos.isnull().sum() )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881f5ad3-88b9-46fa-9bf0-851399352ff9",
   "metadata": {},
   "source": [
    "### 3.3. Imputación automática usando Scikit-learn\n",
    "\n",
    "El uso de ***Data Imputation*** está bastante normalizado y, en nuestro caso, será la alternativa que utilicemos. Pero, en esta ocasión, en lugar de hacer los cálculos directamente, emplearemos un objeto de [SimpleImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) de Scikit-Learn, ya que nos facilitará la integración de esta operación con un *pipeline* de preprocesado (último paso).\n",
    "\n",
    "En este punto es importante que expliquemos el concepto de **estimador** ([*estimator*](https://scikit-learn.org/stable/glossary.html#term-estimator)) de Scikit-Learn. Los estimadores son objetos que pueden aprender de los datos, y para ello contienen un método <code>fit()</code>. Existe un tipo especial de estimadores que se denominan **transformadores** (*transformer*) que pueden aprender sobre unos datos y transformar un dataset con la información aprendida a través del método <code>transform()</code>. Los transformadores tienen, además, un método que permite integrar las dos acciones directamente: <code>fit_transform()</code>. \n",
    "\n",
    "Un *SimpleImputer* es un estimador-transformador que puede aprender un estadístico (ej. mediana) y aplicarlo, sustituyendo con él a cualquier valor NaN. Lo interesante es que la interfaz de un estimador es estándard y eso nos ayudará a componer *pipelines* usando la salida de un estimador como la entrada del siguiente.\n",
    "\n",
    "\n",
    "**Importante**: No todas las estrategias valen para todos los tipos de atributos. Por ejemplo, no podemos calcular la media de un atributo categórico, pero sí la categoría más frecuente.\n",
    "\n",
    "\n",
    "**Nota**: Scikit-learn dispone de *Imputers* mas avanzados como, por ejemplo,  [KNNImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html) o [IterativeImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea3aa61-f0d7-4be1-8bb2-a83d1e4f2b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# creamos un objeto de SimpleImputer. En el constructor, definimos el estadístico a calcular.\n",
    "dataimputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "# Inicio Nota:\n",
    "# trainset[[\"total_bedrooms\"]] devuelve un DataFrame con una columna. Lo que necesita el SimpleImputer\n",
    "# trainset[\"total_bedrooms\"] devuelve una Serie de Pandas (una columna)\n",
    "# Fin Nota.\n",
    "\n",
    "\n",
    "dataimputer.fit(trainset[[\"total_bedrooms\"]]) # fit permite aprender la mediana (strategy) de los datos que se pasan al método\n",
    "\n",
    "# Imprimimos el valor calculado sobre el dataset proporcionado\n",
    "print(dataimputer.statistics_)\n",
    "\n",
    "# generamos una copia del trainset para no modificar el original\n",
    "trainset_sinnulos = trainset.copy()\n",
    "\n",
    "# Imputa la media a los datos faltantes de un dataset empleando el método transform()\n",
    "# Los datos imputados los escribimos en el dataset trainset_sinnulos\n",
    "# Nota: el método de Sckit-Learn devuelve un array de numpy, que, si es de una dimensión, puede asignarse directamente como una columna\n",
    "trainset_sinnulos[\"total_bedrooms\"] = dataimputer.transform(trainset_sinnulos[[\"total_bedrooms\"]])\n",
    "\n",
    "\n",
    "# ALTERNATIVA usando fit_transform\n",
    "# trainset_sinnulos = trainset.copy()\n",
    "# trainset_sinnulos[\"total_bedrooms\"] = dataimputer.fit_transform(trainset_sinnulos[[\"total_bedrooms\"]])\n",
    "\n",
    "print(trainset_sinnulos.isnull().sum())\n",
    "\n",
    "\n",
    "# SimpleImputer podría ser aplicado sobre cualquier nuevo conjunto\n",
    "# imputando sus valores nulos con la media calculada sobre el conjunto de entrenamiento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d349520-329c-4bf3-aecd-4a8091ec85a6",
   "metadata": {},
   "source": [
    "## 4. Discretización de variables\n",
    "\n",
    "\n",
    "En algunas ocasiones, puede ser interesante convertir valores continuos en valores discretos. La técnica consiste en agrupar diferentes subrangos numéricos en *discretizaciones* o *buckets*. \n",
    "- Ej. Atributo edad [0..100): menores (0-18) – adulto joven (18-35) – adulto (35-65) – senior (+65)\n",
    "\n",
    "Con esta técnica, estaríamos generando una variable categórica que puede ser de utilidad para el desarrollo de los modelos. \n",
    "En nuestro caso de estudio podríamos plantearnos si categorizar la edad del edificio puede mejorar el entrenamiento del modelo. ¿Podemos considerar que un edificio de 30 años es más barato que uno nuevo en la misma zona? \n",
    "\n",
    "\n",
    "Podemos establecer los grupos \"a mano\" empleando el método [<code>cut()</code>](https://pandas.pydata.org/docs/reference/api/pandas.cut.html) de Pandas o emplear un transformador de Scikit-Learn como [KBinsDiscretizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html).\n",
    "\n",
    "Si nos fijamos en el histograma de la mediana de la edad de las edificaciones de cada manzana, veremos que tiene un límite superior en el que se agrupan todos los inmuebles de más de 52 años. Este dato es interesante y puede ayudarnos a decidir si aplicar la estrategia de categorización sobre dicha variable, ya que por defecto, ya tiene una *categoría* de más de 50.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a41efac-a918-471e-830f-52a188f04a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset[\"housing_median_age\"].plot(kind=\"hist\", bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b87ff49-9534-40b6-aba2-0a6210fa362e",
   "metadata": {},
   "source": [
    "\n",
    "Probaremos a repartir los valores en 4 categorías (representativas de los cuantiles). Recordad que los *transformadores* no devuelven un DataFrame. Tenemos que construirlo con la información que generan. \n",
    "\n",
    "El método <code>get_feature_names_out()</code> devuelve el nombre de las características empleadas para transformar. En este caso, genera nombres para las nuevas características (una por cada categoría creada sobre la característica *housing_median_age*). Podremos usar dichos nombres para crear nuestro DataFrame.\n",
    "\n",
    "La forma de codificar las categorías que se está generando es con la estrategia *one-hot encoding*. La descripción de esta estrategia se realiza en el siguiente paso de preprocesado (Transformación de las variables categóricas a numéricas).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ea7e18-1143-4446-bc2d-057f57290e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "# creamos un objeto de la clase KBinsDiscretizer configurado para crear 4 bins en base a los cuantiles\n",
    "discretizer = KBinsDiscretizer(n_bins=4, encode='onehot-dense', strategy='quantile')\n",
    "\n",
    "# calculamos y aplicamos la discretización\n",
    "aux = discretizer.fit_transform(trainset[[\"housing_median_age\"]])\n",
    "print(discretizer.get_feature_names_out())\n",
    "\n",
    "# fit_transform() no devuelve un DataFrame. Tenemos que crearlo.\n",
    "# Respetaremos el índice del conjunto de entrenamiento y los nombres usados en la transformación\n",
    "housing_median_age_cat = pd.DataFrame(aux, columns=discretizer.get_feature_names_out(), index=trainset.index)\n",
    "\n",
    "print(housing_median_age_cat.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21b739b-b41f-4665-add6-5389a559d5e7",
   "metadata": {},
   "source": [
    "## 5. Gestión de variables categóricas\n",
    "### 5.1. Transformación de las variables categóricas a numéricas.\n",
    "\n",
    "\n",
    "A menudo nos encontraremos con datasets con variables categóricas (como el caso *ocean_proximity*) pero, sin embargo, una buena parte de los modelos ML existentes están limitados a trabajar con variables numéricas, por lo que tendremos que transformar los valores categóricos a numéricos. Tenemos diferentes estrategias que deberemos evaluar de forma separada para decidir qué hacemos con cada atributo: \n",
    "\n",
    "\n",
    "* ¿Tiene sentido mantener esa característica? Si un atributo tiene una gran cantidad de posibles valores (ej. un nombre) tendremos que plantearnos si tiene sentido mantenerlo o si, simplemente, es mejor eliminarlo.\n",
    "     - Para determinar si un atributo categórico es relevante para el modelo, podemos analizar su variabilidad. Si las categorías del atributo se repiten poco o son prácticamente constantes, es probable que no aporten valor predictivo y puedan ser eliminadas.\n",
    "* Si un atributo solo tiene dos posibles categorías (binomial) solo necesitamos modificar el atributo para que sus dos valores sean representados por 1/0 (true/false).\n",
    "* Asignar un número a cada posible categoría de un atributo  (***label encoding***).\n",
    "   - **IMPORTANTE**: esta estrategia solo puede emplearse si las categorías tienen un orden entre ellas (ej. frío, templado, caliente). Si entre ellas no se puede asignar un orden (ej. rojo, verde, azul) entonces esta **aproximación es incorrecta** y debe gestionarse de otra forma (ej. *one-hot encoding*).\n",
    "* Cuando trabajamos con valores categóricos que no tienen un orden intrínseco y el conjunto de posibles valores es limitado, podemos utilizar la técnica conocida como ***one-hot encoding***. Esta estrategia consiste en crear automáticamente una nueva variable para representar cada valor categórico posible de la característica original. Para cada observación, se asigna un valor de 1 a la nueva variable correspondiente si esa observación presenta el valor categórico en cuestión, mientras que las demás variables reciben un valor de 0. \n",
    "\n",
    "\n",
    "| ![encoding](img/encoding.png) | \n",
    "|:--:| \n",
    "| *Label encoding vs one-hot encoding. Fuente original: Wikipedia* |\n",
    "\n",
    ". \n",
    "\n",
    "\n",
    "\n",
    "En el contexto de *Machine Learning Housing Corporation* y el atributo *ocean_proximity* parece que podríamos establecer un orden (distancia respecto al mar), aunque sería complicado establecer la prioridad entre \"NEAR OCEAN\" y \"NEAR BAY\".\n",
    "\n",
    "**EJERCICIO**\n",
    "- Utiliza la clase  [OrdinalEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html) de Scikit-Learn para codificar el atributo *\"ocean_proximity\"* Genera una asignación entre categorías y números siguiendo la aproximación *label encoding*. \n",
    "- Por defecto, OrdinalEncoder asigna un número  por orden de aparición de la categoría. **Esto no es apropiado en nuestro caso**, ya que queremos establecer un orden concreto dependiendo de la cercanía al mar.\n",
    "- Modifica el parámetro *categories* del constructor de [OrdinalEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html) para configurar el siguiente orden: \"ISLAND\",\"NEAR OCEAN\", \"NEAR BAY\", \"<1H OCEAN\", \"INLAND\"\n",
    "- Imprime el resultado de ejecutar el método <code>fit_transform()</code>\n",
    "- **Importante**: no modifiques el dataset de entrenamiento en el proceso. No asignes el resultado generado por <code>fit_transform()</code>.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2df9699-21b2-4417-9a2a-58f6c066f2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ejercicio ###\n",
    "# Utiliza la clase OrdinalEncoder de  Scikit-Learn para codificar el atributo \"ocean_proximity\"\n",
    "# Genera una asignación entre categorías y números siguiendo la aproximación *label encoding*. \n",
    "# Por defecto, OrdinalEncoder asigna un número  por orden de aparición de la categoría. Esto no es apropiado en nuestro caso\n",
    "# Modifica el parámetro \"categories\" para configurar el siguiente orden: \"ISLAND\",\"NEAR OCEAN\", \"NEAR BAY\", \"<1H OCEAN\", \"INLAND\"\n",
    "# Imprime el resultado de ejecutar el método fit_transform\n",
    "# IMPORTANTE: no modifiques el dataset de entrenamiento en el proceso.\n",
    "# No asignes el resultado generado por <fit_transform().\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a45987-fc79-4dd6-a59a-0060d270bf56",
   "metadata": {},
   "source": [
    "Aunque la estrategia de asignar directamente números a categorías podría ser válida en nuestro problema, debido al orden intrínsico de los posibles valores, la realidad es que la estrategia denominada *one-hot encoding* es la más habitual y es la que desarrollaremos en este caso para la versión final. Para realizar esta operación de preprocesado, emplearemos el *transformador* de Scikit-Learn [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html).\n",
    "\n",
    "[OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) procesa un DataFrame y genera una matriz con tantas columnas como valores categóricos, en las que cada fila representa una observación y el cruce filas con columnas tiene el valor 1 si esa observación tiene ese valor categórico asociado o 0 en caso contrario.\n",
    "\n",
    "Por defecto, el *transformador* OneHotEncoder genera una matriz de tipo *sparse* para ahorrar espacio, ya que típicamente será una matriz con multitud de columnas y filas en las que la mayoría de los valores serán cero. Para favorecer la comprensión del ejercicio, forzaremos a que devuelva una matriz densa (*sparse_output=False*)\n",
    "\n",
    "El método <code>get_feature_names_out()</code> nos permite obtener los nombres de las columnas formadas por el nombre del atributo original y la categoría concreta analizada.\n",
    "\n",
    "Fijaos que nuestro objeto OneHotEncoder queda \"cargado\" con la información, lo que luego nos permitirá aplicar el mismo procesado sobre el conjunto de Test o sobre cualquier otro.\n",
    "\n",
    "\n",
    "Como siempre, el resultado del *transformador*, es un array de numpy, por lo que tendremos que construir un DataFrame con él, si es necesario.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c71bdc-de91-443f-9408-915435a470c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#sparse_output=False fuerza a que el resultado sea una matriz completa\n",
    "encoder=OneHotEncoder(sparse_output=False)\n",
    "encoded_ocean=encoder.fit_transform(trainset[[\"ocean_proximity\"]])\n",
    "print(encoder.get_feature_names_out())\n",
    "\n",
    "\n",
    "\n",
    "# Crear un DataFrame con las columnas codificadas\n",
    "encoded_ocean_df = pd.DataFrame(encoded_ocean,columns=encoder.get_feature_names_out(),index=trainset.index)\n",
    "print(encoded_ocean_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0493352b-b24a-44b6-8e1a-292bd1dbd422",
   "metadata": {},
   "source": [
    "**EJERCICIO**\n",
    "\n",
    "Pandas contiene un método que permite hacer *one-hot encoding* de forma fácil y directa: [get_dummies()](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html). \n",
    "\n",
    "Prueba el el código proporcionado (modifica y experimenta lo que consideres oportuni) y comenta en una celda del Notebook las razones por las que no deberíamos usar este método en nuestro proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f499aa5-fbbd-4882-bf02-25c005afc285",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EJERCICIO ###\n",
    "#Analiza el API del método, prueba y modifica el siguiente código como consideres y\n",
    "#explica las razones por las que consideras que este método puede no ser apropiado\n",
    "\n",
    "#Primero codificamos el dataset de entrenamiento\n",
    "print(trainset.columns)\n",
    "cod_trainset=pd.get_dummies(trainset, columns=[\"ocean_proximity\"])\n",
    "print(cod_trainset.columns)\n",
    "print(cod_trainset.head())\n",
    "\n",
    "#Suponemos que en este punto entrenamos el modelo con los datos \n",
    "\n",
    "#Suponemos que nos llega un conjunto de Test en producción\n",
    "test_nuevo=testset[0:2].copy()#solo tiene 2 observaciones\n",
    "\n",
    "#procesamos el test_nuevo, para poder pasarlo al modelo que suponemos entrenado\n",
    "cod_test_nuevo=pd.get_dummies(test_nuevo, columns=[\"ocean_proximity\"])\n",
    "print(cod_test_nuevo.columns)\n",
    "\n",
    "# Aquí pasaríamos las observaciones de test al modelo\n",
    "#¿Problemas?\n",
    "\n",
    "#Imaginamos que nos llega en producción un nuevo subconjunto de test con una categoría nueva\n",
    "\n",
    "test_nuevo2=testset[2:4].copy()\n",
    "test_nuevo2.at[0,\"ocean_proximity\"]=\"Far away\"\n",
    "#Lo codificamos\n",
    "cod_test_nuevo2=pd.get_dummies(test_nuevo2, columns=[\"ocean_proximity\"])\n",
    "print(cod_test_nuevo2.columns)\n",
    "# Aquí pasaríamos las observaciones de test al modelo\n",
    "#¿Problemas?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461fa048-69be-46fb-bde5-fb9012d66334",
   "metadata": {},
   "source": [
    "**¿Observáis los problemas potenciales?**\n",
    "\n",
    "Explica en esta celda las razones por las que el método <code>get_dummies()</code> puede no ser apropiado\n",
    "#\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf47a2b-5e1c-4652-b6f2-7125dab08e46",
   "metadata": {},
   "source": [
    "## 6. Escalado de las características\n",
    "\n",
    "A menudo, en los problemas que tenemos que modelar, nos encontraremos con datasets que contienen atributos que están en escalas diferentes (ej. nº de habitaciones y precio de la vivienda), esto puede tener un impacto no despreciable en los modelos que entrenemos\n",
    "(dependerá de lo sensible que sea el modelo escogido). Los modelos que tengan en cuenta distancias (KNN, SVM, etc.), emplean los atributos para calcular distancias entre observaciones y con eso determinan su similitud. Si gestionan características con escalas muy diferentes, pueden dar más peso a las características con valores más altos, distorsionando los resultados. \n",
    "\n",
    "Otro grupo de modelos en los que diferentes escalas pueden impactar en su rendimiento, son aquellos que emplean el descenso de gradiente para ajustar sus parámetros (ej. ANN, linear regression, etc.). En la fórmula que se usa para actualizar los pesos y que calcula el “paso” que se va a dar, aparece el valor del atributo ($x_j$). Si los atributos están en escalas muy diferentes, los pasos serán de tamaños diferentes dificultando la convergencia del modelo.\n",
    "\n",
    "\n",
    "| $\\Huge\\theta_j:=\\theta_j -\\alpha \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}-y^{i})x_j^{i})$| \n",
    "|:--:| \n",
    "| Gradiente descendente |\n",
    "\n",
    "\n",
    "Por otro lado, existen otro tipo de modelos que son insensibles a las escalas y no requieren el escalado de las variables (aunque tampoco les perjudica) como, por ejemplo, los modelos basados en árboles.\n",
    "\n",
    "\n",
    "Las dos alternativas más habituales para modificar la escala de los atributos son:\n",
    "\n",
    "* **Normalización** (aka. *min-max scaling*): cambia (mapea) la escala de los datos a otra, típicamente entre 0 y 1, pero puede ser entre un valor ‘a’ y otro ‘b’. El problema es que requiere saber el valor mínimo y máximo posible para ese atributo en el momento de entrenar y esto no siempre es posible. Si tenéis un problema en el que desconocéis cómo evolucionará un atributo en producción, entonces no debéis usar este método (todos los valores que surjan por encima del que se detectó como máximo durante el entrenamiento, serán mapeados al mismo valor y pasará lo mismo con el mínimo).\n",
    "\n",
    "| $\\Huge X^´=a+\\frac{(X-X_{min})(b-a)}{X_{max}-X_{min}}$| \n",
    "|:--:| \n",
    "| Fórmula para normalizar los datos entre los valores [a,b]|\n",
    "\n",
    "* **Estandarización** (aka. *Z-Transformation*): es otro método de escalado en el que se transforman los valores para que estos estén centrados alrededor de la media con desviación típica 1. Se realiza en base a un dataset de entrenamiento en el que se calcula su media y su desviación típica y dichos valores se emplean para transformar los datos del conjunto a través de la fórmula mostrada en la siguiente tabla. Los valores de media y desviación típica calculados sobre el conjunto de entrenamiento se emplearán posteriormente para estandarizar los datos de Test. La ventaja de este método es que se puede emplear en producción, aunque los valores reales varíen con el tiempo. No se necesita conocer los máximos y mínimos de cada atributo como en el caso de la normalización.\n",
    "\n",
    "| $\\Huge X^´=\\frac{X-\\mu}{\\sigma}$| \n",
    "|:--:| \n",
    "| Fórmula para estandarizar los datos generando un dataset con media 0 y desviación típica 1.|\n",
    "\n",
    "\n",
    "\n",
    "Scikit-Llearn proporciona clases para escalar los datos a través de cualquiera de las aproximaciones mencionadas:\n",
    "\n",
    "* Normalización:[MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler)\n",
    "* Estandarización: [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "  \n",
    "\n",
    "**Nota**: El proceso de escalado solo tiene sentido en variables numéricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8162299-6685-4b20-bed3-02a04bf5e0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejemplo con estandarización\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scaler = StandardScaler()\n",
    "#seleccionamos solo las variables numéricas para escalar\n",
    "scaled_trainset=std_scaler.fit_transform(trainset.select_dtypes(include=\"number\"))\n",
    "print(scaled_trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e6eb86-2f0b-4794-9e13-b9deec94b1cc",
   "metadata": {},
   "source": [
    "**EJERCICIO**\n",
    "\n",
    "Repite el escalado pero esta vez aplica la normalización (aka min-max scaler)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d66ac1-9989-4e63-a556-225409720385",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EJERCICIO\n",
    "#Repite el escalado pero esta vez aplica la normalización (aka min-max scaler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ec66d3-1188-45d3-a30b-258a0a089840",
   "metadata": {},
   "source": [
    "## Transformación de la distribución de las características\n",
    "\n",
    "Si al analizar la distribución de una característica nos encontramos con un escenario en el que existe una \"cola pesada\" (*heavy tail*),es decir, cuando existe una gran asimetría y aparecen muchos valores alejados de la media, tanto la normalización como la estandarización comprimirán la mayoría de los valores en un rango pequeño y esto es algo con lo que los modelos de aprendizaje automático no trabajan bien. Existen diferentes métodos para intentar transformar los valores de las características buscando una mayor simetría.\n",
    "\n",
    "Para características positivas con una \"cola pesada\" a la derecha, una aproximación común es remplazar el valor por su raíz cuadrada. Si la característica tiene una cola muy larga y pesada, entonces reemplazar los valores de la característica por su logaritmo puede ayudar.\n",
    "\n",
    "Veamos un ejemplo con el caso de *population* en nuestro problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b852adc8-6e21-4942-8963-20e73b982239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import math\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except ImportError as err:\n",
    "    !pip install matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"Valor máximo: {trainset['population'].max()}\")\n",
    "sns.histplot(trainset['population']).set_title(\"Histograma original\")\n",
    "plt.show()\n",
    "squared_trainset=trainset[\"population\"]**1/2\n",
    "\n",
    "\n",
    "sns.histplot(squared_trainset).set_title(\"Transformando la distribución con la raíz cuadrada\")\n",
    "plt.show()\n",
    "\n",
    "log_trainset=trainset[\"population\"].apply(math.log)\n",
    "\n",
    "sns.histplot(log_trainset ).set_title(\"Transformando la distribución con el logaritmo\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a327753-e063-47a3-9393-c0929bbfdd99",
   "metadata": {},
   "source": [
    "Aplicando el logaritmo nos queda una forma más simétrica y apropiada para el desarrollo de los modelos.\n",
    "\n",
    "Conforme a lo analizado en la Unidad 01, esta estrategia podría ser interesante para las siguientes características: \n",
    "- *total_rooms*.\n",
    "- *total_bedrooms*.\n",
    "- *population*.\n",
    "- *households*.\n",
    "- *median_income*.\n",
    "\n",
    "\n",
    "La **discretización** de una variable es otra posible aproximación para gestionar distribuciones asimétricas con \"colas pesadas\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a8e51f-9a80-4887-8fea-ab49c841be8c",
   "metadata": {},
   "source": [
    "### Uso de *transformadores* personalizados para el preprocesado\n",
    "\n",
    "Scikit-Learn proporciona muchos e interesantes *transformadores* para poder limpiar y preprocesar nuestros datos pero también es posible crear nuestros propios *transformadores* a traves de [FunctionTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html)\n",
    "\n",
    "¿Por qué es interesante? Nos va permitir integrar el proceso en un pipeline automático (lo veremos más adelante).\n",
    "\n",
    "Veremos en el siguiente ejemplo como crear una *transformador* para aplicar la operación de logaritmo, que nos permita crear una distribución más simétrica para la variable de *population* (sección anterior) o cualquiera de las otras. **Fijaos** que se puede definir la operación inversa para recrear los datos originales.    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e42d8c-2ce8-4d92-9bd3-4ea687bba68d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import numpy as np\n",
    "except ImportError as err:\n",
    "    !pip install numpy\n",
    "    import numpy as np   \n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "\n",
    "log_transformer = FunctionTransformer(np.log, inverse_func=np.exp)\n",
    "log_trainset = log_transformer.transform(trainset[[\"population\"]])\n",
    "\n",
    "sns.histplot(log_trainset).set_title(\"Transformando la distribución con el logaritmo\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d027819-ef2b-44ff-936f-2b45331bb106",
   "metadata": {},
   "source": [
    "## Ingeniería de variables\n",
    "\n",
    "En esta fase nos enfocaríamos en crear nuevas caracterísiticas que pudiesen ayudar a los modelos. En este problema podríamos crear diferentes características. Algunas tan complejas como calcular la distancia de cada manzana a la ciudad grande más cercana, ya que vimos en la Unidad 01 que podía ser determinante. Por ahora nos centraremos en la creación de variables formadas a partir de la combinación de otras ya existentes, puesto que vimos en la Unidad 01 que tenían más correlación que las originales.\n",
    "\n",
    "Recordemos los ratios que analizamos en la Unidad 01:\n",
    "- $rooms\\_per\\_house=\\frac{total\\_rooms}{households}$.  Media de habitaciones por casa  \n",
    "- $bedrooms\\_ratio=\\frac{total\\_bedrooms}{total\\_rooms}$. Ratio de dormitorios vs habitaciones\n",
    "- $people\\_per\\_house=\\frac{population}{households}$. Media de habitantes por casa\n",
    "\n",
    "Para crear estas variables e integrar el proceso en el *pipeline* de limpieza y preprocesado, nos ayudaremos de los *transformadores* personalizados. Veremos cómo hacerlo de forma independiente, aunque en la versión final, que integremos en el *pipeline*, tendremos que hacer alguna variación para que se integren con el resto de *transformadores*. Los siguientes 3 ejemplos irán aumentando de complejidad hasta obtener un *transformador* genérico para el cálculo de ratios entre 2 variables cualesquiera.\n",
    "\n",
    "\n",
    "**Importante**: La ingeniería de variables se ha presentado en este punto para poder integrala a través de los *FunctionTransformer* pero las variables generadas tienen que pasar por el mismo proceso que el resto como, por ejemplo, el escalado.\n",
    "\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55689f24-5c9d-457d-acee-b39b2e430cc1",
   "metadata": {},
   "source": [
    "**Ejemplo 1**\n",
    "\n",
    "FunctionTransformer para crear la variable $rooms\\_per\\_house$.\n",
    "* <code>lambda X: X[\"total_rooms\"] / X[\"households\"]</code>: es una función anónima (lambda) que recibe un dataset $X$ y devuelve el ratio de 2 de sus columnas <code> X[\"total_rooms\"] / X[\"households\"]</code>\n",
    "\n",
    "\n",
    "\n",
    "**Nota**: Las **funciones lambda** son funciones anónimas y compactas que se definen en una sola línea, ideales para operaciones simples. Se crean usando la palabra clave *lambda*, seguida de los parámetros, dos puntos, y una expresión. Las funciones se crean anónimas si consideramos que no van a tener más que un uso puntual.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd4cb1c-519b-4ded-a392-69b3cc628f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos  la FunctionTransformer con la función que queremos aplicar\n",
    "rooms_per_house_transformer=FunctionTransformer(lambda X: X[\"total_rooms\"] / X[\"households\"])\n",
    "\n",
    "print(trainset[[\"total_rooms\", \"households\"]].head())\n",
    "\n",
    "#aplicamos la transformación\n",
    "ratio=rooms_per_house_transformer.transform(trainset)\n",
    "\n",
    "print(ratio[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2f451e-dfd0-43c7-98e5-93196cd84838",
   "metadata": {},
   "source": [
    "**Ejemplo 2**:\n",
    "\n",
    "En el ejemplo anterior, creando una nueva variable, no tenemos control sobre el nombre que se le asigna. Si queremos incluir este proceso en un *pipeline*, necesitamos poder asignar un nombre automáticamente. Es posible asignar otra función *lambda* al parámetro *feature_names_out* con el objetivo de generar el nombre:\n",
    "* El parámetro <code>feature_names_out</code> requiere un *callable* (función *lamda*) con dos argumentos: *self* y un array con los nombres de las características de entrada (*input_feature names*). Esos nombres los podemos emplear para generar el nombre de la nueva variable. Para acceder al nombre es necesario llamar al método <code>get_feature_names_out()</code> de nuestro objeto *FunctionTransformer* y pasarle una lista con las *inputs feature names*.\n",
    "\n",
    "Al poder generar los nombres automáticamente y tener los valores de la nueva variable, podemos crear un DataFrame con el resultado.\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6410dcf-d4da-4cdc-9746-a030db14c4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos  la FunctionTransformer con la función que queremos aplicar\n",
    "rooms_per_house_transformer=FunctionTransformer(lambda X: X[\"total_rooms\"] / X[\"households\"],\n",
    "                                                feature_names_out=lambda self,input_names: [f\"{input_names[0]}_ratio_{input_names[1]}\"])\n",
    "\n",
    "print(trainset[[\"total_rooms\", \"households\"]].head())\n",
    "\n",
    "\n",
    "#aplicamos la transformación\n",
    "ratio=rooms_per_house_transformer.transform(trainset)\n",
    "rooms_per_house_df=pd.DataFrame(ratio, \n",
    "                             columns=rooms_per_house_transformer.get_feature_names_out([\"total_rooms\",\"households\"]),\n",
    "                             index=trainset.index)\n",
    "\n",
    "print(rooms_per_house_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf794a4-5727-49ed-b015-7c5176b321fa",
   "metadata": {},
   "source": [
    "**Ejemplo 3**:\n",
    "\n",
    "Siguiendo el ejemplo 2, podríamos generar 3 *FunctionTransformer*, uno por cada nueva característica pero, teniendo en cuenta que las 3 se generan de la misma manera, es posible desarrollar un *FunctionTransformer* más general pensando en la integración con el *pipeline*.\n",
    "\n",
    "\n",
    "\n",
    "**Nota**: En esta ocasión le asignaremos un nombre a las funciones, ya que no es obligatorio que sean anónimas. Tal y como se comentó anteriormente, lo normal es generarlas anónimas si van a tener un uso puntual y son muy simples (ej. 1 línea). \n",
    "\n",
    "\n",
    "**Importante**: en el siguiente ejemplo construimos una aproximación general teniendo en cuenta que estamos trabajando con DataFrames y es lo que recibimos como *input*. Cuando integremos estos transformadores en el *pipeline* final y se genere un flujo en el que las salidas de unos son las entradas de los siguientes,  tendremos que variarlo para que operan sobre arrays de numpy, ya que las funciones y métodos de Scikit-Learn generan no generan DataFrames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ba087c-c9bd-41f4-aa0a-bb89087a9fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_ratio(X):\n",
    "#accede a la primera columna del dataframe (0) y a la segunda (1) y devuelve el ratio\n",
    "    return X.iloc[:,0] / X.iloc[:,1]\n",
    "    \n",
    "def ratio_name(function_transformer, feature_names_in):\n",
    "# generamos el nombre empleando la información de las entradas\n",
    "#Se espera que esta función devuelva un array de strings\n",
    "    return [f\"{feature_names_in[0]}_ratio_{feature_names_in[1]}\"] \n",
    "\n",
    "\n",
    "#Creamos nuestro objeto FunctionTransformer\n",
    "#La función para transformar es: column_ratio()\n",
    "#La función para generar el nombre de la nueva característica es ratio_name()\n",
    "ratio_transformer=FunctionTransformer(column_ratio,feature_names_out=ratio_name) # FunctionTransformer genérico para poder crear ratios\n",
    "\n",
    "\n",
    "\n",
    "#aplicamos la transformación genérica y creamos un dataframe con los datos\n",
    "people_per_house_df=pd.DataFrame(ratio_transformer.transform(trainset[[\"population\", \"households\"]]), \n",
    "                             columns=ratio_transformer.get_feature_names_out([\"population\", \"households\"]),\n",
    "                             index=trainset.index)\n",
    "\n",
    "print(people_per_house_df.head())#Visualizamos las primeras filas del dataframe\n",
    "\n",
    "#aplicamos la transformación genérica y creamos un dataframe con los datos\n",
    "bedrooms_ratio_df=pd.DataFrame(ratio_transformer.transform(trainset[[\"total_bedrooms\", \"total_rooms\"]]), \n",
    "                             columns=ratio_transformer.get_feature_names_out([\"total_bedrooms\", \"total_rooms\"]),\n",
    "                             index=trainset.index)\n",
    "print(bedrooms_ratio_df.head())#Visualizamos las primeras filas del dataframe\n",
    "\n",
    "\n",
    "#aplicamos la transformación genérica y creamos un dataframe con los datos\n",
    "rooms_per_house_df=pd.DataFrame(ratio_transformer.transform(trainset[[\"total_rooms\", \"households\"]]), \n",
    "                             columns=ratio_transformer.get_feature_names_out([\"total_rooms\", \"households\"]),\n",
    "                             index=trainset.index)\n",
    "\n",
    "print(rooms_per_house_df.head())#Visualizamos las primeras filas del dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c87b344-5fa9-4ac8-9088-a6a010694a87",
   "metadata": {},
   "source": [
    "## Desarrollo de un pipeline\n",
    "\n",
    "Durante este Notebook hemos visto diferentes procesamientos que se pueden realizar sobre un dataset antes de empezar a entrenar los modelos. Estas operaciones no son las únicas y no todos los problemas requieren las mismas. El tratamiento de los datos (analítica, limpieza y preprocesado) es la fase que habitualmente más tiempo consume (tiempo de la persona especialista) en el desarrollo de modelos. Es importante dedicar el tiempo necesario porque los modelos que desarrollemos serán tan buenos como los datos que tengamos. **Modelos complejos o tiempo de entrenamiento no sustituye una buena preparación y limpieza de datos**.\n",
    "\n",
    "Una vez hemos limpiado y preparado los datos, tendremos una serie de pasos que deben ser ejecutados en un orden concreto, primero sobre el conjunto de  entrenamiento y, posteriormente, sobre el conjunto de Test  ( o sobre los datos que recibamos una vez desplegado el modelo). \n",
    "\n",
    "Scikit-Learn proporciona una clase [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) que nos permitirá crear una secuencia de operadores y que nos ayudará a crear este flujo de operaciones.\n",
    "\n",
    "#### Pipeline para gestionar características numéricas\n",
    "Veamos un ejemplo sencillo del funcionamiento del *Pipeline* centrado en la imputación de datos (gestión de nulos) y una estandarización posterior para tratar con características numéricas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fa4df1-e5a6-4300-a7c1-eb080ce5b42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "num_pipeline = Pipeline([(\"impute\", SimpleImputer(strategy=\"median\")),(\"standardize\", StandardScaler())])\n",
    "print(num_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3732e9a6-43ed-4452-81df-9eedd896545c",
   "metadata": {},
   "source": [
    "El contructor de la clase Pipeline requiere una lista de pasos/operaciones. Cada paso es una tupla (nombre,[estimador](https://scikit-learn.org/stable/glossary.html#term-estimator)). Todos los estimadores incluidos en la lista de pasos, excepto el último, deben ser del tipo *transformador*, es decir, tienen que proporcionar el método <code>fit_transform()</code>. El último paso puede contener un estimador de cualquier tipo (incluyendo un *transformador*). El *nombre* en la tupla es la forma en que nombramos al paso/operación. Solo es necesario tener cuidado que no se repita y tener en cuenta que no puede contener dos guiones bajos (\"__\").\n",
    "\n",
    "Si no queremos darles nombres a los pasos y que estos se generen automáticamente, también es posible emplear la función [make_pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html) que los generará por nosotros empleando el nombre de las clases de los *transformadores*. **Fijaos** que en este caso, los pasos/operaciones NO se pasan dentro de una lista de Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c7b427-33eb-4a9d-a0b2-bbfbd3055306",
   "metadata": {},
   "outputs": [],
   "source": [
    "#los estiamdores se pasan separados por \",\". No van dentro de una lista\n",
    "num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n",
    "print(num_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f9040d-25e8-48d0-8669-3ea30a839741",
   "metadata": {},
   "source": [
    "El objeto *pipeline* nos proporcionará los mismos métodos que el estimador final. Como en nuestro caso el último paso también es un *transformador*, el *pipeline* funcionará también como un *transformador*, por lo que podemos usar el método <code>fit_transform()</code>.\n",
    "\n",
    "Aplicaremos esta *pipeline* solo a las características numéricas empleando <code>select_dtypes(include=\"number\")</code>\n",
    "\n",
    "\n",
    "**Recordad** que los métodos de Scikit-Learn no devuelven un DataFrame, por lo que tendremos que convertirlo nuevamente si queremos mostrarlo como un DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2b5d76-3b99-4ed2-a35b-d0b27dd52995",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print(trainset.select_dtypes(include=\"number\").head())\n",
    "print(10*\"-\")\n",
    "\n",
    "preprocessing_num_pipeline_df = pd.DataFrame(num_pipeline.fit_transform(trainset.select_dtypes(include=\"number\")),\n",
    "                                     columns=num_pipeline.get_feature_names_out(),\n",
    "                                     index=trainset.index)\n",
    "print(preprocessing_num_pipeline_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb70e2f-d00d-4b54-997f-46cb69fe994c",
   "metadata": {},
   "source": [
    "#### Pipeline para gestionar características categóricas\n",
    "Las operaciones de limpieza y preprocesado de las características categóricas son diferentes que para las numéricas. Veamos ahora un ejemplo de la creación de un *pipeline* simple para trabajar con las características categóricas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a0e24d-e432-45ae-8e7c-c8a9c61060e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generamos el pipeline asociado a las características categóricas\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "print(trainset.select_dtypes(include=\"object\").head())#seleccionamos solo las características categóricas\n",
    "#pipeline formado por:\n",
    "#un SimpleImputer que introduce el valor más repetido en caso de existir un nulo\n",
    "#Un codificador OneHotEncoder para convertir las categorías a números\n",
    "cat_pipeline = make_pipeline(SimpleImputer(strategy=\"most_frequent\"),OneHotEncoder(handle_unknown=\"ignore\",sparse_output=False))\n",
    "\n",
    "\n",
    "preprocessing_cat_pipeline_df = pd.DataFrame(cat_pipeline.fit_transform(trainset.select_dtypes(include=\"object\")),\n",
    "                                     columns=cat_pipeline.get_feature_names_out(),\n",
    "                                     index=trainset.index)\n",
    "print(10*\"-\")\n",
    "print(preprocessing_cat_pipeline_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc35fd3-737e-4fdd-a8b1-5686b19d4913",
   "metadata": {},
   "source": [
    "#### Pipeline para gestionar características numéricas y categóricas\n",
    "Por ahora hemos tratado con las características numéricas y categóricas por separado. Sería más conveniente tener un \"selector\" capaz de gestionar los dos tipos y aplicar la parte del *pipeline*  oportuna en cada caso. \n",
    "\n",
    "Para poder gestionar qué operaciones aplicamos a qué características, Scikit-Learn proporciona la clase [ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html). Esta clase permite ejecutar determinadas operaciones sobre determinadas columnas de un DataFrame , por lo que podremos escoger cuáles se ejecutan sobre las características numéricas y cuáles sobre las categóricas. Veamos un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3358ae41-bf7b-4590-bcb1-4c680517b7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_selector, make_column_transformer, ColumnTransformer\n",
    "\n",
    "cat_attribs = [\"ocean_proximity\"] #generamos una lista con las características categóricas\n",
    "num_attribs=trainset.columns.tolist()#generamos una lista con las características categóricas\n",
    "num_attribs.remove(\"ocean_proximity\") #Los atributos numéricos son todos menos ocean_proximity\n",
    "\n",
    "\n",
    "#Generamos el pipeline asociado a las características numéricas\n",
    "num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n",
    "#Generamos el pipeline asociado a las características categóricas\n",
    "cat_pipeline = make_pipeline(SimpleImputer(strategy=\"most_frequent\"),OneHotEncoder(handle_unknown=\"ignore\",sparse_output=False))\n",
    "\n",
    "\n",
    "\n",
    "#Creamos el ColumnTransformer en el que asociamos cada pipeline a las características concretas\n",
    "#ColumnTransformer requiere una lista de tripletes (nombre, pipeline, lista_atributos)\n",
    "preprocessing_selector_pipeline=ColumnTransformer( [(\"num\", num_pipeline, num_attribs),\n",
    "                                                    (\"cat\", cat_pipeline,cat_attribs )\n",
    "                                                   ])\n",
    "\n",
    "##También es posible usar \"make_column_transformer\" para evitar nombrar a los diferentes pipelines\n",
    "##make_column_transformer NO recibe los pipelines dentro de una lista de Python.\n",
    "#preprocessing_selector_pipeline=make_column_transformer( (num_pipeline, num_attribs),(cat_pipeline,cat_attribs ))\n",
    "\n",
    "\n",
    "\n",
    "#ALTERNATIVA\n",
    "#Se puede sustituir la lista de atributos por una función que selecciona TODOS los atributos de un determinado tipo\n",
    "#para no tener que generar la lista a mano\n",
    "#make_column_selector(dtype_include=np.number)\n",
    "#preprocessing_selector=ColumnTransformer( [(\"num\", num_pipeline, make_column_selector(dtype_include=np.number)),(\"cat\", cat_pipeline, make_column_selector(dtype_include=object))])\n",
    "\n",
    "\n",
    "preprocessing_cat_num_pipeline_df= pd.DataFrame(preprocessing_selector_pipeline.fit_transform(trainset), \n",
    "                                    columns=preprocessing_selector_pipeline.get_feature_names_out(),\n",
    "                                    index=trainset.index)\n",
    "print(preprocessing_cat_num_pipeline_df.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3836e588-0936-4367-b534-8488c443a2a6",
   "metadata": {},
   "source": [
    "## Desarrollo del  *Pipeline* final\n",
    "\n",
    "Ahora lo que nos queda es hacer la composición del *pipeline* completo con todas las operaciones que hemos visto durante el Notebook. Lo primero que haremos es separar las salidas (*Outputs*) de las características de entrada (*Inputs*), ya que el procesamiento lo haremos sobre las entradas del modelo.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c587c91a-5c2e-412a-8550-d2ae317ae01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output para nuestro modelo\n",
    "output=[\"median_house_value\"]\n",
    "\n",
    "#Inputs para nuestro modelo. Todas las características menos la de salida\n",
    "inputs=trainset.columns.to_list()\n",
    "inputs.remove(output[0])\n",
    "\n",
    "#Creamos una copia de un dataframe para los valores de salida\n",
    "trainset_output=trainset[output].copy()\n",
    "\n",
    "#Creamos una copia de un dataframe para los valores de entrada\n",
    "trainset_inputs=trainset[inputs].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddaf7c9-7035-4eef-9674-af246b1727cc",
   "metadata": {},
   "source": [
    "A continuación desarrollaremos el *pipeline* completo para limpiar y preprocesar las características de entrada al modelo. **Este proceso tiene una cierta complicación**. Prestad atención y preguntad lo que considereis necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bd507c-d4c1-4141-b036-563827d9e01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler,KBinsDiscretizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import  ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "#1. Operaciones necesarias para la creación de variables nuevas \"rooms_per_house\", \"bedrooms_ratio\", \"people_per_house\"\n",
    "##a. Data Imputation\n",
    "##b. Ingeniería de variables\n",
    "##c. Escalado de variables\n",
    "\n",
    "#Las siguientes funciones son una adaptación de las vistas anteriormente \n",
    "#para crear Transformers personalizados (FunctionTransformer)\n",
    "#Las adaptaciones están centradas en la gestión de ndarrays en lugar de DataFrames\n",
    "def column_ratio(X):\n",
    "    #función para crear el ratio de 2 variables\n",
    "    #Esta creación será parte de un pipeline y en ese flujo, los datos será un ndarray,\n",
    "    #ya que las operaciones de Scikit-Learn no generan dataframes de forma natura\n",
    "    return X[:, [0]] / X[:, [1]]\n",
    "\n",
    "\n",
    "def ratio_name(function_transformer, feature_names_in):\n",
    "    ##función para generar los nombres  con el FunctionTransformer\n",
    "    return [\"ratio\"] # feature names out\n",
    "\n",
    "\n",
    "#pipeline específico para crear las variables nuevas\n",
    "ratio_pipeline= make_pipeline(SimpleImputer(strategy=\"median\"),\n",
    "                              FunctionTransformer(column_ratio,feature_names_out=ratio_name),\n",
    "                              StandardScaler())\n",
    "\n",
    "\n",
    "#pipeline específico para modificar las distribuciones empleando el logaritmo\n",
    "log_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"),\n",
    "                             FunctionTransformer(np.log, feature_names_out=\"one-to-one\"),\n",
    "                             StandardScaler())\n",
    "\n",
    "\n",
    "#pipeline específico para gestionar la variable categórica ocean_proximity\n",
    "#a. imputando la categoría más frecuente, en caso de existir nulos\n",
    "#b. aplicando el OneHotEncoder para codificar las categorías\n",
    "cat_pipeline = make_pipeline(SimpleImputer(strategy=\"most_frequent\"),OneHotEncoder(handle_unknown=\"ignore\",sparse_output=False))\n",
    "\n",
    "\n",
    "#pipeline específico para discretizar la antigüedad de las viviendas \"housing_media_age\"\n",
    "discret_pipeline=make_pipeline(SimpleImputer(strategy=\"median\"),\n",
    "                               KBinsDiscretizer(n_bins=4,encode='onehot-dense', strategy='quantile'),\n",
    "                               OneHotEncoder(handle_unknown=\"ignore\",sparse_output=False)\n",
    "                              )\n",
    "\n",
    "\n",
    "\n",
    "#Usaremos ColumnTransformer para indicar qué pasos operaciones se aplican a qué características\n",
    "#1. Operaciones para generar atributos nuevos con ratio_pipeline\n",
    "#2. Operaciones para modificar la distribución \n",
    "#3. Operaciones para gestionar la variable categórica ocean_proximity\n",
    "#4. Operaciones para discretizar la variable housing_median_age\n",
    "full_pipeline_preprocessing = ColumnTransformer([(\"bedrooms\", ratio_pipeline, [\"total_bedrooms\", \"total_rooms\"]),\n",
    "                                   (\"rooms_per_house\", ratio_pipeline, [\"total_rooms\", \"households\"]),\n",
    "                                   (\"people_per_house\", ratio_pipeline, [\"population\", \"households\"]),\n",
    "                                   (\"log\", log_pipeline, [\"total_bedrooms\", \"total_rooms\", \"population\",\"households\", \"median_income\"]),\n",
    "                                   (\"cat\", cat_pipeline, [\"ocean_proximity\"]),\n",
    "                                  (\"disc\", discret_pipeline, [\"housing_median_age\"])\n",
    "                                   \n",
    "                                  ])\n",
    "\n",
    "\n",
    "\n",
    "preprocesing_trainset_inputs_df = pd.DataFrame(full_pipeline_preprocessing.fit_transform(trainset_inputs), \n",
    "                                    columns=full_pipeline_preprocessing.get_feature_names_out(),\n",
    "                                    index=trainset_inputs.index)\n",
    "\n",
    "\n",
    "print(preprocesing_trainset_inputs_df.head())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bd7bd4-243e-4a64-ab7c-901b5e6705d1",
   "metadata": {},
   "source": [
    "### Aplicación del pipeline a otros datasets\n",
    "\n",
    "Una vez tengamos entrenado un modelo, este siempre esperará el mismo tipo de entradas y generará el mismo tipo de salida. Si nos llega un nuevo conjunto de observaciones, tendremos que aplicar exactamente las mismas operaciones de limpieza y de preprocesado (en el mismo orden y con la misma configuración). La ventaja de tener todo estructurado en un *pipeline* es que podemos reutilizarlo para los nuevos conjuntos. \n",
    "\n",
    "**Importante**: cuándo usemos nuevamente el *pipeline* no podemos volver a configurarlo, tenemos que ejecutarlo con la configuración de entrenamiento, por lo que para ejecutarlo **NO** podemos usar el método <code>fit_transform()</code>, **tenemos que usar el método** <code>transform()</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91493950-dc4f-4062-af3f-ce703762e258",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_inputs=testset[inputs].copy()\n",
    "testset_output=testset[output].copy()\n",
    "\n",
    "preprocesing_testset_inputs_df = pd.DataFrame(full_pipeline_preprocessing.transform(testset_inputs), \n",
    "                                    columns=full_pipeline_preprocessing.get_feature_names_out(),\n",
    "                                    index=testset_inputs.index)\n",
    "\n",
    "\n",
    "print(preprocesing_testset_inputs_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2909cd0-32ea-42b8-bde0-cd6a42b192e3",
   "metadata": {},
   "source": [
    "## Guardado los datos\n",
    "Si queremos emplear los datos preprocesados en otra Unidad, es conveniente que salvemos el DataFrame modificado para su uso posterior.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea413d52-1541-4f89-b1d4-0ec268a5cf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardado de las observaciones preprocesadas de entrenamiento (Inputs)\n",
    "preprocesing_trainset_inputs_df.to_csv(\"./preprocessing_trainset_inputs.csv\",index_label=\"idx\")\n",
    "#Guardado de las observaciones de entrenamiento (Outputs)\n",
    "trainset_output.to_csv(\"./trainset_ouputs.csv\",index_label=\"idx\")\n",
    "\n",
    "#Guardado de las observaciones preprocesadas de test (Inputs)\n",
    "preprocesing_testset_inputs_df.to_csv(\"./preprocessing_testset_inputs.csv\",index_label=\"idx\")\n",
    "testset_output.to_csv(\"./testset_outputs.csv\",index_label=\"idx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257dc5ff-d557-4c0e-871e-f3cae00b573e",
   "metadata": {},
   "source": [
    "## Entrenamiento de los modelos\n",
    "\n",
    "Hemos llegado al final de la unidad para la limpieza y preprocesamiento de datos. En este punto ya tenemos un dataset de entrenamiento preparado para entrenar un modelo. Recordad que los pasos de limpieza y preprocesado no tienen que ser siempre los mismos, dependen del problema y de las características. En esta unidad solo hemos visto algunos ejemplos, quedan muchas otras operaciones que se podrían realizar, pero es una primera aproximación para poder empezar a desarrollar modelos de aprendizaje automático\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
