{"cells":[{"cell_type":"markdown","metadata":{"id":"5aQKQMJTJBPH"},"source":["# Unidad 7: Aprendizaje por Refuerzo\n","\n","En este *Notebook*, implementaremos un agente para jugar al \u003cb\u003e[FrozenLake](https://gymnasium.farama.org/environments/toy_text/frozen_lake/).\u003c/b\u003e\n","\n","![alt text](https://gymnasium.farama.org/_images/frozen_lake.gif)\n","\n","\n","**Descripci贸n**\n","\n","El objetivo de este juego es ir \u003cb\u003edesde la posici贸n inicial (S, Start) a la posici贸n final (G, Goal)\u003c/b\u003e caminando entre placas congeladas (F, Frozen) de un lago evitando los agujeros en el hielo (H, holes).\n","\n","Los agujeros en el hielo se distribuyen en ubicaciones fijas al usar un mapa predeterminado. Existe la posibilidad de generar mapa con agujeros en ubicaciones aleatorias. Los mundos generados aleatoriamente siempre tendr谩n un camino hacia el objetivo.\n","\n","Desde la posici贸n inicial (S), el jugador realiza movimientos sobre las placas congeladas (F) hasta alcanzar el objetivo (G) o caer en un agujero (H). Pero el lago es resbaladizo (a menos que esta opci贸n est茅 deshabilitada), por lo que a veces puede moverse perpendicularmente a la direcci贸n deseada.\n","\n","**Espacio de acciones:** las acciones est谩n en el rango {0, 3}, indicando la direcci贸n en la que se mueve el jugador.\n","*   0: Mover a la izquierda\n","*   1: Mover hacia abajo\n","*   2: Mover a la derecha\n","*   3: Mover hacia arriba\n","\n","**Espacio de Observaci贸n:** la observaci贸n representa la posici贸n actual del jugador como current_row * ncols + current_col (donde tanto la fila como la columna comienzan en 0). La observaci贸n se devuelve como un int().\n","\n","Por ejemplo, la posici贸n de la meta en el mapa 4x4 se puede calcular de la siguiente manera: 3 * 4 + 3 = 15. El n煤mero de observaciones posibles depende del tama帽o del mapa.\n","\n","\n","**Estado Inicial:** el episodio comienza con el jugador en el estado [0] (ubicaci贸n [0, 0]).\n","\n","\n","**Recompensas:** se obtienen las siguientes recompensas:\n","*   Llegar a la meta: +1\n","*   Caer en hoyo: 0\n","*   Permanecer en hielo: 0\n","\n","\n","**Fin del Episodio:** el episodio termina si ocurre alguno de los eventos siguientes:\n","\n","a) Terminaci贸n:\n","*   El jugador se mueve a un hoyo.\n","*   El jugador llega a la meta en max(nrow) * max(ncol) - 1 (ubicaci贸n [max(nrow)-1, max(ncol)-1]).\n","\n","b) Truncamiento (al usar el contenedor time_limit):\n","*   La duraci贸n del episodio es de 100 minutos para el entorno 4x4 y de 200 minutos para el entorno FrozenLake8x8-v1.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"54AIvDov_7aa"},"source":["## 1. Instalamos e importamos las dependencias en Google Colab y hacemos una prueba\n","\n","Usaremos 3 librer铆as de Python:\n","\n","1.   Random: para generar r煤meros aleatorios.\n","2.   Numpy: para nuestra Tabla-Q.\n","3.   Gym: para el entorno del FrozenLake. GYM Es una librer铆a para experimentar con problemas y t茅cnicas de RL creada por OpenAI.\n","\n","A continuaci贸n inicializamos el entorno (sin resbalones) y vamos intentar dar 5 pasos sin caer en un agujero. A cada paso que damos imprimimos el estado del agente y del entorno."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":152,"status":"ok","timestamp":1742556769692,"user":{"displayName":"J.C. Burguillo","userId":"10595095129539658601"},"user_tz":-60},"id":"gxxpHDIs_lvg","outputId":"34f2f369-e87a-4a1a-f9c5-2992afdf679f"},"outputs":[{"name":"stdout","output_type":"stream","text":["  (Up)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","\n","\n","Estado actual:  0\n","  (Up)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","\n","\n","Estado actual:  0\n","  (Down)\n","SFFF\n","\u001b[41mF\u001b[0mHFH\n","FFFH\n","HFFG\n","\n","\n","Estado actual:  4\n","  (Left)\n","SFFF\n","\u001b[41mF\u001b[0mHFH\n","FFFH\n","HFFG\n","\n","\n","Estado actual:  4\n","  (Down)\n","SFFF\n","FHFH\n","\u001b[41mF\u001b[0mFFH\n","HFFG\n","\n","\n","Estado actual:  8\n","Ultima recompensa:  0.0\n","Pasos:  5\n"]}],"source":["import random\n","\n","import numpy as np\n","\n","try:\n","    import gymnasium as gym\n","except ImportError as err:\n","    !pip install gymnasium\n","    import gymnasium as gym\n","\n","\n","# Creamos el entorno para entrenar nuestro agente (el lago no es resbaladizo)\n","env = gym.make (\"FrozenLake-v1\", is_slippery=False, render_mode=\"ansi\")\n","\n","env.reset()     # Inicializamos el entorno\n","steps = 0  # Inicializamos los pasos que damos\n","\n","\n","while steps \u003c 5:     # Vamos un dar un m谩ximo de 5 pasos al azar\n","  action = env.action_space.sample()   # Generamos una acci贸n al azar\n","  new_state, reward, terminated, truncated, info = env.step (action)\n","  print (env.render())                 # Imprimimos el estado del entorno\n","  steps += 1                      # Incrementamos los pasos\n","  print(\"\\nEstado actual: \", new_state)\n","\n","print(\"Ultima recompensa: \", reward)\n","print(\"Pasos: \", steps)"]},{"cell_type":"markdown","metadata":{"id":"JEtXMldxQ7uw"},"source":["## 2. Creamos la Tabla-Q y la inicializamos\n","- Ahora, debemos crear nuestra tabla Q, pero 驴c贸mo sabemos cu谩ntas filas (estados) y columnas (acciones) necesitamos? Debemos obtener `action_size` y `state_size`.\n","- OpenAI Gym nos ayuda a obtener estos datos de un entorno (en nuestro caso, FrozenLake-v1): `env.action_space.n` y `env.observation_space.n`."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1742556769694,"user":{"displayName":"J.C. Burguillo","userId":"10595095129539658601"},"user_tz":-60},"id":"GRmioK9YI_cp","outputId":"a0badf71-eead-429c-a2a2-9aa771380858"},"outputs":[{"name":"stdout","output_type":"stream","text":["Action size:  4\n","State size:  16\n","[[0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]]\n"]}],"source":["action_size = env.action_space.n\n","state_size = env.observation_space.n\n","\n","print (\"Action size: \", action_size)\n","print (\"State size: \", state_size)\n","\n","# Inicializamos nuestra Tabla-Q con state_size filas y action_size columnas\n","qtable = np.zeros ((state_size, action_size))\n","print (qtable)"]},{"cell_type":"markdown","metadata":{"id":"3qqZLKzOAObt"},"source":["Podemos ver que nuestra `qtable` tiene 4x4=16 filas con los estados/posiciones (0 estado inicial y 15 estado objetivo) y 4 columnas con las 4 posibles acciones (izquierda, derecha, arriba, abajo)."]},{"cell_type":"markdown","metadata":{"id":"WEGeWKKsAu7X"},"source":["## 3. Inicializamos los hiperpar谩metros\n","\n","A continuaci贸n inicializamos un conjunto de hiperpar谩metros que vamos a usar en el juego."]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1742556769695,"user":{"displayName":"J.C. Burguillo","userId":"10595095129539658601"},"user_tz":-60},"id":"FJhPxx7UAunE"},"outputs":[],"source":["total_episodes = 20000       # Episodios totales\n","learning_rate = 0.7          # Factor de aprendizaje\n","max_steps = 99               # N煤mero m谩ximo de movimientos por episodio\n","gamma = 0.95                 # Ratio de descuento\n","\n","# Par谩metros de exploraci贸n\n","epsilon = 1.0                 # Ratio de exploraci贸n\n","max_epsilon = 1.0             # Exploration probability at start\n","min_epsilon = 0.01            # Minimum exploration probability\n","decay_rate = 0.005            # Ratio de ca铆da exponencial durante la exploraci贸n"]},{"cell_type":"markdown","metadata":{"id":"rcdaN_DbA3ES"},"source":["## 4. El algoritmo Q-Learning\n","\n","\n","* Q-learning es una t茅cnica de aprendizaje por refuerzo que tiene como objetivo aprender una estrategia que le diga a un agente qu茅 acci贸n tomar bajo qu茅 circunstancias.\n","\n","\n","* Para ello utiliza la ***Q-Function*** definida como:\n","\n","\n","\u003cspan style=\"font-size:20px\"\u003e\n","$$\\widehat{Q}(s, a) = Q(s, a) + \\alpha \\cdot \\left [ R(s,a) + \\left (\\gamma \\cdot \\underset{{a}'}{max} Q({s}',{a}') \\right ) - Q(s,a) \\right ]$$\n","\u003c/span\u003e\n","\n","\n","* Siendo:\n","    + $\\widehat{Q}(s, a)$:El nuevo valor de $Q(s, a)$\n","    + $Q(s, a)$: El antiguo valor de $Q(s, a)$ o valor actual.\n","    + $\\alpha$: Factor de aprendizaje (Learning Rate) que indica \"*cuanto*\" queremos aprender en cada acci贸n.\n","    + $R(s,a)$: Recompensa por tomar la acci贸n '$a$' desde el estado actual $s$.\n","    + $\\gamma$: Factor de descuento.\n","    + $\\underset{{a}'}{max} Q({s}',{a}')$: Valor de $Q(s, a)$ tras tomar la mejor acci贸n posible.\n","    \n","    \n","* Este algoritmo puede aprender la estrategia optimizando la recompensa a corto o largo plazo o puede maximizar la recompensa a largo plazo aunque esto le suponga no realizar el movimiento m谩s prometedor dado un estado. Para elegir entre ambas estrategias usamos el hiperpar谩metro $\\gamma$ (factor de descuento) que tendr谩 que tener un valor cercano a $0$ si queremos tomar una estrategia a corto plazo y un valor cercano a $1$ si decidimos tomar una estrategia a largo plazo.\n","\n","\n","* En este algoritmo debemos tener en cuenta los siguientes puntos:\n","    1. El agente se ejecutara un n煤mero determinado de veces (episodios).\n","    2. Cada vez que se ejecute, el agente partir谩 del estado inicial $S$ por lo que hay que inicializar el entorno.\n","    3. Para cada ejecuci贸n, el agente calcular谩 los valores de las acciones que puede tomar en cada uno de los estados con la Q-Function y actualizar谩 el valor $Q(s,a)$ en la Tabla-Q.\n","    4. El agente en cada estado puede seleccionar las acciones de dos formas:\n","        + Explorando: Selecciona una acci贸n al azar.\n","        + Explotando: Selecciona la mejor acci贸n entre todas las posibles.\n","    5. El algoritmo se ejecuta hasta que el agente llega al estado final.\n","    \n","    \n","* Para determinar si el agente tiene que explorar o explotar en un determinado estado se define un par谩metro conocido como \"greedy_control\" ($\\in$) que no es m谩s que la definici贸n de una probabilidad de explotaci贸n o exploraci贸n; por ejemplo, si definimos $\\in=0.1$, un 10% de las veces exploraremos y un 90% de las veces explotaremos el conocimiento adquirido por el sistema."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"5Dqqo_8LA5De"},"outputs":[{"name":"stdout","output_type":"stream","text":["Recompensa media global: 0.0\n","\n","La tabla-Q obtenida es:\n","[[0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]]\n"]}],"source":["# Lista de recompensas\n","rewards = []\n","\n","# Reiniciamos la qtable (por si lo ejecutamos varias veces)\n","qtable = np.zeros ((state_size, action_size))\n","\n","# Ciclo de ejecuci贸n sobre total_episodes\n","for episode in range(total_episodes):\n","    # Reiniciamos el entorno\n","    state, info = env.reset()\n","    step = 0\n","    done = False\n","    total_rewards = 0\n","\n","    for step in range (max_steps):\n","        ## Elegimos un valor aleatorio para decidir si exploraramos o explotamos\n","        eps_greedy = random.uniform (0, 1)\n","\n","        ## Si el valor aleatorio es mayor que epsilon --\u003e explotaci贸n (tomamos la acci贸n con el mayor valor de Q para este estado)\n","        if eps_greedy \u003e epsilon:\n","            action = np.argmax (qtable[state,:])    # Elegimos una acci贸n (action) en el estado actual (state)\n","\n","        # En el otro caso --\u003e exploraci贸n (elegimos cualquier acci贸n al azar)\n","        else:\n","            action = env.action_space.sample()      # Elegimos una acci贸n (action) en el estado actual (state)\n","\n","\n","        # Usamos la acci贸n (action) y observamos: el nuevo estado (new_state) y la recompensa (reward)\n","        # 'env.step' nos devuelve 5 valores: obs, reward, terminated, truncated, info\n","        new_state, reward, terminated, truncated, info = env.step (action)\n","        done = terminated or truncated    # combinamos terminated y truncated en done\n","\n","\n","        # Actualizamos Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n","        # qtable[new_state,:] : indica todas las acciones que podemos tomar en el nuevo estado\n","        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n","\n","        total_rewards += reward\n","\n","        # Nuestro nuevo estado es state\n","        state = new_state\n","\n","        # Si e ha ca铆do en un agujero o alcanzado el objetivo se termina el episodio\n","        if done == True:\n","            break\n","\n","    # Reducimos epsilon (porque cada vez exploraremos menos)\n","    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp (-decay_rate*episode)\n","    rewards.append (total_rewards)\n","\n","\n","print (\"Recompensa media global: \" +  str(sum(rewards)/total_episodes), end='\\n\\n')\n","print (\"La tabla-Q obtenida es:\")\n","print (qtable)"]},{"cell_type":"markdown","metadata":{"id":"R5czk9qTBQIU"},"source":["## 5. Usamos nuestra Tabla-Q para jugar a FrozenLake !\n","- Tras un largo periodo de aprendizade nuestra Tabla-Q ya puede usarse para jugar a FrozenLake\". Vamos a jugar 5 veces.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Bt8UsREaBNkJ"},"outputs":[{"name":"stdout","output_type":"stream","text":["****************************************************\n","EPISODIO:  0\n","****************************************************\n","EPISODIO:  1\n","****************************************************\n","EPISODIO:  2\n","****************************************************\n","EPISODIO:  3\n","****************************************************\n","EPISODIO:  4\n"]}],"source":["env.reset()\n","\n","for episode in range(5):      # Juega en 5 intentos\n","    state, info = env.reset()\n","    step = 0\n","    done = False\n","    print(\"****************************************************\")\n","    print(\"EPISODIO: \", episode)\n","\n","    for step in range (max_steps):\n","\n","        # Toma la acci贸n que tiene la mayor recompensa futura estimada en ese estado\n","        action = np.argmax (qtable[state,:])\n","\n","        new_state, reward, terminated, truncated, info = env.step (action)\n","        done = terminated or truncated # combinamos terminated o truncated en done\n","\n","        if done:\n","            if new_state == 15:\n","                print (\"Alcanzamos el objetivo \")\n","            else:\n","                print (\"Ca铆mos en un agujero 锔\")\n","\n","            print (\"N煤mero de pasos: \", step)\n","\n","            break\n","        state = new_state\n","env.close()"]},{"cell_type":"markdown","metadata":{"id":"wNjB1mhPFA1D"},"source":["## 6. Vuelve a la secci贸n (3) y modifica los hiperpar谩metros para ver qu茅 ocurre.\n","- Modifica: total_episodes, learning_rate, gamma, etc.\n","- Ejecuta el paso 4 nuevamente para ver qu茅 se obtiene.\n","- **Haz un breve estudio de sensibilidad de los par谩metros, en este escenario, indicando cu谩les son las mejores elecciones y cu谩l influye m谩s en los resultados !**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"F2WAfCng-0rN"},"outputs":[],"source":["# Incluye aqu铆 el c贸digo o las gr谩ficas que muestren el estudio de sensibilidad de par谩metros.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lgv7fnDPFu9-"},"source":["## 7. Genera un nuevo c贸digo usando ahora el algoritmo SARSA !\n","\n","* El algoritmo SARSA (StateActionRewardStateAction) es una t茅cnica de aprendizaje por refuerzo similar al Q-learning, con la diferencia de que en la funci贸n Q no selecciona el valor m谩ximo esperado $\\underset{{a}'}{max} Q({s}',{a}')$, si no que selecciona la acci贸n que hubiesemos tomado en ese nuevo estado ${s}'$ con la pol铆tica actual que estamos utilizando.\n","\n","\n","* Veamos como queda la funci贸n de Estado-Acci贸n:\n","\n","\n","\u003cspan style=\"font-size:20px\"\u003e\n","$$\\widehat{Q}(s, a) = Q(s, a) + \\alpha \\cdot \\left [ R(s,a) + \\left (\\gamma \\cdot Q({s}',{a}') \\right ) - Q(s,a) \\right ]$$\n","\u003c/span\u003e\n","\n","\n","* Siendo:\n","    + $\\widehat{Q}(s, a)$:El nuevo valor de $Q(s, a)$\n","    + $Q(s, a)$: El antiguo valor de $Q(s, a)$ o valor actual.\n","    + $\\alpha$: Factor de aprendizaje (Learning Rate) que indica \"*cuanto*\" queremos aprender en cada acci贸n.\n","    + $R(s,a)$: Recompensa por tomar la acci贸n '$a$' desde el estado actual $s$.\n","    + $\\gamma$: Factor de descuento.\n","    + $Q({s}',{a}')$: Valor de la mejor acci贸n que hubiesemos tomado en el nuevo estado ${a}'$\n","\n","\n","* Como se puede observar en el Pseudoc贸digo la diferencia fundamental frente al Q-Learning es que se selecciona la mejor acci贸n a tomar en el estado actual (igual que en Q-Learning) pero en la funci贸n se toma como valor $Q({s}',{a}')$, es decir, la acci贸n que se tomar铆a en el estado al que nos mover铆amos a continuaci贸n.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"3LY1OnLOGMrL"},"outputs":[],"source":["# Inclumos los hiperpar谩metros a continuaci贸n en el c贸digo para simplificar las modificaciones\n","total_episodes = 20000       # Episodios totales\n","learning_rate = 0.7          # Factor de aprendizaje\n","max_steps = 99               # N煤mero m谩ximo de movimientos por episodio\n","gamma = 0.95                 # Ratio de descuento\n","\n","# Par谩metros de exploraci贸n\n","epsilon = 1.0                 # Ratio de exploraci贸n\n","max_epsilon = 1.0             # Exploration probability at start\n","min_epsilon = 0.01            # Minimum exploration probability\n","decay_rate = 0.005            # Ratio de ca铆da exponencial durante la exploraci贸n\n","\n","\n","# Incluye tu nuevo c贸digo de SARSA a continuaci贸n..."]},{"cell_type":"markdown","metadata":{"id":"GOfIpKhN_SBI"},"source":["\n","## 8. Haz un nuevo estudio de la sensibilidad de los par谩metros sobre SARSA\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"DlOrV2XOOcVb"},"outputs":[],"source":["# Incluye aqu铆 el c贸digo o las gr谩ficas que muestren el estudio de sensibilidad de par谩metros."]},{"cell_type":"markdown","metadata":{"id":"qOPRce3JN7vJ"},"source":["## 9. Compara ambos algoritmos (QL vs. SARSA) en un entorno resbaladizo"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"hdhhu-hbOEaW"},"outputs":[],"source":["# Incluye aqu铆 los resultados (c贸digo o gr谩ficas) de la comparaci贸n..."]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}