{"cells":[{"cell_type":"markdown","metadata":{"id":"5aQKQMJTJBPH"},"source":["# Unidad 7: Aprendizaje por Refuerzo\n","\n","En este *Notebook*, implementaremos un agente para jugar al \u003cb\u003e[FrozenLake](https://gymnasium.farama.org/environments/toy_text/frozen_lake/).\u003c/b\u003e\n","\n","![alt text](https://gymnasium.farama.org/_images/frozen_lake.gif)\n","\n","\n","**Descripción**\n","\n","El objetivo de este juego es ir \u003cb\u003edesde la posición inicial (S, Start) a la posición final (G, Goal)\u003c/b\u003e caminando entre placas congeladas (F, Frozen) de un lago evitando los agujeros en el hielo (H, holes).\n","\n","Los agujeros en el hielo se distribuyen en ubicaciones fijas al usar un mapa predeterminado. Existe la posibilidad de generar mapa con agujeros en ubicaciones aleatorias. Los mundos generados aleatoriamente siempre tendrán un camino hacia el objetivo.\n","\n","Desde la posición inicial (S), el jugador realiza movimientos sobre las placas congeladas (F) hasta alcanzar el objetivo (G) o caer en un agujero (H). Pero el lago es resbaladizo (a menos que esta opción esté deshabilitada), por lo que a veces puede moverse perpendicularmente a la dirección deseada.\n","\n","**Espacio de acciones:** las acciones están en el rango {0, 3}, indicando la dirección en la que se mueve el jugador.\n","*   0: Mover a la izquierda\n","*   1: Mover hacia abajo\n","*   2: Mover a la derecha\n","*   3: Mover hacia arriba\n","\n","**Espacio de Observación:** la observación representa la posición actual del jugador como current_row * ncols + current_col (donde tanto la fila como la columna comienzan en 0). La observación se devuelve como un int().\n","\n","Por ejemplo, la posición de la meta en el mapa 4x4 se puede calcular de la siguiente manera: 3 * 4 + 3 = 15. El número de observaciones posibles depende del tamaño del mapa.\n","\n","\n","**Estado Inicial:** el episodio comienza con el jugador en el estado [0] (ubicación [0, 0]).\n","\n","\n","**Recompensas:** se obtienen las siguientes recompensas:\n","*   Llegar a la meta: +1\n","*   Caer en hoyo: 0\n","*   Permanecer en hielo: 0\n","\n","\n","**Fin del Episodio:** el episodio termina si ocurre alguno de los eventos siguientes:\n","\n","a) Terminación:\n","*   El jugador se mueve a un hoyo.\n","*   El jugador llega a la meta en max(nrow) * max(ncol) - 1 (ubicación [max(nrow)-1, max(ncol)-1]).\n","\n","b) Truncamiento (al usar el contenedor time_limit):\n","*   La duración del episodio es de 100 minutos para el entorno 4x4 y de 200 minutos para el entorno FrozenLake8x8-v1.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"54AIvDov_7aa"},"source":["## 1. Instalamos e importamos las dependencias en Google Colab y hacemos una prueba\n","\n","Usaremos 3 librerías de Python:\n","\n","1.   Random: para generar rúmeros aleatorios.\n","2.   Numpy: para nuestra Tabla-Q.\n","3.   Gym: para el entorno del FrozenLake. GYM Es una librería para experimentar con problemas y técnicas de RL creada por OpenAI.\n","\n","A continuación inicializamos el entorno (sin resbalones) y vamos intentar dar 5 pasos sin caer en un agujero. A cada paso que damos imprimimos el estado del agente y del entorno."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":152,"status":"ok","timestamp":1742556769692,"user":{"displayName":"J.C. Burguillo","userId":"10595095129539658601"},"user_tz":-60},"id":"gxxpHDIs_lvg","outputId":"34f2f369-e87a-4a1a-f9c5-2992afdf679f"},"outputs":[{"name":"stdout","output_type":"stream","text":["  (Up)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","\n","\n","Estado actual:  0\n","  (Up)\n","\u001b[41mS\u001b[0mFFF\n","FHFH\n","FFFH\n","HFFG\n","\n","\n","Estado actual:  0\n","  (Down)\n","SFFF\n","\u001b[41mF\u001b[0mHFH\n","FFFH\n","HFFG\n","\n","\n","Estado actual:  4\n","  (Left)\n","SFFF\n","\u001b[41mF\u001b[0mHFH\n","FFFH\n","HFFG\n","\n","\n","Estado actual:  4\n","  (Down)\n","SFFF\n","FHFH\n","\u001b[41mF\u001b[0mFFH\n","HFFG\n","\n","\n","Estado actual:  8\n","Ultima recompensa:  0.0\n","Pasos:  5\n"]}],"source":["import random\n","\n","import numpy as np\n","\n","try:\n","    import gymnasium as gym\n","except ImportError as err:\n","    !pip install gymnasium\n","    import gymnasium as gym\n","\n","\n","# Creamos el entorno para entrenar nuestro agente (el lago no es resbaladizo)\n","env = gym.make (\"FrozenLake-v1\", is_slippery=False, render_mode=\"ansi\")\n","\n","env.reset()     # Inicializamos el entorno\n","steps = 0  # Inicializamos los pasos que damos\n","\n","\n","while steps \u003c 5:     # Vamos un dar un máximo de 5 pasos al azar\n","  action = env.action_space.sample()   # Generamos una acción al azar\n","  new_state, reward, terminated, truncated, info = env.step (action)\n","  print (env.render())                 # Imprimimos el estado del entorno\n","  steps += 1                      # Incrementamos los pasos\n","  print(\"\\nEstado actual: \", new_state)\n","\n","print(\"Ultima recompensa: \", reward)\n","print(\"Pasos: \", steps)"]},{"cell_type":"markdown","metadata":{"id":"JEtXMldxQ7uw"},"source":["## 2. Creamos la Tabla-Q y la inicializamos\n","- Ahora, debemos crear nuestra tabla Q, pero ¿cómo sabemos cuántas filas (estados) y columnas (acciones) necesitamos? Debemos obtener `action_size` y `state_size`.\n","- OpenAI Gym nos ayuda a obtener estos datos de un entorno (en nuestro caso, FrozenLake-v1): `env.action_space.n` y `env.observation_space.n`."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1742556769694,"user":{"displayName":"J.C. Burguillo","userId":"10595095129539658601"},"user_tz":-60},"id":"GRmioK9YI_cp","outputId":"a0badf71-eead-429c-a2a2-9aa771380858"},"outputs":[{"name":"stdout","output_type":"stream","text":["Action size:  4\n","State size:  16\n","[[0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]]\n"]}],"source":["action_size = env.action_space.n\n","state_size = env.observation_space.n\n","\n","print (\"Action size: \", action_size)\n","print (\"State size: \", state_size)\n","\n","# Inicializamos nuestra Tabla-Q con state_size filas y action_size columnas\n","qtable = np.zeros ((state_size, action_size))\n","print (qtable)"]},{"cell_type":"markdown","metadata":{"id":"3qqZLKzOAObt"},"source":["Podemos ver que nuestra `qtable` tiene 4x4=16 filas con los estados/posiciones (0 estado inicial y 15 estado objetivo) y 4 columnas con las 4 posibles acciones (izquierda, derecha, arriba, abajo)."]},{"cell_type":"markdown","metadata":{"id":"WEGeWKKsAu7X"},"source":["## 3. Inicializamos los hiperparámetros\n","\n","A continuación inicializamos un conjunto de hiperparámetros que vamos a usar en el juego."]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1742556769695,"user":{"displayName":"J.C. Burguillo","userId":"10595095129539658601"},"user_tz":-60},"id":"FJhPxx7UAunE"},"outputs":[],"source":["total_episodes = 20000       # Episodios totales\n","learning_rate = 0.7          # Factor de aprendizaje\n","max_steps = 99               # Número máximo de movimientos por episodio\n","gamma = 0.95                 # Ratio de descuento\n","\n","# Parámetros de exploración\n","epsilon = 1.0                 # Ratio de exploración\n","max_epsilon = 1.0             # Exploration probability at start\n","min_epsilon = 0.01            # Minimum exploration probability\n","decay_rate = 0.005            # Ratio de caída exponencial durante la exploración"]},{"cell_type":"markdown","metadata":{"id":"rcdaN_DbA3ES"},"source":["## 4. El algoritmo Q-Learning\n","\n","\n","* Q-learning es una técnica de aprendizaje por refuerzo que tiene como objetivo aprender una estrategia que le diga a un agente qué acción tomar bajo qué circunstancias.\n","\n","\n","* Para ello utiliza la ***Q-Function*** definida como:\n","\n","\n","\u003cspan style=\"font-size:20px\"\u003e\n","$$\\widehat{Q}(s, a) = Q(s, a) + \\alpha \\cdot \\left [ R(s,a) + \\left (\\gamma \\cdot \\underset{{a}'}{max} Q({s}',{a}') \\right ) - Q(s,a) \\right ]$$\n","\u003c/span\u003e\n","\n","\n","* Siendo:\n","    + $\\widehat{Q}(s, a)$:El nuevo valor de $Q(s, a)$\n","    + $Q(s, a)$: El antiguo valor de $Q(s, a)$ o valor actual.\n","    + $\\alpha$: Factor de aprendizaje (Learning Rate) que indica \"*cuanto*\" queremos aprender en cada acción.\n","    + $R(s,a)$: Recompensa por tomar la acción '$a$' desde el estado actual $s$.\n","    + $\\gamma$: Factor de descuento.\n","    + $\\underset{{a}'}{max} Q({s}',{a}')$: Valor de $Q(s, a)$ tras tomar la mejor acción posible.\n","    \n","    \n","* Este algoritmo puede aprender la estrategia optimizando la recompensa a corto o largo plazo o puede maximizar la recompensa a largo plazo aunque esto le suponga no realizar el movimiento más prometedor dado un estado. Para elegir entre ambas estrategias usamos el hiperparámetro $\\gamma$ (factor de descuento) que tendrá que tener un valor cercano a $0$ si queremos tomar una estrategia a corto plazo y un valor cercano a $1$ si decidimos tomar una estrategia a largo plazo.\n","\n","\n","* En este algoritmo debemos tener en cuenta los siguientes puntos:\n","    1. El agente se ejecutara un número determinado de veces (episodios).\n","    2. Cada vez que se ejecute, el agente partirá del estado inicial $S$ por lo que hay que inicializar el entorno.\n","    3. Para cada ejecución, el agente calculará los valores de las acciones que puede tomar en cada uno de los estados con la Q-Function y actualizará el valor $Q(s,a)$ en la Tabla-Q.\n","    4. El agente en cada estado puede seleccionar las acciones de dos formas:\n","        + Explorando: Selecciona una acción al azar.\n","        + Explotando: Selecciona la mejor acción entre todas las posibles.\n","    5. El algoritmo se ejecuta hasta que el agente llega al estado final.\n","    \n","    \n","* Para determinar si el agente tiene que explorar o explotar en un determinado estado se define un parámetro conocido como \"greedy_control\" ($\\in$) que no es más que la definición de una probabilidad de explotación o exploración; por ejemplo, si definimos $\\in=0.1$, un 10% de las veces exploraremos y un 90% de las veces explotaremos el conocimiento adquirido por el sistema."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"5Dqqo_8LA5De"},"outputs":[{"name":"stdout","output_type":"stream","text":["Recompensa media global: 0.0\n","\n","La tabla-Q obtenida es:\n","[[0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]]\n"]}],"source":["# Lista de recompensas\n","rewards = []\n","\n","# Reiniciamos la qtable (por si lo ejecutamos varias veces)\n","qtable = np.zeros ((state_size, action_size))\n","\n","# Ciclo de ejecución sobre total_episodes\n","for episode in range(total_episodes):\n","    # Reiniciamos el entorno\n","    state, info = env.reset()\n","    step = 0\n","    done = False\n","    total_rewards = 0\n","\n","    for step in range (max_steps):\n","        ## Elegimos un valor aleatorio para decidir si exploraramos o explotamos\n","        eps_greedy = random.uniform (0, 1)\n","\n","        ## Si el valor aleatorio es mayor que epsilon --\u003e explotación (tomamos la acción con el mayor valor de Q para este estado)\n","        if eps_greedy \u003e epsilon:\n","            action = np.argmax (qtable[state,:])    # Elegimos una acción (action) en el estado actual (state)\n","\n","        # En el otro caso --\u003e exploración (elegimos cualquier acción al azar)\n","        else:\n","            action = env.action_space.sample()      # Elegimos una acción (action) en el estado actual (state)\n","\n","\n","        # Usamos la acción (action) y observamos: el nuevo estado (new_state) y la recompensa (reward)\n","        # 'env.step' nos devuelve 5 valores: obs, reward, terminated, truncated, info\n","        new_state, reward, terminated, truncated, info = env.step (action)\n","        done = terminated or truncated    # combinamos terminated y truncated en done\n","\n","\n","        # Actualizamos Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n","        # qtable[new_state,:] : indica todas las acciones que podemos tomar en el nuevo estado\n","        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n","\n","        total_rewards += reward\n","\n","        # Nuestro nuevo estado es state\n","        state = new_state\n","\n","        # Si e ha caído en un agujero o alcanzado el objetivo se termina el episodio\n","        if done == True:\n","            break\n","\n","    # Reducimos epsilon (porque cada vez exploraremos menos)\n","    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp (-decay_rate*episode)\n","    rewards.append (total_rewards)\n","\n","\n","print (\"Recompensa media global: \" +  str(sum(rewards)/total_episodes), end='\\n\\n')\n","print (\"La tabla-Q obtenida es:\")\n","print (qtable)"]},{"cell_type":"markdown","metadata":{"id":"R5czk9qTBQIU"},"source":["## 5. Usamos nuestra Tabla-Q para jugar a FrozenLake !\n","- Tras un largo periodo de aprendizade nuestra Tabla-Q ya puede usarse para jugar a FrozenLake\". Vamos a jugar 5 veces.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Bt8UsREaBNkJ"},"outputs":[{"name":"stdout","output_type":"stream","text":["****************************************************\n","EPISODIO:  0\n","****************************************************\n","EPISODIO:  1\n","****************************************************\n","EPISODIO:  2\n","****************************************************\n","EPISODIO:  3\n","****************************************************\n","EPISODIO:  4\n"]}],"source":["env.reset()\n","\n","for episode in range(5):      # Juega en 5 intentos\n","    state, info = env.reset()\n","    step = 0\n","    done = False\n","    print(\"****************************************************\")\n","    print(\"EPISODIO: \", episode)\n","\n","    for step in range (max_steps):\n","\n","        # Toma la acción que tiene la mayor recompensa futura estimada en ese estado\n","        action = np.argmax (qtable[state,:])\n","\n","        new_state, reward, terminated, truncated, info = env.step (action)\n","        done = terminated or truncated # combinamos terminated o truncated en done\n","\n","        if done:\n","            if new_state == 15:\n","                print (\"Alcanzamos el objetivo 🏆\")\n","            else:\n","                print (\"Caímos en un agujero ☠️\")\n","\n","            print (\"Número de pasos: \", step)\n","\n","            break\n","        state = new_state\n","env.close()"]},{"cell_type":"markdown","metadata":{"id":"wNjB1mhPFA1D"},"source":["## 6. Vuelve a la sección (3) y modifica los hiperparámetros para ver qué ocurre.\n","- Modifica: total_episodes, learning_rate, gamma, etc.\n","- Ejecuta el paso 4 nuevamente para ver qué se obtiene.\n","- **Haz un breve estudio de sensibilidad de los parámetros, en este escenario, indicando cuáles son las mejores elecciones y cuál influye más en los resultados !**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"F2WAfCng-0rN"},"outputs":[],"source":["# Incluye aquí el código o las gráficas que muestren el estudio de sensibilidad de parámetros.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"lgv7fnDPFu9-"},"source":["## 7. Genera un nuevo código usando ahora el algoritmo SARSA !\n","\n","* El algoritmo SARSA (State–Action–Reward–State–Action) es una técnica de aprendizaje por refuerzo similar al Q-learning, con la diferencia de que en la función Q no selecciona el valor máximo esperado $\\underset{{a}'}{max} Q({s}',{a}')$, si no que selecciona la acción que hubiesemos tomado en ese nuevo estado ${s}'$ con la política actual que estamos utilizando.\n","\n","\n","* Veamos como queda la función de Estado-Acción:\n","\n","\n","\u003cspan style=\"font-size:20px\"\u003e\n","$$\\widehat{Q}(s, a) = Q(s, a) + \\alpha \\cdot \\left [ R(s,a) + \\left (\\gamma \\cdot Q({s}',{a}') \\right ) - Q(s,a) \\right ]$$\n","\u003c/span\u003e\n","\n","\n","* Siendo:\n","    + $\\widehat{Q}(s, a)$:El nuevo valor de $Q(s, a)$\n","    + $Q(s, a)$: El antiguo valor de $Q(s, a)$ o valor actual.\n","    + $\\alpha$: Factor de aprendizaje (Learning Rate) que indica \"*cuanto*\" queremos aprender en cada acción.\n","    + $R(s,a)$: Recompensa por tomar la acción '$a$' desde el estado actual $s$.\n","    + $\\gamma$: Factor de descuento.\n","    + $Q({s}',{a}')$: Valor de la mejor acción que hubiesemos tomado en el nuevo estado ${a}'$\n","\n","\n","* Como se puede observar en el Pseudocódigo la diferencia fundamental frente al Q-Learning es que se selecciona la mejor acción a tomar en el estado actual (igual que en Q-Learning) pero en la función se toma como valor $Q({s}',{a}')$, es decir, la acción que se tomaría en el estado al que nos moveríamos a continuación.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"3LY1OnLOGMrL"},"outputs":[],"source":["# Inclumos los hiperparámetros a continuación en el código para simplificar las modificaciones\n","total_episodes = 20000       # Episodios totales\n","learning_rate = 0.7          # Factor de aprendizaje\n","max_steps = 99               # Número máximo de movimientos por episodio\n","gamma = 0.95                 # Ratio de descuento\n","\n","# Parámetros de exploración\n","epsilon = 1.0                 # Ratio de exploración\n","max_epsilon = 1.0             # Exploration probability at start\n","min_epsilon = 0.01            # Minimum exploration probability\n","decay_rate = 0.005            # Ratio de caída exponencial durante la exploración\n","\n","\n","# Incluye tu nuevo código de SARSA a continuación..."]},{"cell_type":"markdown","metadata":{"id":"GOfIpKhN_SBI"},"source":["\n","## 8. Haz un nuevo estudio de la sensibilidad de los parámetros sobre SARSA\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"DlOrV2XOOcVb"},"outputs":[],"source":["# Incluye aquí el código o las gráficas que muestren el estudio de sensibilidad de parámetros."]},{"cell_type":"markdown","metadata":{"id":"qOPRce3JN7vJ"},"source":["## 9. Compara ambos algoritmos (QL vs. SARSA) en un entorno resbaladizo"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"hdhhu-hbOEaW"},"outputs":[],"source":["# Incluye aquí los resultados (código o gráficas) de la comparación..."]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}